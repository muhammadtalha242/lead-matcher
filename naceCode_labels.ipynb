{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile =  './data/nexxt_change_sales_listings_geocoded_short_test.csv' \n",
    "# sales_file_nace =  './data/nexxt_change_sales_listings_geocoded.csv' \n",
    "sales_file_brachen =  './data/branche_nexxt_change_sales_listings.csv' \n",
    "sales_file_nace =  './data/dub_listings_geo.csv'\n",
    "buyer_file_nace =  './data/nexxt_change_purchase_listings_geocoded.csv' \n",
    "nacecode_josn =  './data/nace_codes.json' \n",
    "nacecode_array_josn =  './data/nace_codes_array.json' \n",
    "nacecode_array_obj =  './data/nace_codes_object.json' \n",
    "nacecode_array_obj_ext =  './data/nace_codes_object_ext.json' \n",
    "nacecode_array_obj_du =  './data/nace_codes_object_du.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load SpaCy's German model with NER and POS capabilities\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_sm')\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Preprocess text function with NER and POS tagging\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # Remove URLs, emails, and long numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    # Remove non-alphabetic characters (keeping German characters)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Initialize stopwords and stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Retain nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities and include them\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}]\n",
    "    entities = [stemmer.stem(ent.lower()) for ent in entities if ent.lower() not in stop_words]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Load NACE codes\n",
    "def load_nace_codes(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes\n",
    "\n",
    "# Create embeddings\n",
    "def create_embeddings(texts, model):\n",
    "    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Load Data\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# Filepaths (replace with your actual file paths)\n",
    "sellers_filepath = sales_file_brachen  # Path to your sellers CSV file\n",
    "nace_codes_filepath = nacecode_array_obj_du  # Path to your NACE codes JSON file\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"ðŸš€ ~ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "# model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"ðŸš€ ~ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# Preprocess NACE descriptions\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "nace_embeddings = create_embeddings(nace_descriptions, model)\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ ~ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# Preprocess 'branchen' field in sellers data\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Create embeddings for 'branchen'\n",
    "branchen_embeddings = create_embeddings(sellers_df['preprocessed_branchen'].tolist(), model)\n",
    "print(\"ðŸš€ ~ Created embeddings for 'branchen' field.\")\n",
    "\n",
    "# Compute cosine similarity between 'branchen' embeddings and NACE embeddings\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "\n",
    "# Assign the NACE code with the highest similarity\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"ðŸš€ ~ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optional: Set a similarity threshold to filter uncertain assignments\n",
    "similarity_threshold = 0.2\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] if row['assigned_nace_similarity'] >= similarity_threshold else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the preliminary assignments\n",
    "sellers_df.to_csv(f'{sellers_filepath}_nace.csv', index=False)\n",
    "print(\"ðŸš€ ~ Saved sellers data with preliminary NACE code assignments to 'sellers_with_preassigned_nace.csv'.\")\n",
    "\n",
    "# Review the assignments\n",
    "print(\"\\nSample of NACE Code Assignments:\")\n",
    "print(sellers_df[['branchen', 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "updated with better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Sellers and NACE codes loaded.\n",
      "ðŸš€ Loaded SentenceTransformer model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e77d79f6064218921452df2b0ec6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Created embeddings for NACE descriptions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5556263db66b4b39a9464a44e70c72f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Created embeddings for sellers' 'branchen' field.\n",
      "ðŸš€ Assigned preliminary NACE codes based on 'branchen' similarity.\n",
      "ðŸš€ Saved sellers data with assigned NACE codes to: ./data/branche_nexxt_change_sales_listings_nace.csv\n",
      "\n",
      "Sample of NACE Code Assignments:\n",
      "                                            branchen assigned_nace_code  \\\n",
      "0              Verarbeitendes Gewerbe > Maschinenbau            C28.9.2   \n",
      "1                          Gastgewerbe > Gastronomie                I56   \n",
      "2  Dienstleistung > Sonstige Dienstleistungen > S...            C18.1.4   \n",
      "3  Handel > GroÃŸhandel und Handelsvermittlung; Ve...                G46   \n",
      "4  Baugewerbe > Bauinstallation > Elektroinstalla...            F43.2.1   \n",
      "5       Handwerk > Handwerke fÃ¼r den privaten Bedarf            C28.2.4   \n",
      "6  Handwerk > Ausbaugewerbe > Installateur und He...            G46.7.4   \n",
      "7                     GrundstÃ¼cks- und Wohnungswesen            Q86.9.0   \n",
      "8                                             Handel              G45.1   \n",
      "9           Information und Kommunikation > Sonstige                  J   \n",
      "\n",
      "   assigned_nace_similarity nace_code  \n",
      "0                  0.709497   C28.9.2  \n",
      "1                  1.000000       I56  \n",
      "2                  0.943178   C18.1.4  \n",
      "3                  0.706547       G46  \n",
      "4                  0.716215   F43.2.1  \n",
      "5                  0.543486   C28.2.4  \n",
      "6                  0.574208   G46.7.4  \n",
      "7                  0.530533   Q86.9.0  \n",
      "8                  0.689414     G45.1  \n",
      "9                  1.000000         J  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load SpaCy's German model (for tokenization, NER, POS tagging)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_sm')\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Preprocessing function (with German NER, POS, etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    - Lowercases the text.\n",
    "    - Removes URLs, emails, & large digit sequences.\n",
    "    - Filters out non-alphabetic chars except German Umlauts/ÃŸ.\n",
    "    - Uses SpaCy to keep only NOUN, PROPN, VERB tokens not in stopwords.\n",
    "    - Applies Snowball stemming on remaining tokens.\n",
    "    - Also includes certain named entities (ORG, PRODUCT, GPE).\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, emails, large numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    \n",
    "    # Keep only letters and German characters\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s]', '', text)\n",
    "    \n",
    "    # Compact multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Initialize German stopwords and Snowball stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Keep nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities (ORG, PRODUCT, GPE) and include them\n",
    "    entities = [\n",
    "        ent.text for ent in doc.ents \n",
    "        if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}\n",
    "    ]\n",
    "    # Stem and remove stopwords from entities\n",
    "    entities = [\n",
    "        stemmer.stem(ent.lower()) \n",
    "        for ent in entities \n",
    "        if ent.lower() not in stop_words\n",
    "    ]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Load NACE codes from JSON\n",
    "# -------------------------------------------------------------------------\n",
    "def load_nace_codes(filepath):\n",
    "    \"\"\"\n",
    "    Expects a JSON file where keys = NACE code, values = textual descriptions.\n",
    "    Example:\n",
    "      {\n",
    "        \"01.1\": \"Growing of non-perennial crops\",\n",
    "        \"01.2\": \"Growing of perennial crops\",\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings with sentence-transformers\n",
    "# -------------------------------------------------------------------------\n",
    "def create_embeddings(texts, model):\n",
    "    \"\"\"\n",
    "    Uses the model to encode a list of texts,\n",
    "    returning normalized NumPy arrays of embeddings.\n",
    "    \"\"\"\n",
    "    return model.encode(\n",
    "        texts, \n",
    "        show_progress_bar=True, \n",
    "        convert_to_numpy=True, \n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "sellers_filepath = sales_file_brachen       # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"ðŸš€ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = create_embeddings(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = create_embeddings(sellers_df['preprocessed_branchen'].tolist(), model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"ðŸš€ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.2\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['branchen', 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/nltk/__init__.py:146\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/nltk/chunk/__init__.py:155\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/nltk/chunk/api.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mChunkParserI\u001b[39;00m(ParserI):\n\u001b[1;32m     19\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    A processing interface for identifying non-overlapping groups in\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    unrestricted text.  Typically, chunk parsers are used to find base\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    will always generate a parse.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/nltk/parse/__init__.py:100\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecursivedescent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     96\u001b[0m     RecursiveDescentParser,\n\u001b[1;32m     97\u001b[0m     SteppingRecursiveDescentParser,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshiftreduce\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShiftReduceParser, SteppingShiftReduceParser\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransitionparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransitionParser\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TestGrammar, extract_test_sentences, load_parser\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mviterbi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViterbiParser\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/nltk/parse/transitionparser.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svm\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_svmlight_file\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/svm/__init__.py:12\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# See http://scikit-learn.sourceforge.net/modules/svm.html for complete\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# documentation.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         of their respective owners.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# License: BSD 3 clause (C) INRIA 2010\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bounds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m l1_min_c\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC, SVR, LinearSVC, LinearSVR, NuSVC, NuSVR, OneClassSVM\n\u001b[1;32m     14\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearSVR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_min_c\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/svm/_classes.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, OutlierMixin, RegressorMixin, _fit_context\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearClassifierMixin, LinearModel, SparseCoefMixin\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, StrOptions\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/linear_model/__init__.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARDRegression, BayesianRidge\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coordinate_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ElasticNet,\n\u001b[1;32m     11\u001b[0m     ElasticNetCV,\n\u001b[1;32m     12\u001b[0m     Lasso,\n\u001b[1;32m     13\u001b[0m     LassoCV,\n\u001b[1;32m     14\u001b[0m     MultiTaskElasticNet,\n\u001b[1;32m     15\u001b[0m     MultiTaskElasticNetCV,\n\u001b[1;32m     16\u001b[0m     MultiTaskLasso,\n\u001b[1;32m     17\u001b[0m     MultiTaskLassoCV,\n\u001b[1;32m     18\u001b[0m     enet_path,\n\u001b[1;32m     19\u001b[0m     lasso_path,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_glm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GammaRegressor, PoissonRegressor, TweedieRegressor\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_huber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuberRegressor\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputMixin, RegressorMixin, _fit_context\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch, check_array, check_scalar\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     MetadataRouter,\n\u001b[1;32m     24\u001b[0m     MethodMapping,\n\u001b[1;32m     25\u001b[0m     _raise_for_params,\n\u001b[1;32m     26\u001b[0m     get_routing_for_object,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/model_selection/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Tools for model selection, such as cross validation and hyper-parameter tuning.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification_threshold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     FixedThresholdClassifier,\n\u001b[1;32m      7\u001b[0m     TunedThresholdClassifierCV,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/model_selection/_classification_threshold.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseEstimator,\n\u001b[1;32m      8\u001b[0m     ClassifierMixin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     clone,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotFittedError\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     check_scoring,\n\u001b[1;32m     16\u001b[0m     get_scorer_names,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BaseScorer\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/metrics/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Score functions, performance metrics, pairwise metrics and distance computations.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     accuracy_score,\n\u001b[1;32m      6\u001b[0m     balanced_accuracy_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     zero_one_loss,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/metrics/cluster/__init__.py:25\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[1;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     v_measure_score,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[1;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[1;32m     28\u001b[0m     silhouette_samples,\n\u001b[1;32m     29\u001b[0m     silhouette_score,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py:23\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _atol_for_type\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     Interval,\n\u001b[1;32m     20\u001b[0m     StrOptions,\n\u001b[1;32m     21\u001b[0m     validate_params,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VALID_METRICS, pairwise_distances, pairwise_distances_chunked\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_number_of_labels\u001b[39m(n_labels, n_samples):\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        Number of samples.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/metrics/pairwise.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_samples, check_non_negative\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgKmin\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Utility Functions\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py:94\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Pairwise Distances Reductions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m#    (see :class:`MiddleTermComputer{32,64}`).\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     95\u001b[0m     ArgKmin,\n\u001b[1;32m     96\u001b[0m     ArgKminClassMode,\n\u001b[1;32m     97\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[1;32m     98\u001b[0m     RadiusNeighbors,\n\u001b[1;32m     99\u001b[0m     RadiusNeighborsClassMode,\n\u001b[1;32m    100\u001b[0m     sqeuclidean_row_norms,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseDistancesReductionDispatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean_row_norms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m ]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# ruff: noqa: E501\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/self-learning/lead-matcher/venv/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:13\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     BOOL_METRICS,\n\u001b[1;32m     10\u001b[0m     METRIC_MAPPING64,\n\u001b[1;32m     11\u001b[0m     DistanceMetric,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     ArgKmin32,\n\u001b[1;32m     15\u001b[0m     ArgKmin64,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin_classmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     ArgKminClassMode32,\n\u001b[1;32m     19\u001b[0m     ArgKminClassMode64,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _sqeuclidean_row_norms32, _sqeuclidean_row_norms64\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import shelve\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Ensure nltk stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states and cities.\"\"\"\n",
    "    locations = set()\n",
    "    german_states = {\n",
    "        'baden-wÃ¼rttemberg', 'bayern', 'berlin', 'brandenburg', 'bremen',\n",
    "        'hamburg', 'hessen', 'mecklenburg-vorpommern', 'niedersachsen',\n",
    "        'nordrhein-westfalen', 'rheinland-pfalz', 'saarland', 'sachsen',\n",
    "        'sachsen-anhalt', 'schleswig-holstein', 'thÃ¼ringen'\n",
    "    }\n",
    "\n",
    "    if pd.isna(location):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split on common delimiters\n",
    "        parts = re.split(r'[>/,\\n]\\s*', str(location))\n",
    "        split_locations = []\n",
    "        for part in parts:\n",
    "            part = part.strip().lower()\n",
    "            if part:\n",
    "                # Further split by space if multiple states are concatenated\n",
    "                words = part.split()\n",
    "                temp = []\n",
    "                current = \"\"\n",
    "                for word in words:\n",
    "                    if word in german_states:\n",
    "                        if current:\n",
    "                            temp.append(current.strip())\n",
    "                        current = word\n",
    "                    else:\n",
    "                        current += \" \" + word if current else word\n",
    "                if current:\n",
    "                    temp.append(current.strip())\n",
    "                split_locations.extend(temp)\n",
    "        \n",
    "        for loc in split_locations:\n",
    "            loc = loc.strip().lower()\n",
    "            if loc:\n",
    "                if loc in german_states:\n",
    "                    locations.add(loc.title())  # Capitalize for better geocoding\n",
    "                else:\n",
    "                    # Remove \"region\" and other common prefixes\n",
    "                    clean_part = re.sub(r'^region\\s+', '', loc)\n",
    "                    if clean_part:\n",
    "                        locations.add(clean_part.title())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return locations\n",
    "\n",
    "# def get_all_unique_locations(buyers_df, sellers_df):\n",
    "#     \"\"\"Extract all unique locations from buyers and sellers dataframes.\"\"\"\n",
    "#     unique_locations = set()\n",
    "\n",
    "#     for df, name in [(buyers_df, 'buyers'), (sellers_df, 'sellers')]:\n",
    "#         logging.info(f'Extracting locations from {name} dataframe...')\n",
    "#         for idx, location in df['location'].items():\n",
    "#             locations = _extract_location_parts(location)\n",
    "#             unique_locations.update(locations)\n",
    "\n",
    "#     logging.info(f'Total unique locations found: {len(unique_locations)}')\n",
    "#     return unique_locations\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode unique locations with caching.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"buyer_seller_matching\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3, error_wait_seconds=10.0)\n",
    "\n",
    "    # Ensure cache directory exists\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if cache_dir and not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        for location in unique_locations:\n",
    "            if location in geocode_cache:\n",
    "                continue  # Already cached\n",
    "            try:\n",
    "                logging.info(f'Geocoding location: {location}')\n",
    "                loc = geocode(location + \", Germany\")\n",
    "                if loc:\n",
    "                    geocode_cache[location] = {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "                    logging.info(f'Geocoded {location}: ({loc.latitude}, {loc.longitude})')\n",
    "                else:\n",
    "                    geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "                    logging.warning(f'Geocoding failed for location: {location}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Geocoding error for location '{location}': {e}\")\n",
    "                geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "\n",
    "def update_dataframe_with_geocodes(df, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Add latitude and longitude columns to the dataframe based on locations.\"\"\"\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            lat_list = []\n",
    "            lon_list = []\n",
    "            for loc in locations:\n",
    "                geocode_info = geocode_cache.get(loc, {'latitude': None, 'longitude': None})\n",
    "                if geocode_info['latitude'] is not None and geocode_info['longitude'] is not None:\n",
    "                    lat_list.append(geocode_info['latitude'])\n",
    "                    lon_list.append(geocode_info['longitude'])\n",
    "                else:\n",
    "                    # If geocoding failed, append None\n",
    "                    lat_list.append(None)\n",
    "                    lon_list.append(None)\n",
    "            # Convert lists to JSON strings for CSV compatibility\n",
    "            latitudes.append(json.dumps(lat_list))\n",
    "            longitudes.append(json.dumps(lon_list))\n",
    "\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    return df\n",
    "\n",
    "# Paths to input and output files\n",
    "sellers_input_path = './data/branche_nexxt_change_sales_listings_nace.csv'\n",
    "sellers_output_path = './data/branche_nexxt_change_sales_listings_nace_geocoded.csv'\n",
    "\n",
    "# buyers_input_path = './data/nexxt_change_purchase_listings.csv'\n",
    "# buyers_output_path = './data/nexxt_change_purchase_listings_geocoded.csv'\n",
    "cache_path = './geocode_cache.db'\n",
    "\n",
    "# Load buyer and seller datasets\n",
    "logging.info('Loading buyer and seller datasets...')\n",
    "# buyers_df = pd.read_csv(buyers_input_path)\n",
    "sellers_df = pd.read_csv(sellers_input_path)\n",
    "\n",
    "# Extract unique locations\n",
    "# unique_locations = get_all_unique_locations(sellers_df)\n",
    "\n",
    "# Geocode locations with caching\n",
    "geocode_locations(sellers_df, cache_path=cache_path)\n",
    "\n",
    "# Update dataframes with geocodes\n",
    "logging.info('Updating buyers dataframe with geocodes...')\n",
    "# buyers_df = update_dataframe_with_geocodes(buyers_df, cache_path=cache_path)\n",
    "\n",
    "logging.info('Updating sellers dataframe with geocodes...')\n",
    "sellers_df = update_dataframe_with_geocodes(sellers_df, cache_path=cache_path)\n",
    "\n",
    "# Save updated dataframes to new CSV files\n",
    "logging.info('Saving updated buyers dataframe...')\n",
    "# buyers_df.to_csv(buyers_output_path, index=False)\n",
    "\n",
    "logging.info('Saving updated sellers dataframe...')\n",
    "sellers_df.to_csv(sellers_output_path, index=False)\n",
    "\n",
    "logging.info('Geocoding process completed successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# combined = pd.read_csv('./data/combined.csv')\n",
    "training_updated_branchen = pd.read_csv('./data/nexxt_change_data_for_model_training_updated_branchen.csv')\n",
    "nexxt_updated_branche = pd.read_csv('./data/branche_nexxt_change_sales_listings_nace_geocoded.csv')\n",
    "dejuna = pd.read_csv('./data/buyer_dejuna_geocoded_test-.csv')\n",
    "dub = pd.read_csv('./data/dub_listings.csv')\n",
    "branche_nexxt_change_sales_listings = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source1_path=\"./data/nexxt_change_sales_listings_update.csv\"\n",
    "source2_path=\"./data/dub_listings.csv\"\n",
    "output_path=\"./data/combined.csv\"\n",
    "temp = pd.read_csv(source1_path, sep=',')\n",
    "    # --- Read Source1 ---\n",
    "df1 = pd.read_csv(source1_path, sep=',')\n",
    "\n",
    "# Rename columns in Source1 to our standardized names (where needed).\n",
    "# If the columns in Source1 are already named exactly as we want\n",
    "# (e.g., 'title', 'description', 'location', etc.), we simply keep them.\n",
    "# Otherwise, rename them accordingly.\n",
    "df1.rename(columns={\n",
    "    # 'date': 'some_other_name',            # Not needed in final, so not renamed\n",
    "    'title': 'title',\n",
    "    'description': 'description',\n",
    "    'long_description': 'long_description',\n",
    "    'branchen': 'branchen',\n",
    "    'location': 'location'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only the standardized columns from Source1\n",
    "# Some columns (like date, url, standort, etc.) are not needed in the final dataset\n",
    "df1 = df1[['title', 'long_description', 'description', 'branchen', 'location']]\n",
    "\n",
    "# --- Read Source2 ---\n",
    "df2 = pd.read_csv(source2_path, sep=',')\n",
    "\n",
    "# Rename columns in Source2 to our standardized names\n",
    "df2.rename(columns={\n",
    "    'Title': 'title',\n",
    "    'Beschreibung des Verkaufsangebots': 'long_description',\n",
    "    'Anforderungen an den KÃ¤ufer': 'description',\n",
    "    'Branchen': 'branchen',\n",
    "    'Region': 'location'\n",
    "}, inplace=True)\n",
    "\n",
    "# Select only the standardized columns from Source2\n",
    "df2 = df2[['title', 'long_description', 'description', 'branchen', 'location']]\n",
    "\n",
    "# --- Concatenate both DataFrames ---\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "df_combined.to_csv(output_path, index=False)\n",
    "print(f\"Combined data has been saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT USEFULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# Usage example:\n",
    "input_file = \"./data/nexxt_change_sales_listings_update.csv\"      # CSV with a column named \"URL\"\n",
    "output_file = './data/nexxt_change_sales_listings_udpate_branche.csv'\n",
    "def get_branche_text(url):\n",
    "    \"\"\"\n",
    "    Opens the URL in a Selenium-driven browser, \n",
    "    finds the 'Branche :' label, and extracts the associated text.\n",
    "    Returns the extracted text or an empty string if not found.\n",
    "    \"\"\"\n",
    "    # Initialize the Selenium driver (Chrome)\n",
    "    # If you already have chromedriver in PATH, you could do:\n",
    "    # driver = webdriver.Chrome()\n",
    "    # Otherwise, webdriver_manager will install the correct version:\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    driver= webdriver.Chrome(options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Method 1: Find <dt> containing text \"Branche\", then find sibling <dd>\n",
    "        # XPATH Explanation:\n",
    "        #   1) We look for any element whose text contains the word \"Branche\".\n",
    "        #   2) Then we get the immediately following sibling <dd>.\n",
    "        \n",
    "        # Some sites might have leading/trailing spaces or slightly different text (like \"Branche :\"),\n",
    "        # so we use 'contains(text(), \"Branche\")' rather than an exact match.\n",
    "        element = driver.find_element(\n",
    "            By.XPATH,\n",
    "            \"//*[contains(normalize-space(text()), 'Branche')]/following-sibling::dd[1]\"\n",
    "        )\n",
    "        branche_text =  element.text.strip()\n",
    "\n",
    "        return branche_text\n",
    "\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(f\"Could not find 'Branche :' text on page: {url}\")\n",
    "        return \"\"\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "with open(input_file, mode='r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        url = row['url'].strip()\n",
    "        print(f\"Processing: {url}\")\n",
    "        branche_text = get_branche_text(url)\n",
    "        results.append({\n",
    "            'url': url,\n",
    "            'branche': branche_text\n",
    "        })\n",
    "\n",
    "# Write results to the output CSV\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "    fieldnames = ['URL', 'Branche']\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for item in results:\n",
    "        writer.writerow(item)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Setup and Initialization\n",
    "# -----------------------------\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "german_stopwords = set(stopwords.words('german'))\n",
    "\n",
    "# Initialize spaCy German model\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Initialize Sentence Transformer Model\n",
    "# You can choose a suitable pre-trained model. 'paraphrase-multilingual-MiniLM-L12-v2' is effective for German.\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Parsing and Flattening NACE JSON with Hierarchical Descriptions\n",
    "# -----------------------------\n",
    "\n",
    "def extract_nace_codes(nace_list, parent_descriptions=[], parent_code=''):\n",
    "    \"\"\"\n",
    "    Recursively extract NACE codes with concatenated parent descriptions.\n",
    "    \n",
    "    Args:\n",
    "        nace_list (list): List of NACE code dictionaries.\n",
    "        parent_descriptions (list): List of parent descriptions.\n",
    "        parent_code (str): Concatenated parent codes.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries with 'code' and 'full_description'.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for item in nace_list:\n",
    "        code = item['code']\n",
    "        description = item['description']\n",
    "        full_code = f\"{parent_code}{code}\" if parent_code else code\n",
    "        # Concatenate parent descriptions\n",
    "        concatenated_description = ' '.join(parent_descriptions + [description])\n",
    "        records.append({'code': full_code, 'full_description': concatenated_description})\n",
    "        # Recursively extract child codes\n",
    "        if item.get('children'):\n",
    "            records.extend(extract_nace_codes(\n",
    "                item['children'],\n",
    "                parent_descriptions + [description],\n",
    "                parent_code=full_code + '.'\n",
    "            ))\n",
    "    return records\n",
    "\n",
    "# Load NACE codes from JSON file\n",
    "with open(nacecode_array_obj_du, 'r', encoding='utf-8') as file:\n",
    "    nace_data = json.load(file)\n",
    "\n",
    "# Extract and flatten NACE codes with hierarchical descriptions\n",
    "# nace_records = extract_nace_codes(nace_data)\n",
    "# Convert the dictionary to a DataFrame\n",
    "nace_df = pd.DataFrame.from_dict(nace_data, orient='index')\n",
    "\n",
    "# Reset the index to turn the NACE codes into a column\n",
    "nace_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename the 'index' column to 'nace_code'\n",
    "nace_df.rename(columns={'index': 'code'}, inplace=True)\n",
    "# nace_df['synonyms'] = nace_df['synonyms'].apply(', '.join)\n",
    "\n",
    "nace_df = pd.DataFrame(nace_data.items(), columns=['code', 'full_description'])\n",
    "# nace_df['full_description'] = nace_df[['description','synonyms']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Display first few NACE codes\n",
    "print(\"Flattened NACE Codes with Hierarchical Descriptions:\")\n",
    "print(nace_df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Loading and Preprocessing the Business Dataset\n",
    "# -----------------------------\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(dataFile, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"\\nSample Business Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Combine relevant text columns into a single 'combined_text' column\n",
    "data['combined_text'] = data[['title', 'description', 'long_description', 'branchen']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Text Preprocessing Function\n",
    "# -----------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing stopwords, lemmatizing, and removing punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ not in german_stopwords\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to business combined text\n",
    "data['processed_text'] = data['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Apply preprocessing to NACE full descriptions\n",
    "nace_df['processed_description'] = nace_df['full_description'].apply(preprocess_text)\n",
    "\n",
    "# Display preprocessed texts\n",
    "print(\"\\nPreprocessed Business Text:\")\n",
    "print(data['processed_text'].head())\n",
    "\n",
    "print(\"\\nPreprocessed NACE Descriptions:\")\n",
    "print(nace_df[['code', 'processed_description']].head())\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Generating Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "# Generate embeddings for NACE codes\n",
    "nace_embeddings = embedding_model.encode(nace_df['processed_description'].tolist(), convert_to_tensor=False, show_progress_bar=True)\n",
    "\n",
    "# Generate embeddings for business data\n",
    "business_embeddings = embedding_model.encode(data['processed_text'].tolist(), convert_to_tensor=False, show_progress_bar=True)\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "nace_embeddings = np.array(nace_embeddings)\n",
    "business_embeddings = np.array(business_embeddings)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Dimensionality Reduction (Optional)\n",
    "# -----------------------------\n",
    "\n",
    "# If you wish to reduce the dimensionality of embeddings (e.g., for visualization or to speed up computations),\n",
    "# you can use techniques like PCA, t-SNE, or UMAP. However, for similarity computations, it's often best to keep\n",
    "# embeddings in their original high-dimensional space.\n",
    "\n",
    "# Example: Using PCA to reduce to 100 dimensions\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# nace_embeddings_reduced = pca.fit_transform(nace_embeddings)\n",
    "# business_embeddings_reduced = pca.transform(business_embeddings)\n",
    "\n",
    "# For this script, we'll proceed without dimensionality reduction.\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Computing Similarity and Assigning NACE Codes\n",
    "# -----------------------------\n",
    "\n",
    "# Compute cosine similarity between each business embedding and all NACE embeddings\n",
    "# This can be memory-intensive for large datasets. Consider processing in batches if needed.\n",
    "\n",
    "# To optimize memory usage, process in smaller batches\n",
    "def assign_nace_codes(business_embeds, nace_embeds, nace_codes, nace_dec, batch_size=1000, similarity_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Assign NACE codes to business embeddings based on cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        business_embeds (np.array): Array of business embeddings.\n",
    "        nace_embeds (np.array): Array of NACE embeddings.\n",
    "        nace_codes (list): List of NACE codes corresponding to nace_embeds.\n",
    "        nace_dec (list): List of NACE descriptions corresponding to nace_embeds.\n",
    "        batch_size (int): Number of samples to process in each batch.\n",
    "        similarity_threshold (float): Minimum similarity score to consider a match.\n",
    "        \n",
    "    Returns:\n",
    "        list: Assigned NACE codes for each business.\n",
    "        list: Corresponding similarity scores.\n",
    "        list: Corresponding NACE descriptions.\n",
    "    \"\"\"\n",
    "    assigned_codes = []\n",
    "    similarity_scores = []\n",
    "    nace_des = []\n",
    "    num_business = business_embeds.shape[0]\n",
    "    \n",
    "    for start in range(0, num_business, batch_size):\n",
    "        end = min(start + batch_size, num_business)\n",
    "        batch = business_embeds[start:end]\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(batch, nace_embeds)\n",
    "        \n",
    "        # Process each business in the batch\n",
    "        for sim in similarity:\n",
    "            # Get indices of similarities above the threshold\n",
    "            above_threshold_indices = np.where(sim >= similarity_threshold)[0]\n",
    "            # Sort indices by similarity score in descending order\n",
    "            sorted_indices = above_threshold_indices[np.argsort(-sim[above_threshold_indices])]\n",
    "            # Select top 3 indices\n",
    "            top_indices = sorted_indices[:3]\n",
    "            \n",
    "            if len(top_indices) > 0:\n",
    "                assigned_codes.append([nace_codes[idx] for idx in top_indices])\n",
    "                similarity_scores.append([sim[idx] for idx in top_indices])\n",
    "                nace_des.append([nace_dec[idx] for idx in top_indices])\n",
    "            else:\n",
    "                assigned_codes.append(['Unclassified'])\n",
    "                similarity_scores.append([0])\n",
    "                nace_des.append(['Unclassified'])\n",
    "        \n",
    "        print(f\"Processed batch {start} to {end} of {num_business}\")\n",
    "    \n",
    "    return assigned_codes, similarity_scores, nace_des\n",
    "\n",
    "# Assign NACE codes to business data\n",
    "assigned_nace_codes, similarity_scores, nace_des = assign_nace_codes(\n",
    "    business_embeddings, \n",
    "    nace_embeddings, \n",
    "    nace_df['code'].tolist(), \n",
    "    nace_df['processed_description'].tolist(), \n",
    "    batch_size=1000, \n",
    "    similarity_threshold=0.1  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Add the assigned codes and similarity scores to the dataframe\n",
    "data['predicted_nace_code'] = assigned_nace_codes\n",
    "data['similarity_score'] = similarity_scores\n",
    "data['nace_des'] = nace_des\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Saving the Labeled Dataset\n",
    "# -----------------------------\n",
    "\n",
    "# Save the labeled data to a new CSV file\n",
    "# data.to_csv('labeled_data_with_nace.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\nNACE Code assignment completed and saved to 'labeled_data_with_nace.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_text'].to_csv('preprocessing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = SnowballStemmer('german')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Load NACE codes\n",
    "def load_nace_codes(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes\n",
    "\n",
    "# Create embeddings\n",
    "def create_embeddings(texts, model):\n",
    "    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Map NACE code\n",
    "def map_nace_code(text, nace_codes, nace_embeddings, model):\n",
    "    text_embedding = create_embeddings([text], model)[0]\n",
    "    similarities = cosine_similarity([text_embedding], nace_embeddings)[0]\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    return list(nace_codes.keys())[best_match_idx]\n",
    "\n",
    "def add_nace_codes(df, nace_codes, nace_embeddings, model):\n",
    "    df['nace_code'] = df.apply(\n",
    "        lambda row: map_nace_code(\n",
    "            preprocess_text(' '.join([\n",
    "                str(row.get('title', '')),\n",
    "                str(row.get('description', '')),\n",
    "                str(row.get('long_description', '')),\n",
    "                str(row.get('branchen', ''))\n",
    "            ])), nace_codes, nace_embeddings, model\n",
    "        ), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Map NACE codes to DataFrame\n",
    "def map_nace_codes(df, nace_codes, nace_embeddings, model):\n",
    "    df = add_nace_codes(df, nace_codes, nace_embeddings, model)\n",
    "    df['nace_description'] = df['nace_code'].apply(lambda code: nace_codes.get(code, \"\"))\n",
    "    return df\n",
    "\n",
    "# Load sample datasets\n",
    "sellers_df = pd.read_csv(dataFile)\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Load and preprocess NACE codes\n",
    "nace_codes = load_nace_codes(nacecode_array_obj_du)\n",
    "print(\"ðŸš€ ~ nace_codes:\", nace_codes)\n",
    "\n",
    "# The values in nace_codes are already the descriptions, so we can use them directly\n",
    "nace_descriptions = [preprocess_text(description) for description in nace_codes.values()]\n",
    "\n",
    "# Create embeddings for NACE descriptions\n",
    "nace_embeddings = create_embeddings(nace_descriptions, model)\n",
    "\n",
    "sellers_df['post_precessed_text'] = sellers_df.apply( lambda row: preprocess_text(' '.join([str(row.get('title', '')), str(row.get('description', '')), str(row.get('long_description', '')), str(row.get('branchen', ''))])), axis=1)\n",
    "sellers_df = map_nace_codes(sellers_df, nace_codes, nace_embeddings, model)\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"\\nSellers DataFrame with NACE codes:\")\n",
    "print(sellers_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
