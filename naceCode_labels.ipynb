{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile =  './data/nexxt_change_sales_listings_geocoded_short_test.csv' \n",
    "sales_file_nace =  './data/nexxt_change_sales_listings_geocoded_nace.csv' \n",
    "buyer_file_nace =  './data/nexxt_change_purchase_listings_geocoded_nace.csv' \n",
    "nacecode_josn =  './data/nace_codes.json' \n",
    "nacecode_array_josn =  './data/nace_codes_array.json' \n",
    "nacecode_array_obj =  './data/nace_codes_object.json' \n",
    "nacecode_array_obj_ext =  './data/nace_codes_object_ext.json' \n",
    "nacecode_array_obj_du =  './data/nace_codes_object_du.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Setup and Initialization\n",
    "# -----------------------------\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "german_stopwords = set(stopwords.words('german'))\n",
    "\n",
    "# Initialize spaCy German model\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Initialize Sentence Transformer Model\n",
    "# You can choose a suitable pre-trained model. 'paraphrase-multilingual-MiniLM-L12-v2' is effective for German.\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Parsing and Flattening NACE JSON with Hierarchical Descriptions\n",
    "# -----------------------------\n",
    "\n",
    "def extract_nace_codes(nace_list, parent_descriptions=[], parent_code=''):\n",
    "    \"\"\"\n",
    "    Recursively extract NACE codes with concatenated parent descriptions.\n",
    "    \n",
    "    Args:\n",
    "        nace_list (list): List of NACE code dictionaries.\n",
    "        parent_descriptions (list): List of parent descriptions.\n",
    "        parent_code (str): Concatenated parent codes.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries with 'code' and 'full_description'.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for item in nace_list:\n",
    "        code = item['code']\n",
    "        description = item['description']\n",
    "        full_code = f\"{parent_code}{code}\" if parent_code else code\n",
    "        # Concatenate parent descriptions\n",
    "        concatenated_description = ' '.join(parent_descriptions + [description])\n",
    "        records.append({'code': full_code, 'full_description': concatenated_description})\n",
    "        # Recursively extract child codes\n",
    "        if item.get('children'):\n",
    "            records.extend(extract_nace_codes(\n",
    "                item['children'],\n",
    "                parent_descriptions + [description],\n",
    "                parent_code=full_code + '.'\n",
    "            ))\n",
    "    return records\n",
    "\n",
    "# Load NACE codes from JSON file\n",
    "with open(nacecode_array_obj_du, 'r', encoding='utf-8') as file:\n",
    "    nace_data = json.load(file)\n",
    "\n",
    "# Extract and flatten NACE codes with hierarchical descriptions\n",
    "# nace_records = extract_nace_codes(nace_data)\n",
    "# Convert the dictionary to a DataFrame\n",
    "nace_df = pd.DataFrame.from_dict(nace_data, orient='index')\n",
    "\n",
    "# Reset the index to turn the NACE codes into a column\n",
    "nace_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename the 'index' column to 'nace_code'\n",
    "nace_df.rename(columns={'index': 'code'}, inplace=True)\n",
    "# nace_df['synonyms'] = nace_df['synonyms'].apply(', '.join)\n",
    "\n",
    "nace_df = pd.DataFrame(nace_data.items(), columns=['code', 'full_description'])\n",
    "# nace_df['full_description'] = nace_df[['description','synonyms']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Display first few NACE codes\n",
    "print(\"Flattened NACE Codes with Hierarchical Descriptions:\")\n",
    "print(nace_df.head())\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Loading and Preprocessing the Business Dataset\n",
    "# -----------------------------\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv(dataFile, delimiter=',', encoding='utf-8')\n",
    "\n",
    "# Display first few rows of the dataset\n",
    "print(\"\\nSample Business Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Combine relevant text columns into a single 'combined_text' column\n",
    "data['combined_text'] = data[['title', 'description', 'long_description', 'branchen']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Text Preprocessing Function\n",
    "# -----------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by lowercasing, removing stopwords, lemmatizing, and removing punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and not token.is_punct and token.lemma_ not in german_stopwords\n",
    "    ]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to business combined text\n",
    "data['processed_text'] = data['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Apply preprocessing to NACE full descriptions\n",
    "nace_df['processed_description'] = nace_df['full_description'].apply(preprocess_text)\n",
    "\n",
    "# Display preprocessed texts\n",
    "print(\"\\nPreprocessed Business Text:\")\n",
    "print(data['processed_text'].head())\n",
    "\n",
    "print(\"\\nPreprocessed NACE Descriptions:\")\n",
    "print(nace_df[['code', 'processed_description']].head())\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Generating Embeddings\n",
    "# -----------------------------\n",
    "\n",
    "# Generate embeddings for NACE codes\n",
    "nace_embeddings = embedding_model.encode(nace_df['processed_description'].tolist(), convert_to_tensor=False, show_progress_bar=True)\n",
    "\n",
    "# Generate embeddings for business data\n",
    "business_embeddings = embedding_model.encode(data['processed_text'].tolist(), convert_to_tensor=False, show_progress_bar=True)\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "nace_embeddings = np.array(nace_embeddings)\n",
    "business_embeddings = np.array(business_embeddings)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Dimensionality Reduction (Optional)\n",
    "# -----------------------------\n",
    "\n",
    "# If you wish to reduce the dimensionality of embeddings (e.g., for visualization or to speed up computations),\n",
    "# you can use techniques like PCA, t-SNE, or UMAP. However, for similarity computations, it's often best to keep\n",
    "# embeddings in their original high-dimensional space.\n",
    "\n",
    "# Example: Using PCA to reduce to 100 dimensions\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# nace_embeddings_reduced = pca.fit_transform(nace_embeddings)\n",
    "# business_embeddings_reduced = pca.transform(business_embeddings)\n",
    "\n",
    "# For this script, we'll proceed without dimensionality reduction.\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Computing Similarity and Assigning NACE Codes\n",
    "# -----------------------------\n",
    "\n",
    "# Compute cosine similarity between each business embedding and all NACE embeddings\n",
    "# This can be memory-intensive for large datasets. Consider processing in batches if needed.\n",
    "\n",
    "# To optimize memory usage, process in smaller batches\n",
    "def assign_nace_codes(business_embeds, nace_embeds, nace_codes, nace_dec, batch_size=1000, similarity_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Assign NACE codes to business embeddings based on cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        business_embeds (np.array): Array of business embeddings.\n",
    "        nace_embeds (np.array): Array of NACE embeddings.\n",
    "        nace_codes (list): List of NACE codes corresponding to nace_embeds.\n",
    "        nace_dec (list): List of NACE descriptions corresponding to nace_embeds.\n",
    "        batch_size (int): Number of samples to process in each batch.\n",
    "        similarity_threshold (float): Minimum similarity score to consider a match.\n",
    "        \n",
    "    Returns:\n",
    "        list: Assigned NACE codes for each business.\n",
    "        list: Corresponding similarity scores.\n",
    "        list: Corresponding NACE descriptions.\n",
    "    \"\"\"\n",
    "    assigned_codes = []\n",
    "    similarity_scores = []\n",
    "    nace_des = []\n",
    "    num_business = business_embeds.shape[0]\n",
    "    \n",
    "    for start in range(0, num_business, batch_size):\n",
    "        end = min(start + batch_size, num_business)\n",
    "        batch = business_embeds[start:end]\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = cosine_similarity(batch, nace_embeds)\n",
    "        \n",
    "        # Process each business in the batch\n",
    "        for sim in similarity:\n",
    "            # Get indices of similarities above the threshold\n",
    "            above_threshold_indices = np.where(sim >= similarity_threshold)[0]\n",
    "            # Sort indices by similarity score in descending order\n",
    "            sorted_indices = above_threshold_indices[np.argsort(-sim[above_threshold_indices])]\n",
    "            # Select top 3 indices\n",
    "            top_indices = sorted_indices[:3]\n",
    "            \n",
    "            if len(top_indices) > 0:\n",
    "                assigned_codes.append([nace_codes[idx] for idx in top_indices])\n",
    "                similarity_scores.append([sim[idx] for idx in top_indices])\n",
    "                nace_des.append([nace_dec[idx] for idx in top_indices])\n",
    "            else:\n",
    "                assigned_codes.append(['Unclassified'])\n",
    "                similarity_scores.append([0])\n",
    "                nace_des.append(['Unclassified'])\n",
    "        \n",
    "        print(f\"Processed batch {start} to {end} of {num_business}\")\n",
    "    \n",
    "    return assigned_codes, similarity_scores, nace_des\n",
    "\n",
    "# Assign NACE codes to business data\n",
    "assigned_nace_codes, similarity_scores, nace_des = assign_nace_codes(\n",
    "    business_embeddings, \n",
    "    nace_embeddings, \n",
    "    nace_df['code'].tolist(), \n",
    "    nace_df['processed_description'].tolist(), \n",
    "    batch_size=1000, \n",
    "    similarity_threshold=0.1  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Add the assigned codes and similarity scores to the dataframe\n",
    "data['predicted_nace_code'] = assigned_nace_codes\n",
    "data['similarity_score'] = similarity_scores\n",
    "data['nace_des'] = nace_des\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Saving the Labeled Dataset\n",
    "# -----------------------------\n",
    "\n",
    "# Save the labeled data to a new CSV file\n",
    "# data.to_csv('labeled_data_with_nace.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\nNACE Code assignment completed and saved to 'labeled_data_with_nace.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_text'].to_csv('preprocessing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from nltk.stem import SnowballStemmer\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = SnowballStemmer('german')\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Load NACE codes\n",
    "def load_nace_codes(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes\n",
    "\n",
    "# Create embeddings\n",
    "def create_embeddings(texts, model):\n",
    "    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Map NACE code\n",
    "def map_nace_code(text, nace_codes, nace_embeddings, model):\n",
    "    text_embedding = create_embeddings([text], model)[0]\n",
    "    similarities = cosine_similarity([text_embedding], nace_embeddings)[0]\n",
    "    best_match_idx = np.argmax(similarities)\n",
    "    return list(nace_codes.keys())[best_match_idx]\n",
    "\n",
    "def add_nace_codes(df, nace_codes, nace_embeddings, model):\n",
    "    df['nace_code'] = df.apply(\n",
    "        lambda row: map_nace_code(\n",
    "            preprocess_text(' '.join([\n",
    "                str(row.get('title', '')),\n",
    "                str(row.get('description', '')),\n",
    "                str(row.get('long_description', '')),\n",
    "                str(row.get('branchen', ''))\n",
    "            ])), nace_codes, nace_embeddings, model\n",
    "        ), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Map NACE codes to DataFrame\n",
    "def map_nace_codes(df, nace_codes, nace_embeddings, model):\n",
    "    df = add_nace_codes(df, nace_codes, nace_embeddings, model)\n",
    "    df['nace_description'] = df['nace_code'].apply(lambda code: nace_codes.get(code, \"\"))\n",
    "    return df\n",
    "\n",
    "# Load sample datasets\n",
    "sellers_df = pd.read_csv(dataFile)\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Load and preprocess NACE codes\n",
    "nace_codes = load_nace_codes(nacecode_array_obj_du)\n",
    "print(\"🚀 ~ nace_codes:\", nace_codes)\n",
    "\n",
    "# The values in nace_codes are already the descriptions, so we can use them directly\n",
    "nace_descriptions = [preprocess_text(description) for description in nace_codes.values()]\n",
    "\n",
    "# Create embeddings for NACE descriptions\n",
    "nace_embeddings = create_embeddings(nace_descriptions, model)\n",
    "\n",
    "sellers_df['post_precessed_text'] = sellers_df.apply( lambda row: preprocess_text(' '.join([str(row.get('title', '')), str(row.get('description', '')), str(row.get('long_description', '')), str(row.get('branchen', ''))])), axis=1)\n",
    "sellers_df = map_nace_codes(sellers_df, nace_codes, nace_embeddings, model)\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(\"\\nSellers DataFrame with NACE codes:\")\n",
    "print(sellers_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ~ Sellers and NACE codes loaded.\n",
      "🚀 ~ Loaded SentenceTransformer model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3dcba52dec45b291691b5cfb052ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ~ Created embeddings for NACE descriptions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7caecf46b381464b852c9f83b7189935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 ~ Created embeddings for 'branchen' field.\n",
      "🚀 ~ Assigned preliminary NACE codes based on 'branchen' similarity.\n",
      "🚀 ~ Saved sellers data with preliminary NACE code assignments to 'sellers_with_preassigned_nace.csv'.\n",
      "\n",
      "Sample of NACE Code Assignments:\n",
      "                                            branchen assigned_nace_code  \\\n",
      "0  Dienstleistung; Baugewerbe; Grundstücks- und W...              G46.9   \n",
      "1                                                NaN            D35.1.4   \n",
      "2                                                NaN            D35.1.4   \n",
      "3  Verarbeitendes Gewerbe > Herstellung von elekt...            C28.9.2   \n",
      "4                                                NaN            D35.1.4   \n",
      "\n",
      "   assigned_nace_similarity nace_code  \n",
      "0                  0.591516     G46.9  \n",
      "1                  1.000000   D35.1.4  \n",
      "2                  1.000000   D35.1.4  \n",
      "3                  0.692239   C28.9.2  \n",
      "4                  1.000000   D35.1.4  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load SpaCy's German model with NER and POS capabilities\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_sm')\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Preprocess text function with NER and POS tagging\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # Remove URLs, emails, and long numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    # Remove non-alphabetic characters (keeping German characters)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Initialize stopwords and stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Retain nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities and include them\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}]\n",
    "    entities = [stemmer.stem(ent.lower()) for ent in entities if ent.lower() not in stop_words]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Load NACE codes\n",
    "def load_nace_codes(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes\n",
    "\n",
    "# Create embeddings\n",
    "def create_embeddings(texts, model):\n",
    "    return model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Load Data\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# Filepaths (replace with your actual file paths)\n",
    "sellers_filepath = buyer_file_nace  # Path to your sellers CSV file\n",
    "nace_codes_filepath = nacecode_array_obj_du  # Path to your NACE codes JSON file\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"🚀 ~ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "# model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"🚀 ~ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# Preprocess NACE descriptions\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "nace_embeddings = create_embeddings(nace_descriptions, model)\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 ~ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# Preprocess 'branchen' field in sellers data\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Create embeddings for 'branchen'\n",
    "branchen_embeddings = create_embeddings(sellers_df['preprocessed_branchen'].tolist(), model)\n",
    "print(\"🚀 ~ Created embeddings for 'branchen' field.\")\n",
    "\n",
    "# Compute cosine similarity between 'branchen' embeddings and NACE embeddings\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "\n",
    "# Assign the NACE code with the highest similarity\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"🚀 ~ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optional: Set a similarity threshold to filter uncertain assignments\n",
    "similarity_threshold = 0.5\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] if row['assigned_nace_similarity'] >= similarity_threshold else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the preliminary assignments\n",
    "sellers_df.to_csv(sellers_filepath, index=False)\n",
    "print(\"🚀 ~ Saved sellers data with preliminary NACE code assignments to 'sellers_with_preassigned_nace.csv'.\")\n",
    "\n",
    "# Review the assignments\n",
    "print(\"\\nSample of NACE Code Assignments:\")\n",
    "print(sellers_df[['branchen', 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
