{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= 1. requirements matching  > 0.85\n",
    "            1. semantic analysis > 0.9\n",
    "                a. similarity keywords\n",
    "            2. Nace Code match > 1.0\n",
    "        2. Location matching\n",
    "            1. exact location matching\n",
    "            2. Geocoding matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shelve\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from geopy.distance import geodesic \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORTS\n",
    "dataDIR = './data/'\n",
    "originalSalesNexxtChangeData = f'{dataDIR}branche_nexxt_change_sales_listings_scrape.csv'\n",
    "dejunaPurchases = './data/dejuna_buyer_latest.csv'\n",
    "\n",
    "nacecode_josn =  './data/nace_codes.json' \n",
    "nacecode_array_josn =  './data/nace_codes_array.json' \n",
    "nacecode_array_obj =  './data/nace_codes_object.json' \n",
    "nacecode_array_obj_ext =  './data/nace_codes_object_ext.json' \n",
    "nacecode_array_obj_du =  './data/nace_codes_object_du.json'\n",
    " \n",
    "dataFile =  './data/nexxt_change_sales_listings_geocoded_short_test.csv' \n",
    "# sales_file_nace =  './data/nexxt_change_sales_listings_geocoded.csv' \n",
    "sales_file_brachen =  './data/branche_nexxt_change_sales_listings.csv' \n",
    "sales_file_nace =  './data/dub_listings_geo.csv'\n",
    "buyer_file_nace =  './data/nexxt_change_purchase_listings_geocoded.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load SpaCy's German model (for tokenization, NER, POS tagging)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_sm')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_sm')\n",
    "    nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. Preprocessing function (with German NER, POS, etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    - Lowercases the text.\n",
    "    - Removes URLs, emails, & large digit sequences.\n",
    "    - Filters out non-alphabetic chars except German Umlauts/ß.\n",
    "    - Uses SpaCy to keep only NOUN, PROPN, VERB tokens not in stopwords.\n",
    "    - Applies Snowball stemming on remaining tokens.\n",
    "    - Also includes certain named entities (ORG, PRODUCT, GPE).\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, emails, large numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    \n",
    "    # Keep only letters and German characters\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', '', text)\n",
    "    \n",
    "    # Compact multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Initialize German stopwords and Snowball stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Keep nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities (ORG, PRODUCT, GPE) and include them\n",
    "    entities = [\n",
    "        ent.text for ent in doc.ents \n",
    "        if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}\n",
    "    ]\n",
    "    # Stem and remove stopwords from entities\n",
    "    entities = [\n",
    "        stemmer.stem(ent.lower()) \n",
    "        for ent in entities \n",
    "        if ent.lower() not in stop_words\n",
    "    ]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states and cities.\"\"\"\n",
    "    locations = set()\n",
    "    german_states = {\n",
    "        'baden-württemberg', 'bayern', 'berlin', 'brandenburg', 'bremen',\n",
    "        'hamburg', 'hessen', 'mecklenburg-vorpommern', 'niedersachsen',\n",
    "        'nordrhein-westfalen', 'rheinland-pfalz', 'saarland', 'sachsen',\n",
    "        'sachsen-anhalt', 'schleswig-holstein', 'thüringen'\n",
    "    }\n",
    "\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split on common delimiters\n",
    "        parts = re.split(r'[>/\\n]\\s*', location)\n",
    "        split_locations = []\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.strip().lower()\n",
    "            if part:\n",
    "                # Further split by space if multiple states are concatenated\n",
    "                words = part.split()\n",
    "                temp = []\n",
    "                current = \"\"\n",
    "                for word in words:\n",
    "                    if word.lower() in german_states:\n",
    "                        if current:\n",
    "                            temp.append(current.strip())\n",
    "                        current = word\n",
    "                    else:\n",
    "                        current += \" \" + word if current else word\n",
    "                if current:\n",
    "                    temp.append(current.strip())\n",
    "                split_locations.extend(temp)\n",
    "\n",
    "        for loc in split_locations:\n",
    "            loc = loc.strip().lower()\n",
    "            if loc:\n",
    "                if loc in german_states:\n",
    "                    locations.add(loc.title())  # Capitalize for better geocoding\n",
    "                else:\n",
    "                    # Clean up common prefixes like \"region\"\n",
    "                    clean_part = re.sub(r'^region\\s+', '', loc)\n",
    "                    if clean_part:\n",
    "                        locations.add(clean_part.title())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return locations\n",
    "\n",
    "\n",
    "# def _extract_location_parts(location):\n",
    "#     \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "#     locations = set()\n",
    "\n",
    "#     if not location or not isinstance(location, str):\n",
    "#         return locations\n",
    "\n",
    "#     try:\n",
    "#         # Split location string by \" > \", handling the hierarchical structure\n",
    "#         parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "#         for part in parts:\n",
    "#             part = part.split(\">\")\n",
    "#             locations.add(part[-1].strip())\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "#     return list(locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A',\n",
       " 'Augsburg',\n",
       " 'B',\n",
       " 'Bayern',\n",
       " 'Berlin',\n",
       " 'Brandenburg',\n",
       " 'C',\n",
       " 'Dresden',\n",
       " 'Frankfurt Am Main',\n",
       " 'Hamburg',\n",
       " 'Hannover',\n",
       " 'Hessen',\n",
       " 'Leipzig',\n",
       " 'München',\n",
       " 'Niedersachsen',\n",
       " 'Nürnberg',\n",
       " 'Sachsen'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _extract_location_parts('''Sachsen / Leipzig / Leipzig, Stadt''')\n",
    "_extract_location_parts('''Berlin\n",
    "Sachsen > Leipzig\n",
    "Sachsen > Dresden\n",
    "Brandenburg\n",
    "Niedersachsen > Hannover\n",
    "Hessen > Frankfurt am Main\n",
    "                        Hamburg\n",
    "                        Bayern > München\n",
    "                        Bayern > Nürnberg\n",
    "                        Bayern > Augsburg\n",
    "                        A\n",
    "                        B > C''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Load NACE codes from JSON\n",
    "# -------------------------------------------------------------------------\n",
    "def load_nace_codes(filepath):\n",
    "    \"\"\"\n",
    "    Expects a JSON file where keys = NACE code, values = textual descriptions.\n",
    "    Example:\n",
    "      {\n",
    "        \"01.1\": \"Growing of non-perennial crops\",\n",
    "        \"01.2\": \"Growing of perennial crops\",\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings with sentence-transformers\n",
    "# -------------------------------------------------------------------------\n",
    "def get_embedding_batch(texts, model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode texts in batches to optimize memory usage.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, \n",
    "                              convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embeddings.astype('float32')  # Use float32 to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = pd.read_csv(sellers_filepath) \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"🚀 Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"🚀 Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "#     lambda x: ' '.join(x.split('>'))\n",
    "# )\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"🚀 Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"🚀 Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"🚀 Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another NACE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings using Hugging Face\n",
    "# -------------------------------------------------------------------------\n",
    "def create_hf_embeddings(texts, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "            embeddings.append(cls_embedding.squeeze().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load Sellers and NACE Data\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "# Filepaths (update to your actual paths)\n",
    "# sellers_filepath = './data/dejuna_buyer_latest.csv'\n",
    "sellers_filepath = originalSalesNexxtChangeData\n",
    "nace_codes_filepath = './data/nace_codes_object_du.json'\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"🚀 Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize Hugging Face model and tokenizer\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"🚀 Loaded Hugging Face model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "nace_embeddings = create_hf_embeddings(nace_descriptions, tokenizer, model)\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#  lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "branchen_embeddings = create_hf_embeddings(sellers_df['preprocessed_branchen'].tolist(), tokenizer, model)\n",
    "print(\"🚀 Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "\n",
    "similarity_threshold = 0.7\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] if row['assigned_nace_similarity'] >= similarity_threshold else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_hf_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"🚀 Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[[ 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEOCODING LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 20:58:36,799 - INFO - Extracting locations from buyers dataframe...\n",
      "2025-01-19 20:58:36,800 - INFO - Extracting locations from sellers dataframe...\n",
      "2025-01-19 20:58:36,805 - INFO - Total unique locations found: 419\n",
      "2025-01-19 20:58:36,834 - INFO - Geocoding location: Freiburg im Breisgau\n",
      "2025-01-19 20:58:37,164 - INFO - Geocoded Freiburg im Breisgau: (47.9960901, 7.8494005)\n",
      "2025-01-19 20:58:37,167 - INFO - Geocoding location: Landsberg am Lech\n",
      "2025-01-19 20:58:38,263 - INFO - Geocoded Landsberg am Lech: (48.0497474, 10.8768728)\n",
      "2025-01-19 20:58:38,265 - INFO - Geocoding location: Offenbach am Main\n",
      "2025-01-19 20:58:39,186 - INFO - Geocoded Offenbach am Main: (50.1055002, 8.7610698)\n",
      "2025-01-19 20:58:39,187 - INFO - Geocoding location: Rüdesheim am Rhein\n",
      "2025-01-19 20:58:40,208 - INFO - Geocoded Rüdesheim am Rhein: (49.9789358, 7.92339)\n",
      "2025-01-19 20:58:40,209 - INFO - Geocoding location: Weiden i.d.OPf.\n",
      "2025-01-19 20:58:41,163 - INFO - Geocoded Weiden i.d.OPf.: (49.6752749, 12.1631636)\n",
      "2025-01-19 20:58:41,163 - INFO - Geocoding location: Landau in der Pfalz\n",
      "2025-01-19 20:58:42,175 - INFO - Geocoded Landau in der Pfalz: (49.1982825, 8.1123441)\n",
      "2025-01-19 20:58:42,176 - INFO - Geocoding location: Neustadt an der Weinstraße\n",
      "2025-01-19 20:58:43,181 - INFO - Geocoded Neustadt an der Weinstraße: (49.3539802, 8.1350021)\n",
      "2025-01-19 20:58:43,182 - INFO - Geocoding location: Pfaffenhofen a.d.Ilm\n",
      "2025-01-19 20:58:44,185 - INFO - Geocoded Pfaffenhofen a.d.Ilm: (48.5296743, 11.5084954)\n",
      "2025-01-19 20:58:44,187 - INFO - Geocoding location: Mühldorf a.Inn\n",
      "2025-01-19 20:58:45,174 - INFO - Geocoded Mühldorf a.Inn: (48.2405007, 12.5250991)\n",
      "2025-01-19 20:58:45,176 - INFO - Geocoding location: Frankfurt am Main\n",
      "2025-01-19 20:58:46,191 - INFO - Geocoded Frankfurt am Main: (50.1106444, 8.6820917)\n",
      "2025-01-19 20:58:46,192 - INFO - Geocoding location: Region Hannover\n",
      "2025-01-19 20:58:47,174 - INFO - Geocoded Region Hannover: (52.4026852, 9.801197840611401)\n",
      "2025-01-19 20:58:47,174 - INFO - Geocoding location: Neustadt a.d.Waldnaab\n",
      "2025-01-19 20:58:48,193 - INFO - Geocoded Neustadt a.d.Waldnaab: (49.7314121, 12.1732938)\n",
      "2025-01-19 20:58:48,194 - INFO - Geocoding location: Brandenburg an der Havel\n",
      "2025-01-19 20:58:49,180 - INFO - Geocoded Brandenburg an der Havel: (52.4108261, 12.5497933)\n",
      "2025-01-19 20:58:49,182 - INFO - Geocoding location: Ludwigshafen am Rhein\n",
      "2025-01-19 20:58:50,166 - INFO - Geocoded Ludwigshafen am Rhein: (49.4704113, 8.4381568)\n",
      "2025-01-19 20:58:50,167 - INFO - Geocoding location: Mülheim an der Ruhr\n",
      "2025-01-19 20:58:51,255 - INFO - Geocoded Mülheim an der Ruhr: (51.4272925, 6.8829192)\n",
      "2025-01-19 20:58:51,255 - INFO - Geocoding location: Neumarkt i.d.OPf.\n",
      "2025-01-19 20:58:52,157 - INFO - Geocoded Neumarkt i.d.OPf.: (49.279624, 11.4594662)\n",
      "2025-01-19 20:58:52,217 - INFO - Saving updated sellers dataframe...\n",
      "2025-01-19 20:58:52,362 - INFO - Geocoding process completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Saved sellers data with assigned geocodes to: ./data/dejuna_buyer_latest_hf_nace_geocoded.csv\n",
      " ./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_all_unique_locations(buyers_df, sellers_df):\n",
    "    \"\"\"Extract all unique locations from buyers and sellers dataframes.\"\"\"\n",
    "    unique_locations = set()\n",
    "\n",
    "    for df, name in [(buyers_df, 'buyers'), (sellers_df, 'sellers')]:\n",
    "        logging.info(f'Extracting locations from {name} dataframe...')\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            \n",
    "            unique_locations.update(locations)\n",
    "\n",
    "    logging.info(f'Total unique locations found: {len(unique_locations)}')\n",
    "    return unique_locations\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode unique locations with caching.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"buyer_seller_matching\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3, error_wait_seconds=10.0)\n",
    "\n",
    "    # Ensure cache directory exists\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if cache_dir and not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        for location in unique_locations:\n",
    "\n",
    "            if location in geocode_cache:\n",
    "                continue  # Already cached\n",
    "            try:\n",
    "                logging.info(f'Geocoding location: {location}')\n",
    "                loc = geocode(location + \", Germany\")\n",
    "                if loc:\n",
    "                    geocode_cache[location] = {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "                    logging.info(f'Geocoded {location}: ({loc.latitude}, {loc.longitude})')\n",
    "                else:\n",
    "                    geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "                    logging.warning(f'Geocoding failed for location: {location}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Geocoding error for location '{location}': {e}\")\n",
    "                geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "\n",
    "def update_dataframe_with_geocodes(df, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Add latitude and longitude columns to the dataframe based on locations.\"\"\"\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            lat_list = []\n",
    "            lon_list = []\n",
    "            for loc in locations:\n",
    "                geocode_info = geocode_cache.get(loc, {'latitude': None, 'longitude': None})\n",
    "                if geocode_info['latitude'] is not None and geocode_info['longitude'] is not None:\n",
    "                    lat_list.append(geocode_info['latitude'])\n",
    "                    lon_list.append(geocode_info['longitude'])\n",
    "                else:\n",
    "                    # If geocoding failed, append None\n",
    "                    lat_list.append(None)\n",
    "                    lon_list.append(None)\n",
    "            # Convert lists to JSON strings for CSV compatibility\n",
    "            latitudes.append(json.dumps(lat_list))\n",
    "            longitudes.append(json.dumps(lon_list))\n",
    "\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    return df\n",
    "\n",
    "# Paths to input and output files\n",
    "buyer_filepath = './data/dejuna_buyer_latest_hf_nace.csv'\n",
    "sellers_filepath = './data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv'\n",
    "\n",
    "cache_path = './geocode_cache.db'\n",
    "\n",
    "# Load buyer and seller datasets\n",
    "\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "unique_locations = get_all_unique_locations(buyers_df, sellers_df)\n",
    "# Geocode locations with caching\n",
    "geocode_locations(unique_locations, cache_path=cache_path)\n",
    "\n",
    "# Update dataframes with geocodes\n",
    "\n",
    "# logging.info('Updating sellers dataframe with geocodes...')\n",
    "sellers_df = update_dataframe_with_geocodes(sellers_df, cache_path=cache_path)\n",
    "buyers_df = update_dataframe_with_geocodes(buyers_df, cache_path=cache_path)\n",
    "\n",
    "# # Save updated dataframes to new CSV files\n",
    "logging.info('Saving updated sellers dataframe...')\n",
    "sellers_output_file = sellers_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "buyers_output_file = buyer_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "sellers_df.to_csv(sellers_output_file, index=False)\n",
    "buyers_df.to_csv(buyers_output_file, index=False)\n",
    "logging.info('Geocoding process completed successfully.')\n",
    "print(f\"🚀 Saved sellers data with assigned geocodes to: {buyers_output_file}\\n {sellers_output_file}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 12:19:59,937 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=Ausland%2C+Germany&format=json&limit=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for 'Sachsen': (50.9295798, 13.4585052)\n",
      "Coordinates for 'Ausland': (52.5448387, 13.4194785)\n",
      "Distance between 'Sachsen' and 'Ausland': 179.74 km\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "location1 = \"Sachsen\"\n",
    "location2 = \"Ausland\"\n",
    "#  {'Berlin',\n",
    "#  'Brandenburg',\n",
    "#  'Dresden',\n",
    "#  'Hannover',\n",
    "#  'Leipzig',\n",
    "#  'Niedersachsen',\n",
    "#  'Sachsen'}\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>long_description</th>\n",
       "      <th>source</th>\n",
       "      <th>contact details</th>\n",
       "      <th>Industrie</th>\n",
       "      <th>Sub-Industrie</th>\n",
       "      <th>preprocessed_branchen</th>\n",
       "      <th>assigned_nace_code</th>\n",
       "      <th>assigned_nace_similarity</th>\n",
       "      <th>nace_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>processed_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>28.03.24</td>\n",
       "      <td>Berlin\\nSachsen &gt; Leipzig\\nSachsen &gt; Dresden\\n...</td>\n",
       "      <td>Geschäftsführer sucht Dienstleistungsunternehmen</td>\n",
       "      <td>Christian ist Geschäftsführer im Logistik-Bere...</td>\n",
       "      <td>Gesucht wird ein etabliertes und profitables D...</td>\n",
       "      <td>dejuna</td>\n",
       "      <td>Dr. Christian Schneider\\nDeine E-Mail: christi...</td>\n",
       "      <td>Transport und Logistik</td>\n",
       "      <td>Logistikdienstleistungen, Spedition</td>\n",
       "      <td>transport logist logistikdienstleist spedition</td>\n",
       "      <td>H52.2.1</td>\n",
       "      <td>0.797184</td>\n",
       "      <td>H52.2.1</td>\n",
       "      <td>[52.8455492, 52.510885, 51.3406321, 52.3744779...</td>\n",
       "      <td>[13.2461296, 13.3989367, 12.3747329, 9.7385532...</td>\n",
       "      <td>[Brandenburg, Berlin, Leipzig, Hannover, Niede...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date                                           location  \\\n",
       "12  28.03.24  Berlin\\nSachsen > Leipzig\\nSachsen > Dresden\\n...   \n",
       "\n",
       "                                               title  \\\n",
       "12  Geschäftsführer sucht Dienstleistungsunternehmen   \n",
       "\n",
       "                                          description  \\\n",
       "12  Christian ist Geschäftsführer im Logistik-Bere...   \n",
       "\n",
       "                                     long_description  source  \\\n",
       "12  Gesucht wird ein etabliertes und profitables D...  dejuna   \n",
       "\n",
       "                                      contact details               Industrie  \\\n",
       "12  Dr. Christian Schneider\\nDeine E-Mail: christi...  Transport und Logistik   \n",
       "\n",
       "                          Sub-Industrie  \\\n",
       "12  Logistikdienstleistungen, Spedition   \n",
       "\n",
       "                             preprocessed_branchen assigned_nace_code  \\\n",
       "12  transport logist logistikdienstleist spedition            H52.2.1   \n",
       "\n",
       "    assigned_nace_similarity nace_code  \\\n",
       "12                  0.797184   H52.2.1   \n",
       "\n",
       "                                             latitude  \\\n",
       "12  [52.8455492, 52.510885, 51.3406321, 52.3744779...   \n",
       "\n",
       "                                            longitude  \\\n",
       "12  [13.2461296, 13.3989367, 12.3747329, 9.7385532...   \n",
       "\n",
       "                                   processed_location  \n",
       "12  [Brandenburg, Berlin, Leipzig, Hannover, Niede...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sales[sales['standort'].apply(lambda x: '''Berlin\n",
    "# Sachsen > Leipzig\n",
    "# Sachsen > Dresden\n",
    "# Brandenburg\n",
    "# Niedersachsen > Hannover''' in x)];\n",
    "\n",
    "purchase[purchase['location'].apply(lambda x: '''Berlin\n",
    "Sachsen > Leipzig\n",
    "Sachsen > Dresden\n",
    "Brandenburg\n",
    "Niedersachsen > Hannover''' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[10:30]\n",
    "purchase = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')[10:30]\n",
    "\n",
    "\n",
    "sales['processed_location'] = sales['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "purchase['processed_location'] = purchase['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "\n",
    "# # Check if any element of sales processed_location is in purchase processed_location\n",
    "purchase['latitude'] = purchase['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "purchase['longitude'] = purchase['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "sales['latitude'] = sales['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sales['longitude'] = sales['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "\n",
    "def match_locations(sales_locations, purchase_locations):\n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "\n",
    "# Create a dataframe to store matched locations\n",
    "matched_locations = []\n",
    "\n",
    "for idx, row in sales.iterrows():\n",
    "    for idx2, row2 in purchase.iterrows():\n",
    "        if match_locations(row.processed_location, row2.processed_location):\n",
    "            matched_locations.append({\n",
    "                'sales_id': idx,\n",
    "                'sales_location': row.location,\n",
    "                'sales_processed_location': row.processed_location,\n",
    "                'purchase_id': idx2,\n",
    "                'purchase_location': row2.location,\n",
    "                'sales_longitude': s_lon,\n",
    "                'sales_latitude': s_lat,\n",
    "                'purchase_longitude': p_lon,\n",
    "                'purchase_latitude': p_lat,\n",
    "                'purchase_processed_location': row2.processed_location,\n",
    "                'distance_km': None\n",
    "            })\n",
    "        else:\n",
    "            # Calculate distance between sales and purchase locations\n",
    "            sales_lat_lon = zip(row.latitude, row.longitude)\n",
    "            purchase_lat_lon = zip(row2.latitude, row2.longitude)\n",
    "            for s_lat, s_lon in sales_lat_lon:\n",
    "                for p_lat, p_lon in purchase_lat_lon:\n",
    "                    if s_lat is not None and s_lon is not None and p_lat is not None and p_lon is not None:\n",
    "                        print(f\"Calculating distance between ({s_lat}, {s_lon}) and ({p_lat}, {p_lon})\")\n",
    "                        distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                        print(f\"Distance: {distance}\")\n",
    "                        if distance <= 50:\n",
    "                            matched_locations.append({\n",
    "                                'sales_id': idx,\n",
    "                                'sales_processed_location': row.processed_location,\n",
    "                                'sales_location': row.location,\n",
    "                                'sales_longitude': s_lon,\n",
    "                                'sales_latitude': s_lat,\n",
    "                                'purchase_longitude': p_lon,\n",
    "                                'purchase_latitude': p_lat,\n",
    "                                'purchase_id': idx2,\n",
    "                                'purchase_location': row2.location,\n",
    "                                'purchase_processed_location': row2.processed_location,\n",
    "                                'distance_km': distance\n",
    "                            })\n",
    "matched_locations_df = pd.DataFrame(matched_locations)\n",
    "print(matched_locations_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Load synonyms CSV\n",
    "synonyms_df = pd.read_csv('./data/Updated_Keywords_and_Synonyms.csv')\n",
    "synonym_dict = {}\n",
    "for _, row in synonyms_df.iterrows():\n",
    "    keyword = row['Keyword'].lower()\n",
    "    synonyms = row.dropna().tolist()[1:]\n",
    "    synonyms = [syn.lower() for syn in synonyms]\n",
    "    synonym_dict[keyword] = synonyms\n",
    "\n",
    "# Define augmentation functions\n",
    "def extract_keywords(text, top_n=5):\n",
    "    # vectorizer = TfidfVectorizer(stop_words='german', max_features=top_n)\n",
    "    vectorizer = CountVectorizer(stop_words = german_stop_words) # Now use this in your pipeline\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "def augment_text_with_synonyms(text, top_n=5):\n",
    "    keywords = extract_keywords(text, top_n)\n",
    "    synonyms = []\n",
    "    for word in keywords:\n",
    "        synonyms.extend(synonym_dict.get(word, []))\n",
    "    synonyms = ' '.join(synonyms)\n",
    "    return f\"{text} {synonyms}\"\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text_de(text):\n",
    "    doc = nlp_de(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "def analyze_matches(matches_df, buyers_df, sellers_df):\n",
    "    \"\"\"Analyze matching results and print key metrics.\"\"\"\n",
    "    logging.info(\"\\n=== Matching Analysis ===\")\n",
    "    \n",
    "    total_buyers = len(buyers_df)\n",
    "    total_sellers = len(sellers_df)\n",
    "    total_matches = len(matches_df)\n",
    "    \n",
    "    logging.info(f\"Total buyers: {total_buyers}\")\n",
    "    logging.info(f\"Total sellers: {total_sellers}\") \n",
    "    logging.info(f\"Total matches found: {total_matches}\")\n",
    "    if total_buyers > 0:\n",
    "        logging.info(f\"Average matches per buyer: {total_matches/total_buyers:.2f}\")\n",
    "    else:\n",
    "        logging.info(\"No buyers to match against.\")\n",
    "\n",
    "    # Save top matches for manual review\n",
    "    top_matches = matches_df.head(10)\n",
    "    top_matches.to_csv('./matches/top_matches_for_review.csv', index=False)\n",
    "    logging.info(\"Saved top 10 matches for manual review\")\n",
    "    \n",
    "    return {\n",
    "        'total_matches': total_matches,\n",
    "        'matches_per_buyer': total_matches/total_buyers if total_buyers else 0,\n",
    "        'buyer_match_rate': len(matches_df['buyer_title'].unique())/total_buyers if total_buyers else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "\n",
    "# Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "logging.info('Loading the Sentence Transformer model...')\n",
    "# model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model {model_name}: {e}\")\n",
    "\n",
    "# Encode sellers' combined_text\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = get_embedding_batch(seller_texts, model, batch_size=64)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "# Set similarity threshold\n",
    "similarity_threshold = 0.78\n",
    "# Optional text length filter\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info('Starting matching process...')\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarity scores to all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "\n",
    "    # Indices above threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "\n",
    "    for seller_idx in matching_indices:\n",
    "        seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "        confidence_score = sim_scores[seller_idx]\n",
    "        if confidence_score < similarity_threshold:\n",
    "            continue\n",
    "\n",
    "        confidence_scores.append(confidence_score)\n",
    "\n",
    "        match = {\n",
    "            'buyer_date': buyer_row.get('date', ''),\n",
    "            'buyer_title': buyer_row.get('title', ''),\n",
    "            'buyer_description': buyer_row.get('description', ''),\n",
    "            'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "            'buyer_location': buyer_row.get('location', ''),\n",
    "            'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "            'seller_date': seller_row.get('date', ''),\n",
    "            'seller_title': seller_row.get('title', ''),\n",
    "            'seller_description': seller_row.get('description', ''),\n",
    "            'seller_long_description': seller_row.get('long_description', ''),\n",
    "            'seller_location': seller_row.get('location', ''),\n",
    "            'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "            \n",
    "            'similarity_score': confidence_score\n",
    "        }\n",
    "        matches.append(match)\n",
    "\n",
    "    # Progress logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "logging.info('Creating matches DataFrame...')\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Sort by confidence score\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "\n",
    "    # Save all matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    logging.info(f'Saved all matches: {len(matches_df)} records => {output_all}')\n",
    "\n",
    "    # Analyze results\n",
    "    metrics = analyze_matches(matches_df, buyers_df, sellers_df)\n",
    "    \n",
    "    # Optionally filter for high confidence\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= 0.95]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f'Saved high confidence matches: {len(high_conf_df)} records => {output_high_conf}')\n",
    "else:\n",
    "    logging.info('No matches found.')\n",
    "\n",
    "# Final memory cleanup\n",
    "del seller_embeddings\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 12:42:44,324 - INFO - Loading datasets...\n",
      "2025-01-21 12:42:44,447 - INFO - Preprocessing buyers' text fields...\n",
      "2025-01-21 12:42:45,212 - INFO - Preprocessing sellers' text fields...\n"
     ]
    }
   ],
   "source": [
    "def match_locations(sales_locations, purchase_locations):\n",
    "    \"\"\"\n",
    "    Check if any element of sales_locations is in purchase_locations.\n",
    "    \"\"\"\n",
    "    if(any(loc in purchase_locations for loc in sales_locations)): print(f'purchase_locations: {purchase_locations}, sales_locations: {sales_locations}, {any(loc in purchase_locations for loc in sales_locations)}')\n",
    "    \n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "\n",
    "# # Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Load Models\n",
    "# -----------------------------------\n",
    "logging.info(\"Loading SentenceTransformer and CrossEncoder models...\")\n",
    "# bi_encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer(\"xlm-roberta-base\")\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Encode Sellers' Text\n",
    "# -----------------------------------\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 6. Matching Parameters\n",
    "# -----------------------------------\n",
    "similarity_threshold = 0.60\n",
    "cross_encoder_threshold = 0.65\n",
    "top_n = 100  # Number of top candidates to re-rank per buyer\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info(\"Starting matching process...\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 7. Matching Loop\n",
    "# -----------------------------------\n",
    "matched_locations = []\n",
    "\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyers_df.iloc[i]['latitude']\n",
    "    buyer_longitudes = buyers_df.iloc[i]['longitude']\n",
    "    buyer_locations = buyers_df.iloc[i]['processed_location']\n",
    "\n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute cosine similarities with all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "    \n",
    "    # Get indices of sellers with similarity >= threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue  # No matches above threshold\n",
    "    \n",
    "    # Select top N matches based on similarity scores\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:top_n]]\n",
    "    \n",
    "    # Prepare pairs for cross-encoder\n",
    "    buyer_texts = [buyer_text] * len(top_indices)\n",
    "    seller_texts_top = [sellers_df.iloc[idx]['combined_text'] for idx in top_indices]\n",
    "    pairs = list(zip(buyer_texts, seller_texts_top))\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    cross_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Filter based on cross-encoder threshold\n",
    "    for seller_idx, cross_score in zip(top_indices, cross_scores):\n",
    "\n",
    "\n",
    "        if cross_score >= cross_encoder_threshold:\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['processed_location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            if match_locations(seller_locations, buyer_locations):\n",
    "                location_match = True\n",
    "            # else:\n",
    "            #     # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            #     for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "            #         for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "            #             if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "            #                 continue\n",
    "            #             distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "            #             if distance <= 50:  # 50 km threshold\n",
    "            #                 distance_km = distance\n",
    "            #                 location_match = True\n",
    "            #                 break\n",
    "            #         if location_match:\n",
    "            #             break\n",
    "            # if location_match:\n",
    "\n",
    "                match = {\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller_row.get('id', ''),\n",
    "                    'seller_date': seller_row.get('date', ''),\n",
    "                    'seller_title': seller_row.get('title', ''),\n",
    "                    'seller_description': seller_row.get('description', ''),\n",
    "                    'seller_long_description': seller_row.get('long_description', ''),\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'seller_source': seller_row.get('url', ''),\n",
    "                    'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "                    \n",
    "                    'similarity_score': cross_score,\n",
    "                    'distance_km': distance_km if distance_km else 'Within processed locations'\n",
    "\n",
    "                }\n",
    "                matches.append(match)\n",
    "                confidence_scores.append(cross_score)\n",
    "    \n",
    "    # Progress Logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 8. Create Matches DataFrame\n",
    "# -----------------------------------\n",
    "logging.info(\"Creating matches DataFrame...\")\n",
    "matches_df = pd.DataFrame(matches)\n",
    "        \n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "del seller_embeddings\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5256                          Handwerk > Gebäudereinigung\n",
      "5257                          Immobilien > Hausverwaltung\n",
      "5258    Handwerk > Bestattungsgewerbe, Bestattungsunte...\n",
      "5259                          Immobilien > Hausverwaltung\n",
      "5260             Handwerk > Heizung, Sanitär, Klima (SHK)\n",
      "5261                          Immobilien > Hausverwaltung\n",
      "5262                          Immobilien > Hausverwaltung\n",
      "5263    Handwerk > Bestattungsgewerbe, Bestattungsunte...\n",
      "5264    Gesundheits- und Sozialwesen > Gesundheitswese...\n",
      "5265    Gesundheits- und Sozialwesen > Gesundheitswese...\n",
      "Name: branchen, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read sellers and buyers files into dataframes\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "\n",
    "# Add origin column to both dataframes\n",
    "sellers_df['origin'] = 'seller'\n",
    "buyers_df['origin'] = 'buyer'\n",
    "buyers_df['branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} > {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Concatenate both dataframes\n",
    "combined_df = pd.concat([sellers_df, buyers_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE Labels for Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 20:04:56,995 - INFO - Use pytorch device_name: mps\n",
      "2025-01-16 20:04:56,996 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Sellers and NACE codes loaded.\n",
      "🚀 Loaded SentenceTransformer model: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201b94b6d9a54d66b6a1202d1439eacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 20:05:04,151 - INFO - Preprocessing buyers' text fields...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Created embeddings for NACE descriptions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b56ae9d97e64cdba10efc6b217d7a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Created embeddings for sellers' 'branchen' field.\n",
      "🚀 Assigned preliminary NACE codes based on 'branchen' similarity.\n",
      "🚀 Saved sellers data with assigned NACE codes to: buyers_sellers_combined_nace.csv\n",
      "\n",
      "Sample of NACE Code Assignments:\n",
      "  assigned_nace_code  assigned_nace_similarity   nace_code\n",
      "0              C24.1                  0.384563  Unassigned\n",
      "1                I56                  0.593983         I56\n",
      "2            O84.1.3                  0.508671     O84.1.3\n",
      "3                M73                  0.521514         M73\n",
      "4              C26.6                  0.583879       C26.6\n",
      "5              C23.7                  0.386995  Unassigned\n",
      "6                M73                  0.455993         M73\n",
      "7              G47.5                  0.439482       G47.5\n",
      "8              M70.1                  0.376641  Unassigned\n",
      "9                K65                  0.449326         K65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = combined_df \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"🚀 Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"🚀 Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "    lambda x: ' '.join(x.split('>'))\n",
    ")\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        # row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"🚀 Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"🚀 Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = \"buyers_sellers_combined_nace.csv\"\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"🚀 Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [buyer_id, buyer_date, buyer_title, buyer_description, buyer_long_description, buyer_location, buyer_nace_code, seller_id, seller_date, seller_title, seller_description, seller_long_description, seller_location, seller_source, seller_nace_code, similarity_score, distance_km, confidence_score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "temp1= pd.read_csv(\"./matches/nlp_business_all_matches_16_19-31.csv\")\n",
    "temp2= pd.read_csv(\"./matches/nlp_business_all_matches_15_13-11.csv\")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_df = pd.concat([temp1, temp2])\n",
    "# Get all duplicates based on specific columns\n",
    "# Get all rows that only exist once in the dataframe\n",
    "unique_rows = combined_df.drop_duplicates(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)\n",
    "duplicates = unique_rows[unique_rows.duplicated(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)]\n",
    "\n",
    "# unique_rows.to_csv('./matches/unique_rows.csv', index=False)\n",
    "# Display the unique rows\n",
    "# print(unique_rows)\n",
    "print(duplicates)\n",
    "# Display the duplicates\n",
    "# print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niedersachen\n",
      "celle\n",
      "niedersachsen\n",
      "goettingen\n",
      "Locations 1: {'Celle', 'Niedersachen'}\n",
      "Locations 2: {'Goettingen', 'Niedersachsen'}\n",
      "Distance between Celle and Goettingen: 120.84999834028793 KM\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def extract_locations(location_string):\n",
    "    \"\"\"\n",
    "    Extracts locations from a string with nested structure.\n",
    "    \n",
    "    Args:\n",
    "        location_string (str): Input string containing locations in nested format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted locations.\n",
    "    \"\"\"\n",
    "    locations = []\n",
    "\n",
    "    # Split the input into lines\n",
    "    lines = location_string.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Strip whitespace from the line\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Ignore empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # If the line contains '>', it indicates nested locations\n",
    "        if '>' in line:\n",
    "            # Extract the specific location after the last '>'\n",
    "            nested_location = line.split('>')[-1].strip()\n",
    "            locations.append(nested_location)\n",
    "        else:\n",
    "            # Append the standalone location\n",
    "            locations.append(line)\n",
    "\n",
    "    return locations\n",
    "\n",
    "def get_location_coordinates(location):\n",
    "    \"\"\"\n",
    "    Mock function to return coordinates for a given location.\n",
    "    Replace this with an actual geocoding API in production.\n",
    "\n",
    "    Args:\n",
    "        location (str): The location name.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location.\n",
    "    \"\"\"\n",
    "    # Mock coordinates for demonstration purposes\n",
    "    coordinates = {\n",
    "        \"Celle\": (52.6226, 10.0815),\n",
    "        \"Hannover\": (52.3759, 9.7320),\n",
    "        \"Karlsruhe\": (49.0069, 8.4037),\n",
    "        \"Mannheim\": (49.4875, 8.4660),\n",
    "        \"Goettingen\": (51.5413, 9.9158),\n",
    "    }\n",
    "    return coordinates.get(location, None)\n",
    "\n",
    "def locations_match(s1, s2):\n",
    "    \"\"\"\n",
    "    Checks if the locations extracted from two strings have any matches.\n",
    "    If no exact match, it calculates the distance between locations.\n",
    "\n",
    "    Args:\n",
    "        s1 (str): First input string containing locations.\n",
    "        s2 (str): Second input string containing locations.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if there are matching locations or distance < 50KM, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract locations from both strings\n",
    "    locations1 = set(_extract_location_parts(s1))\n",
    "    locations2 = set(_extract_location_parts(s2))\n",
    "\n",
    "    print(f\"Locations 1: {locations1}\")\n",
    "    print(f\"Locations 2: {locations2}\")\n",
    "    # Check for exact match first\n",
    "    if not locations1.isdisjoint(locations2):\n",
    "        return True\n",
    "\n",
    "    # If no exact match, calculate distances\n",
    "    for loc1 in locations1:\n",
    "        for loc2 in locations2:\n",
    "            coord1 = get_location_coordinates(loc1)\n",
    "            coord2 = get_location_coordinates(loc2)\n",
    "\n",
    "            if coord1 and coord2:\n",
    "                distance = geodesic(coord1, coord2).kilometers\n",
    "                print(f\"Distance between {loc1} and {loc2}: {distance} KM\")\n",
    "                if distance < 50:\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "s1 = \"Niedersachen > Celle\"\n",
    "s2 = \"Niedersachsen > Goettingen\"\n",
    "\n",
    "output = locations_match(s1, s2)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
