{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= 1. requirements matching  > 0.85\n",
    "            1. semantic analysis > 0.9\n",
    "                a. similarity keywords\n",
    "            2. Nace Code match > 1.0\n",
    "        2. Location matching\n",
    "            1. exact location matching\n",
    "            2. Geocoding matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shelve\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from geopy.distance import geodesic \n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORTS\n",
    "dataDIR = './data/'\n",
    "originalSalesNexxtChangeData = f'{dataDIR}branche_nexxt_change_sales_listings_scrape.csv'\n",
    "dejunaPurchases = './data/dejuna_buyer_latest.csv'\n",
    "\n",
    "nacecode_josn =  './data/nace_codes.json' \n",
    "nacecode_array_josn =  './data/nace_codes_array.json' \n",
    "nacecode_array_obj =  './data/nace_codes_object.json' \n",
    "nacecode_array_obj_ext =  './data/nace_codes_object_ext.json' \n",
    "nacecode_array_obj_du =  './data/nace_codes_object_du.json'\n",
    " \n",
    "dataFile =  './data/nexxt_change_sales_listings_geocoded_short_test.csv' \n",
    "# sales_file_nace =  './data/nexxt_change_sales_listings_geocoded.csv' \n",
    "sales_file_brachen =  './data/branche_nexxt_change_sales_listings.csv' \n",
    "sales_file_nace =  './data/dub_listings_geo.csv'\n",
    "buyer_file_nace =  './data/nexxt_change_purchase_listings_geocoded.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abbasm1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load SpaCy's German model (for tokenization, NER, POS tagging)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_lg')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_lg')\n",
    "    nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. Preprocessing function (with German NER, POS, etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    - Lowercases the text.\n",
    "    - Removes URLs, emails, & large digit sequences.\n",
    "    - Filters out non-alphabetic chars except German Umlauts/ÃŸ.\n",
    "    - Uses SpaCy to keep only NOUN, PROPN, VERB tokens not in stopwords.\n",
    "    - Applies Snowball stemming on remaining tokens.\n",
    "    - Also includes certain named entities (ORG, PRODUCT, GPE).\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, emails, large numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    \n",
    "    # Keep only letters and German characters\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s]', '', text)\n",
    "    \n",
    "    # Compact multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "\n",
    "    # # Initialize German stopwords and Snowball stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Keep nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities (ORG, PRODUCT, GPE) and include them\n",
    "    entities = [\n",
    "        ent.text for ent in doc.ents \n",
    "        if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}\n",
    "    ]\n",
    "    # Stem and remove stopwords from entities\n",
    "    entities = [\n",
    "        stemmer.stem(ent.lower()) \n",
    "        for ent in entities \n",
    "        if ent.lower() not in stop_words\n",
    "    ]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def split_compounds_regex(text):\n",
    "    \"\"\"\n",
    "    Basic German compound word splitting using regex rules.\n",
    "    Example: \"Kundenzufriedenheit\" â†’ [\"kunden\", \"zufriedenheit\"]\n",
    "    \"\"\"\n",
    "    # Split at common compound connectors (e.g., -s-, -en-, -n-)\n",
    "    parts = re.split(r'(s\\b|en\\b|n\\b|e\\b)(?=\\w{3,})', text)\n",
    "    return [p for p in parts if p and len(p) > 2]\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# Load SpaCy model CORRECTLY (without disabling components during initial load)\n",
    "nlp = spacy.load(\"de_core_news_lg\")  # Load first\n",
    "nlp.disable_pipes(\"parser\", \"ner\")   # Disable components AFTER loading\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Preprocesses German text for similarity tasks.\n",
    "    - Disables static vectors to avoid RuntimeError.\n",
    "    - Uses regex-based compound splitting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    custom_stopwords = {\"unternehmen\", \"firma\", \"dienstleistung\", \"kunde\"}\n",
    "    stop_words = STOP_WORDS.union(custom_stopwords)\n",
    "    \n",
    "    # Token processing (lemmatization + POS filtering)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    # Regex-based compound splitting (no external dependencies)\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and len(token.text) > 8:\n",
    "            parts = re.findall(r'\\b\\w{4,}(?=\\w{4,})', token.text)  # Split long nouns\n",
    "            compounds.extend([p.lower() for p in parts])\n",
    "    tokens.extend(compounds)\n",
    "    \n",
    "    # Filter short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# # Load SpaCy model CORRECTLY (without disabling components during initial load)\n",
    "# nlp = spacy.load(\"de_core_news_lg\")  # Load first\n",
    "# nlp.disable_pipes(\"parser\", \"ner\")   # Disable components AFTER loading\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Preprocesses German text for similarity tasks.\n",
    "    - Disables static vectors to avoid RuntimeError.\n",
    "    - Uses regex-based compound splitting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    custom_stopwords = {\"unternehmen\", \"firma\", \"dienstleistung\", \"kunde\"}\n",
    "    stop_words = STOP_WORDS.union(custom_stopwords)\n",
    "    \n",
    "    # Token processing (lemmatization + POS filtering)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    # Regex-based compound splitting (no external dependencies)\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and len(token.text) > 8:\n",
    "            parts = re.findall(r'\\b\\w{4,}(?=\\w{4,})', token.text)  # Split long nouns\n",
    "            compounds.extend([p.lower() for p in parts])\n",
    "    tokens.extend(compounds)\n",
    "    \n",
    "    # Filter short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_similarity(test_cases, nlp_model, embedding_model):\n",
    "    similarities = []\n",
    "    labels = []\n",
    "    \n",
    "    for text1, text2, is_similar in test_cases:\n",
    "        preprocessed1 = preprocess_text(text1, nlp_model)\n",
    "        preprocessed2 = preprocess_text(text2, nlp_model)\n",
    "        \n",
    "        embedding1 = embedding_model.encode(preprocessed1)\n",
    "        embedding2 = embedding_model.encode(preprocessed2)\n",
    "        \n",
    "        similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "        similarities.append(similarity)\n",
    "        labels.append(is_similar)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    threshold = 0.7\n",
    "    true_positives = np.sum((similarities > threshold) & (labels == True))\n",
    "    false_positives = np.sum((similarities > threshold) & (labels == False))\n",
    "    false_negatives = np.sum((similarities <= threshold) & (labels == True))\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives + 1e-8)  # Avoid division by zero\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    \n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n",
    "    print(f\"Avg Similarity (Matches): {np.mean(similarities[labels]):.2f}\")\n",
    "    print(f\"Avg Similarity (Non-Matches): {np.mean(similarities[~labels]):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    (\"Industriemaschinen\", \"Maschinenbau\", True),\n",
    "    (\"Kundenbetreuung\", \"Autoreparatur\", False),\n",
    "    (\"Logistikdienstleister\", \"Transportunternehmen\", True),\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "evaluate_similarity(test_cases, nlp, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _extract_location_parts(location):\n",
    "#     \"\"\"Extract and categorize location parts into states and cities.\"\"\"\n",
    "#     locations = set()\n",
    "#     german_states = {\n",
    "#         'baden-wÃ¼rttemberg', 'bayern', 'berlin', 'brandenburg', 'bremen',\n",
    "#         'hamburg', 'hessen', 'mecklenburg-vorpommern', 'niedersachsen',\n",
    "#         'nordrhein-westfalen', 'rheinland-pfalz', 'saarland', 'sachsen',\n",
    "#         'sachsen-anhalt', 'schleswig-holstein', 'thÃ¼ringen'\n",
    "#     }\n",
    "\n",
    "\n",
    "#     if not location or not isinstance(location, str):\n",
    "#         return locations\n",
    "\n",
    "#     try:\n",
    "#         # Split on common delimiters\n",
    "#         parts = re.split(r'[>/\\n]\\s*', location)\n",
    "#         split_locations = []\n",
    "\n",
    "#         for part in parts:\n",
    "#             part = part.strip().lower()\n",
    "#             if part:\n",
    "#                 # Further split by space if multiple states are concatenated\n",
    "#                 words = part.split()\n",
    "#                 temp = []\n",
    "#                 current = \"\"\n",
    "#                 for word in words:\n",
    "#                     if word.lower() in german_states:\n",
    "#                         if current:\n",
    "#                             temp.append(current.strip())\n",
    "#                         current = word\n",
    "#                     else:\n",
    "#                         current += \" \" + word if current else word\n",
    "#                 if current:\n",
    "#                     temp.append(current.strip())\n",
    "#                 split_locations.extend(temp)\n",
    "\n",
    "#         for loc in split_locations:\n",
    "#             loc = loc.strip().lower()\n",
    "#             if loc:\n",
    "#                 if loc in german_states:\n",
    "#                     locations.add(loc.title())  # Capitalize for better geocoding\n",
    "#                 else:\n",
    "#                     # Clean up common prefixes like \"region\"\n",
    "#                     clean_part = re.sub(r'^region\\s+', '', loc)\n",
    "#                     if clean_part:\n",
    "#                         locations.add(clean_part.title())\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "#     return locations\n",
    "\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "    locations = set()\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split location string by \" > \", handling the hierarchical structure\n",
    "        parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.split(\">\")\n",
    "            locations.add(part[-1].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return list(locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _extract_location_parts('''Sachsen / Leipzig / Leipzig, Stadt''')\n",
    "_extract_location_parts('''Berlin\n",
    "Sachsen > Leipzig\n",
    "Sachsen > Dresden\n",
    "Brandenburg\n",
    "Niedersachsen > Hannover\n",
    "Hessen > Frankfurt am Main\n",
    "                        Hamburg\n",
    "                        Bayern > MÃ¼nchen\n",
    "                        Bayern > NÃ¼rnberg\n",
    "                        Bayern > Augsburg\n",
    "                        A\n",
    "                        B > C''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Load NACE codes from JSON\n",
    "# -------------------------------------------------------------------------\n",
    "def load_nace_codes(filepath):\n",
    "    \"\"\"\n",
    "    Expects a JSON file where keys = NACE code, values = textual descriptions.\n",
    "    Example:\n",
    "      {\n",
    "        \"01.1\": \"Growing of non-perennial crops\",\n",
    "        \"01.2\": \"Growing of perennial crops\",\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings with sentence-transformers\n",
    "# -------------------------------------------------------------------------\n",
    "def get_embedding_batch(texts, model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode texts in batches to optimize memory usage.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, \n",
    "                              convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embeddings.astype('float32')  # Use float32 to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = pd.read_csv(sellers_filepath) \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"ðŸš€ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "#     lambda x: ' '.join(x.split('>'))\n",
    "# )\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"ðŸš€ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another NACE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings using Hugging Face\n",
    "# -------------------------------------------------------------------------\n",
    "def create_hf_embeddings(texts, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "            embeddings.append(cls_embedding.squeeze().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load Sellers and NACE Data\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "# Filepaths (update to your actual paths)\n",
    "# sellers_filepath = './data/dejuna_buyer_latest.csv'\n",
    "sellers_filepath = originalSalesNexxtChangeData\n",
    "nace_codes_filepath = './data/nace_codes_object_du.json'\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize Hugging Face model and tokenizer\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"ðŸš€ Loaded Hugging Face model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "nace_embeddings = create_hf_embeddings(nace_descriptions, tokenizer, model)\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#  lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "branchen_embeddings = create_hf_embeddings(sellers_df['preprocessed_branchen'].tolist(), tokenizer, model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "\n",
    "similarity_threshold = 0.7\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] if row['assigned_nace_similarity'] >= similarity_threshold else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_hf_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[[ 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEOCODING LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_unique_locations(buyers_df, sellers_df):\n",
    "    \"\"\"Extract all unique locations from buyers and sellers dataframes.\"\"\"\n",
    "    unique_locations = set()\n",
    "\n",
    "    for df, name in [(buyers_df, 'buyers'), (sellers_df, 'sellers')]:\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            # logging.info(f'Extracted locations: {location.lower().split('\\n')}')\n",
    "            unique_locations.update(locations)\n",
    "            \n",
    "\n",
    "    logging.info(f'Total unique locations found: {len(unique_locations)}')\n",
    "    return unique_locations\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode unique locations with caching.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"buyer_seller_matching\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3, error_wait_seconds=10.0)\n",
    "\n",
    "    # Ensure cache directory exists\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if cache_dir and not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        for location in unique_locations:\n",
    "\n",
    "            if location in geocode_cache:\n",
    "                continue  # Already cached\n",
    "            try:\n",
    "                logging.info(f'Geocoding location: {location}')\n",
    "                loc = geocode(location + \", Germany\")\n",
    "                if loc:\n",
    "                    geocode_cache[location] = {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "                    logging.info(f'Geocoded {location}: ({loc.latitude}, {loc.longitude})')\n",
    "                else:\n",
    "                    geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "                    logging.warning(f'Geocoding failed for location: {location}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Geocoding error for location '{location}': {e}\")\n",
    "                geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "\n",
    "def update_dataframe_with_geocodes(df, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Add latitude and longitude columns to the dataframe based on locations.\"\"\"\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            lat_list = []\n",
    "            lon_list = []\n",
    "            for loc in locations:\n",
    "                geocode_info = geocode_cache.get(loc, {'latitude': None, 'longitude': None})\n",
    "                if geocode_info['latitude'] is not None and geocode_info['longitude'] is not None:\n",
    "                    lat_list.append(geocode_info['latitude'])\n",
    "                    lon_list.append(geocode_info['longitude'])\n",
    "                else:\n",
    "                    # If geocoding failed, append None\n",
    "                    lat_list.append(None)\n",
    "                    lon_list.append(None)\n",
    "            # Convert lists to JSON strings for CSV compatibility\n",
    "            latitudes.append(json.dumps(lat_list))\n",
    "            longitudes.append(json.dumps(lon_list))\n",
    "\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    return df\n",
    "\n",
    "# Paths to input and output files\n",
    "buyer_filepath = './data/dejuna_buyer_latest_hf_nace.csv'\n",
    "sellers_filepath = './data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv'\n",
    "\n",
    "cache_path = './geocode_cache.db'\n",
    "\n",
    "# Load buyer and seller datasets\n",
    "\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "unique_locations = get_all_unique_locations(buyers_df, sellers_df)\n",
    "unique_locations\n",
    "# Geocode locations with caching\n",
    "geocode_locations(unique_locations, cache_path=cache_path)\n",
    "\n",
    "# Update dataframes with geocodes\n",
    "\n",
    "# # logging.info('Updating sellers dataframe with geocodes...')\n",
    "sellers_df = update_dataframe_with_geocodes(sellers_df, cache_path=cache_path)\n",
    "buyers_df = update_dataframe_with_geocodes(buyers_df, cache_path=cache_path)\n",
    "\n",
    "# # Save updated dataframes to new CSV files\n",
    "logging.info('Saving updated sellers dataframe...')\n",
    "sellers_output_file = sellers_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "buyers_output_file = buyer_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "sellers_df.to_csv(sellers_output_file, index=False)\n",
    "buyers_df.to_csv(buyers_output_file, index=False)\n",
    "logging.info('Geocoding process completed successfully.')\n",
    "print(f\"ðŸš€ Saved sellers data with assigned geocodes to: {buyers_output_file}\\n {sellers_output_file}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "'''Hamburg\n",
    "Schleswig-Holstein\n",
    "Berlin\n",
    "\n",
    "'''\n",
    "location1 = 'Berlin'\n",
    "location2 = 'Brandenburg'\n",
    "\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[10:30]\n",
    "purchase = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')[10:30]\n",
    "\n",
    "\n",
    "sales['processed_location'] = sales['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "purchase['processed_location'] = purchase['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "\n",
    "# # Check if any element of sales processed_location is in purchase processed_location\n",
    "purchase['latitude'] = purchase['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "purchase['longitude'] = purchase['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "sales['latitude'] = sales['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sales['longitude'] = sales['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "\n",
    "def match_locations(sales_locations, purchase_locations):\n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "\n",
    "# Create a dataframe to store matched locations\n",
    "matched_locations = []\n",
    "\n",
    "for idx, row in sales.iterrows():\n",
    "    for idx2, row2 in purchase.iterrows():\n",
    "        if match_locations(row.processed_location, row2.processed_location):\n",
    "            matched_locations.append({\n",
    "                'sales_id': idx,\n",
    "                'sales_location': row.location,\n",
    "                'sales_processed_location': row.processed_location,\n",
    "                'purchase_id': idx2,\n",
    "                'purchase_location': row2.location,\n",
    "                'sales_longitude': s_lon,\n",
    "                'sales_latitude': s_lat,\n",
    "                'purchase_longitude': p_lon,\n",
    "                'purchase_latitude': p_lat,\n",
    "                'purchase_processed_location': row2.processed_location,\n",
    "                'distance_km': None\n",
    "            })\n",
    "        else:\n",
    "            # Calculate distance between sales and purchase locations\n",
    "            sales_lat_lon = zip(row.latitude, row.longitude)\n",
    "            purchase_lat_lon = zip(row2.latitude, row2.longitude)\n",
    "            for s_lat, s_lon in sales_lat_lon:\n",
    "                for p_lat, p_lon in purchase_lat_lon:\n",
    "                    if s_lat is not None and s_lon is not None and p_lat is not None and p_lon is not None:\n",
    "                        print(f\"Calculating distance between ({s_lat}, {s_lon}) and ({p_lat}, {p_lon})\")\n",
    "                        distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                        print(f\"Distance: {distance}\")\n",
    "                        if distance <= 50:\n",
    "                            matched_locations.append({\n",
    "                                'sales_id': idx,\n",
    "                                'sales_processed_location': row.processed_location,\n",
    "                                'sales_location': row.location,\n",
    "                                'sales_longitude': s_lon,\n",
    "                                'sales_latitude': s_lat,\n",
    "                                'purchase_longitude': p_lon,\n",
    "                                'purchase_latitude': p_lat,\n",
    "                                'purchase_id': idx2,\n",
    "                                'purchase_location': row2.location,\n",
    "                                'purchase_processed_location': row2.processed_location,\n",
    "                                'distance_km': distance\n",
    "                            })\n",
    "matched_locations_df = pd.DataFrame(matched_locations)\n",
    "print(matched_locations_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Load synonyms CSV\n",
    "synonyms_df = pd.read_csv('./data/Updated_Keywords_and_Synonyms.csv')\n",
    "synonym_dict = {}\n",
    "for _, row in synonyms_df.iterrows():\n",
    "    keyword = row['Keyword'].lower()\n",
    "    synonyms = row.dropna().tolist()[1:]\n",
    "    synonyms = [syn.lower() for syn in synonyms]\n",
    "    synonym_dict[keyword] = synonyms\n",
    "\n",
    "# Define augmentation functions\n",
    "def extract_keywords(text, top_n=5):\n",
    "    # vectorizer = TfidfVectorizer(stop_words='german', max_features=top_n)\n",
    "    vectorizer = CountVectorizer(stop_words = german_stop_words) # Now use this in your pipeline\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "def augment_text_with_synonyms(text, top_n=5):\n",
    "    keywords = extract_keywords(text, top_n)\n",
    "    synonyms = []\n",
    "    for word in keywords:\n",
    "        synonyms.extend(synonym_dict.get(word, []))\n",
    "    synonyms = ' '.join(synonyms)\n",
    "    return f\"{text} {synonyms}\"\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text_de(text):\n",
    "    doc = nlp_de(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "def analyze_matches(matches_df, buyers_df, sellers_df):\n",
    "    \"\"\"Analyze matching results and print key metrics.\"\"\"\n",
    "    logging.info(\"\\n=== Matching Analysis ===\")\n",
    "    \n",
    "    total_buyers = len(buyers_df)\n",
    "    total_sellers = len(sellers_df)\n",
    "    total_matches = len(matches_df)\n",
    "    \n",
    "    logging.info(f\"Total buyers: {total_buyers}\")\n",
    "    logging.info(f\"Total sellers: {total_sellers}\") \n",
    "    logging.info(f\"Total matches found: {total_matches}\")\n",
    "    if total_buyers > 0:\n",
    "        logging.info(f\"Average matches per buyer: {total_matches/total_buyers:.2f}\")\n",
    "    else:\n",
    "        logging.info(\"No buyers to match against.\")\n",
    "\n",
    "    # Save top matches for manual review\n",
    "    top_matches = matches_df.head(10)\n",
    "    top_matches.to_csv('./matches/top_matches_for_review.csv', index=False)\n",
    "    logging.info(\"Saved top 10 matches for manual review\")\n",
    "    \n",
    "    return {\n",
    "        'total_matches': total_matches,\n",
    "        'matches_per_buyer': total_matches/total_buyers if total_buyers else 0,\n",
    "        'buyer_match_rate': len(matches_df['buyer_title'].unique())/total_buyers if total_buyers else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "# buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "# sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "\n",
    "# Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "logging.info('Loading the Sentence Transformer model...')\n",
    "# model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "# model_name = 'all-MiniLM-L12-v2'\n",
    "model_name ='aari1995/German_Semantic_STS_V2'\n",
    "\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model {model_name}: {e}\")\n",
    "\n",
    "# Encode sellers' combined_text\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = get_embedding_batch(seller_texts, model, batch_size=64)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "# Set similarity threshold\n",
    "similarity_threshold = 0.93\n",
    "# Optional text length filter\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info('Starting matching process...')\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarity scores to all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "\n",
    "    # Indices above threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "\n",
    "    for seller_idx in matching_indices:\n",
    "        seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "        confidence_score = sim_scores[seller_idx]\n",
    "        if confidence_score < similarity_threshold:\n",
    "            continue\n",
    "\n",
    "        confidence_scores.append(confidence_score)\n",
    "        \n",
    "        match = {\n",
    "            'buyer_date': buyer_row.get('date', ''),\n",
    "            'buyer_title': buyer_row.get('title', ''),\n",
    "            'buyer_description': buyer_row.get('description', ''),\n",
    "            'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "            'buyer_location': buyer_row.get('location', ''),\n",
    "            'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "            'seller_date': seller_row.get('date', ''),\n",
    "            'seller_title': seller_row.get('title', ''),\n",
    "            'seller_description': seller_row.get('description', ''),\n",
    "            'seller_long_description': seller_row.get('long_description', ''),\n",
    "            'seller_location': seller_row.get('location', ''),\n",
    "            'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "            \n",
    "            'similarity_score': confidence_score\n",
    "        }\n",
    "        matches.append(match)\n",
    "\n",
    "    # Progress logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "logging.info('Creating matches DataFrame...')\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Sort by confidence score\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "\n",
    "    # Save all matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    logging.info(f'Saved all matches: {len(matches_df)} records => {output_all}')\n",
    "\n",
    "    # Analyze results\n",
    "    metrics = analyze_matches(matches_df, buyers_df, sellers_df)\n",
    "    \n",
    "    # Optionally filter for high confidence\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= 0.95]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f'Saved high confidence matches: {len(high_conf_df)} records => {output_high_conf}')\n",
    "else:\n",
    "    logging.info('No matches found.')\n",
    "\n",
    "# Final memory cleanup\n",
    "del seller_embeddings\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_locations(sales_locations, purchase_locations):\n",
    "    \"\"\"\n",
    "    Check if any element of sales_locations is in purchase_locations.\n",
    "    \"\"\"\n",
    "    print(f'purchase_locations: {purchase_locations}, sales_locations: {sales_locations}, {any(loc in purchase_locations for loc in sales_locations)}')\n",
    "    \n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "# def match_locations(locations_input, target_location):\n",
    "#     # Step 1: Normalize and parse the locations into a list of location parts\n",
    "#     def parse_location(location):\n",
    "#         return [part.strip().lower() for part in location.split(\">\")]\n",
    "\n",
    "#     # Parse the target location\n",
    "#     target_parts = parse_location(target_location)\n",
    "    \n",
    "#     # Split the multiple input locations by line breaks and parse them\n",
    "#     locations = locations_input.strip().split(\"\\n\")\n",
    "#     parsed_locations = [parse_location(loc) for loc in locations]\n",
    "\n",
    "#     # Step 2: Function to compare the parsed locations with the target\n",
    "#     def compare_location(loc1_parts, loc2_parts):\n",
    "#         max_level = max(len(loc1_parts), len(loc2_parts))\n",
    "#         match_score = 0\n",
    "\n",
    "#         for level in range(max_level):\n",
    "#             loc1_value = loc1_parts[level] if level < len(loc1_parts) else None\n",
    "#             loc2_value = loc2_parts[level] if level < len(loc2_parts) else None\n",
    "            \n",
    "#             if loc1_value and loc2_value:\n",
    "#                 # Exact match at this level\n",
    "#                 if loc1_value == loc2_value:\n",
    "#                     match_score += 1\n",
    "#                 else:\n",
    "#                     break  # If any level does not match, break early\n",
    "#             elif loc1_value is None and loc2_value is None:\n",
    "#                 continue  # Both are missing, no issue\n",
    "#             else:\n",
    "#                 match_score += 0.5  # Partial match (one location is more specific)\n",
    "\n",
    "#         return match_score\n",
    "\n",
    "#     # Step 3: Check if any location matches (either exact or partial)\n",
    "#     # for loc_parts in parsed_locations:\n",
    "#     #     match_score = compare_location(loc_parts, target_parts)\n",
    "#     #     if match_score > 0:  # If there's a partial match or exact match\n",
    "#     #         return True\n",
    "#     for loc_parts in parsed_locations:\n",
    "#         match_score = compare_location(loc_parts, target_parts)\n",
    "#         print(\" > \".join(loc_parts))\n",
    "#         if match_score > 0:  \n",
    "#             if target_coords and location_coordinates.get(\" > \".join(loc_parts)):\n",
    "#                 loc_coords = location_coordinates.get(\" > \".join(loc_parts))\n",
    "#                 if loc_coords:\n",
    "#                     lat1, lon1 = target_coords\n",
    "#                     lat2, lon2 = loc_coords\n",
    "#                     distance = haversine(lat1, lon1, lat2, lon2)\n",
    "#                     if distance <= max_distance_km:\n",
    "#                         return True \n",
    "#             else:\n",
    "#                 return True\n",
    "\n",
    "#     # If no matches found, return False\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Load Models\n",
    "# -----------------------------------\n",
    "logging.info(\"Loading SentenceTransformer and CrossEncoder models...\")\n",
    "# bi_encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer(\"xlm-roberta-base\")\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer('aari1995/German_Semantic_STS_V2')\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Encode Sellers' Text\n",
    "# -----------------------------------\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 6. Matching Parameters\n",
    "# -----------------------------------\n",
    "similarity_threshold = 0.60\n",
    "cross_encoder_threshold = 0.65\n",
    "top_n = 100  # Number of top candidates to re-rank per buyer\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info(\"Starting matching process...\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 7. Matching Loop\n",
    "# -----------------------------------\n",
    "matched_locations = []\n",
    "\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyers_df.iloc[i]['latitude']\n",
    "    buyer_longitudes = buyers_df.iloc[i]['longitude']\n",
    "    buyer_locations = buyers_df.iloc[i]['location']\n",
    "\n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute cosine similarities with all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "    \n",
    "    # Get indices of sellers with similarity >= threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue  # No matches above threshold\n",
    "    \n",
    "    # Select top N matches based on similarity scores\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:top_n]]\n",
    "    \n",
    "    # Prepare pairs for cross-encoder\n",
    "    buyer_texts = [buyer_text] * len(top_indices)\n",
    "    seller_texts_top = [sellers_df.iloc[idx]['combined_text'] for idx in top_indices]\n",
    "    pairs = list(zip(buyer_texts, seller_texts_top))\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    cross_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Filter based on cross-encoder threshold\n",
    "    for seller_idx, cross_score in zip(top_indices, cross_scores):\n",
    "\n",
    "\n",
    "        if cross_score >= cross_encoder_threshold:\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            # if match_locations(seller_locations, buyer_locations):\n",
    "            #     location_match = True\n",
    "            # else:\n",
    "            # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km threshold\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "            if location_match:\n",
    "                logging.info(f\"Match found: Buyer location: {buyer_locations}, Seller location: {seller_locations}\")\n",
    "                match = {\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller_row.get('id', ''),\n",
    "                    'seller_date': seller_row.get('date', ''),\n",
    "                    'seller_title': seller_row.get('title', ''),\n",
    "                    'seller_description': seller_row.get('description', ''),\n",
    "                    'seller_long_description': seller_row.get('long_description', ''),\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'seller_source': seller_row.get('url', ''),\n",
    "                    'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "                    \n",
    "                    'similarity_score': cross_score,\n",
    "                    'distance_km': distance_km if distance_km else 'Within processed locations'\n",
    "\n",
    "                }\n",
    "                matches.append(match)\n",
    "                confidence_scores.append(cross_score)\n",
    "    \n",
    "    # Progress Logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 8. Create Matches DataFrame\n",
    "# -----------------------------------\n",
    "logging.info(\"Creating matches DataFrame...\")\n",
    "matches_df = pd.DataFrame(matches)\n",
    "        \n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "# del seller_embeddings\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# 2025-01-21 12:59:12,950 - INFO - Match found: Buyer location: ['Baden-WÃ¼rttemberg', 'Bayern'], Seller location: ['Rhein-Neckar-Kreis']\n",
    "\n",
    "# purchase_locations: ['Baden-WÃ¼rttemberg', 'Bayern'], sales_locations: ['Rhein-Neckar-Kreis'], False\n",
    "location1 = 'Brandenburg > Brandenburg > Potsdam'\n",
    "location2 = 'OsnabrÃ¼ck'\n",
    "#  {'Berlin',\n",
    "#  'Brandenburg',\n",
    "#  'Dresden',\n",
    "#  'Hannover',\n",
    "#  'Leipzig',\n",
    "#  'Niedersachsen',\n",
    "#  'Sachsen'}\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.distance import geodesic\n",
    "# import math\n",
    "\n",
    "# # Function to geocode a location string into latitude and longitude\n",
    "# def geocode(location):\n",
    "#     \"\"\"\n",
    "#     Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "#     Args:\n",
    "#         location (str): The location string to geocode.\n",
    "    \n",
    "#     Returns:\n",
    "#         tuple: Latitude and longitude of the location, or None if not found.\n",
    "#     \"\"\"\n",
    "#     geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "#     location_data = geolocator.geocode(location)\n",
    "#     if location_data:\n",
    "#         return location_data.latitude, location_data.longitude\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # Haversine formula to calculate distance between two points (lat1, lon1) and (lat2, lon2)\n",
    "# def haversine(lat1, lon1, lat2, lon2):\n",
    "#     R = 6371  # Radius of the Earth in kilometers\n",
    "#     phi1 = math.radians(lat1)\n",
    "#     phi2 = math.radians(lat2)\n",
    "#     delta_phi = math.radians(lat2 - lat1)\n",
    "#     delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "#     a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "#     c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "#     distance = R * c  # Distance in kilometers\n",
    "#     return distance\n",
    "\n",
    "\n",
    "# def match_locations(locations_input, target_location, max_distance_km=50):\n",
    "#     # Step 1: Normalize and parse the locations into a list of location parts\n",
    "#     def parse_location(location):\n",
    "#         return [part.strip().lower() for part in location.split(\">\")]\n",
    "\n",
    "#     # Parse the target location\n",
    "#     target_parts = parse_location(target_location)\n",
    "#     target_coords = geocode(target_location)  # Get coordinates of the target location\n",
    "    \n",
    "#     if not target_coords:\n",
    "#         return False  # If target location couldn't be geocoded, return False\n",
    "\n",
    "#     # Split the multiple input locations by line breaks and parse them\n",
    "#     locations = locations_input.strip().split(\"\\n\")\n",
    "#     parsed_locations = [parse_location(loc) for loc in locations]\n",
    "\n",
    "#     # Step 2: Function to compare the parsed locations with the target\n",
    "#     def compare_location(loc1_parts, loc2_parts):\n",
    "#         max_level = max(len(loc1_parts), len(loc2_parts))\n",
    "#         match_score = 0\n",
    "\n",
    "#         for level in range(max_level):\n",
    "#             loc1_value = loc1_parts[level] if level < len(loc1_parts) else None\n",
    "#             loc2_value = loc2_parts[level] if level < len(loc2_parts) else None\n",
    "            \n",
    "#             if loc1_value and loc2_value:\n",
    "#                 # Exact match at this level\n",
    "#                 if loc1_value == loc2_value:\n",
    "#                     match_score += 1\n",
    "#                 else:\n",
    "#                     break  # If any level does not match, break early\n",
    "#             elif loc1_value is None and loc2_value is None:\n",
    "#                 continue  # Both are missing, no issue\n",
    "#             else:\n",
    "#                 match_score += 0.5  # Partial match (one location is more specific)\n",
    "\n",
    "#         return match_score>0\n",
    "\n",
    "    # # Step 3: Check if any location matches (either exact or partial) and is within proximity\n",
    "    # for loc_parts in parsed_locations:\n",
    "    #     match_score = compare_location(loc_parts, target_parts)\n",
    "\n",
    "    #     # If there's a partial or exact match, we also check the geographical proximity\n",
    "    #     if match_score > 0:  # If there's a name match\n",
    "    #         location_str = \" > \".join(loc_parts)\n",
    "    #         loc_coords = geocode(location_str)  # Get coordinates of the input location\n",
    "            \n",
    "    #         if loc_coords:\n",
    "    #             lat1, lon1 = target_coords\n",
    "    #             lat2, lon2 = loc_coords\n",
    "    #             distance=geodesic(target_coords, loc_coords).kilometers\n",
    "    #             logging.info(f\"Distance between '{target_location}' and '{location_str}': {distance:.2f} km\")\n",
    "    #             # distance = haversine(lat1, lon1, lat2, lon2)\n",
    "    #             if distance <= max_distance_km:\n",
    "    #                 return True  # Match found within the proximity\n",
    "    #         else:\n",
    "    #             continue  # If geocoding failed for the input location, skip it\n",
    "\n",
    "    # # If no matches found, return False\n",
    "    # return False\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# locations_input = '''Berlin\n",
    "# Sachsen > Leipzig\n",
    "# Sachsen > Dresden\n",
    "# Brandenburg\n",
    "# Niedersachsen > Hannover'''\n",
    "\n",
    "# target_location = '''Brandenburg > Brandenburg'''\n",
    "\n",
    "# # Call the function\n",
    "# result = match_locations(locations_input, target_location)\n",
    "# print(result)  # True if any match (name + proximity), False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sellers and buyers files into dataframes\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "\n",
    "# Add origin column to both dataframes\n",
    "sellers_df['origin'] = 'seller'\n",
    "buyers_df['origin'] = 'buyer'\n",
    "buyers_df['branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} > {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Concatenate both dataframes\n",
    "combined_df = pd.concat([sellers_df, buyers_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE Labels for Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = combined_df \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"ðŸš€ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "    lambda x: ' '.join(x.split('>'))\n",
    ")\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        # row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"ðŸš€ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = \"buyers_sellers_combined_nace.csv\"\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1= pd.read_csv(\"./matches/nlp_business_all_matches_16_19-31.csv\")\n",
    "temp2= pd.read_csv(\"./matches/nlp_business_all_matches_15_13-11.csv\")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_df = pd.concat([temp1, temp2])\n",
    "# Get all duplicates based on specific columns\n",
    "# Get all rows that only exist once in the dataframe\n",
    "unique_rows = combined_df.drop_duplicates(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)\n",
    "duplicates = unique_rows[unique_rows.duplicated(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)]\n",
    "\n",
    "# unique_rows.to_csv('./matches/unique_rows.csv', index=False)\n",
    "# Display the unique rows\n",
    "# print(unique_rows)\n",
    "print(duplicates)\n",
    "# Display the duplicates\n",
    "# print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def extract_locations(location_string):\n",
    "    \"\"\"\n",
    "    Extracts locations from a string with nested structure.\n",
    "    \n",
    "    Args:\n",
    "        location_string (str): Input string containing locations in nested format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted locations.\n",
    "    \"\"\"\n",
    "    locations = []\n",
    "\n",
    "    # Split the input into lines\n",
    "    lines = location_string.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Strip whitespace from the line\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Ignore empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # If the line contains '>', it indicates nested locations\n",
    "        if '>' in line:\n",
    "            # Extract the specific location after the last '>'\n",
    "            nested_location = line.split('>')[-1].strip()\n",
    "            locations.append(nested_location)\n",
    "        else:\n",
    "            # Append the standalone location\n",
    "            locations.append(line)\n",
    "\n",
    "    return locations\n",
    "\n",
    "def get_location_coordinates(location):\n",
    "    \"\"\"\n",
    "    Mock function to return coordinates for a given location.\n",
    "    Replace this with an actual geocoding API in production.\n",
    "\n",
    "    Args:\n",
    "        location (str): The location name.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location.\n",
    "    \"\"\"\n",
    "    # Mock coordinates for demonstration purposes\n",
    "    coordinates = {\n",
    "        \"Celle\": (52.6226, 10.0815),\n",
    "        \"Hannover\": (52.3759, 9.7320),\n",
    "        \"Karlsruhe\": (49.0069, 8.4037),\n",
    "        \"Mannheim\": (49.4875, 8.4660),\n",
    "        \"Goettingen\": (51.5413, 9.9158),\n",
    "    }\n",
    "    return coordinates.get(location, None)\n",
    "\n",
    "def locations_match(s1, s2):\n",
    "    \"\"\"\n",
    "    Checks if the locations extracted from two strings have any matches.\n",
    "    If no exact match, it calculates the distance between locations.\n",
    "\n",
    "    Args:\n",
    "        s1 (str): First input string containing locations.\n",
    "        s2 (str): Second input string containing locations.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if there are matching locations or distance < 50KM, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract locations from both strings\n",
    "    locations1 = set(_extract_location_parts(s1))\n",
    "    locations2 = set(_extract_location_parts(s2))\n",
    "\n",
    "    print(f\"Locations 1: {locations1}\")\n",
    "    print(f\"Locations 2: {locations2}\")\n",
    "    # Check for exact match first\n",
    "    if not locations1.isdisjoint(locations2):\n",
    "        return True\n",
    "\n",
    "    # If no exact match, calculate distances\n",
    "    for loc1 in locations1:\n",
    "        for loc2 in locations2:\n",
    "            coord1 = get_location_coordinates(loc1)\n",
    "            coord2 = get_location_coordinates(loc2)\n",
    "\n",
    "            if coord1 and coord2:\n",
    "                distance = geodesic(coord1, coord2).kilometers\n",
    "                print(f\"Distance between {loc1} and {loc2}: {distance} KM\")\n",
    "                if distance < 50:\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "s1 = \"Niedersachen > Celle\"\n",
    "s2 = \"Niedersachsen > Goettingen\"\n",
    "\n",
    "output = locations_match(s1, s2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import os\n",
    "import shelve\n",
    "from unidecode import unidecode\n",
    "import gc\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Location Processing Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract hierarchical location components\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return []\n",
    "    try:\n",
    "        return [part.strip() for part in re.split(r'>|\\n', location) if part.strip()]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Location parsing error: {e}\")\n",
    "        return []\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode locations with hierarchical fallback\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"business_matcher\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "    \n",
    "    with shelve.open(cache_path) as cache:\n",
    "        for loc in unique_locations:\n",
    "            if loc not in cache:\n",
    "                try:\n",
    "                    # Try full location first, then fallback through hierarchy\n",
    "                    parts = _extract_location_parts(loc)\n",
    "                    for i in range(len(parts)):\n",
    "                        query = \", \".join(parts[-i-1:]) + \", Germany\"\n",
    "                        result = geocode(query)\n",
    "                        if result:\n",
    "                            cache[loc] = (result.latitude, result.longitude)\n",
    "                            break\n",
    "                    if loc not in cache:\n",
    "                        cache[loc] = (None, None)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Geocoding failed for {loc}: {e}\")\n",
    "                    cache[loc] = (None, None)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Text Processing Functions (German-optimized)\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_german_text(text):\n",
    "    \"\"\"Basic cleaning for German business text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Remove URLs and special characters\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\b\\d{10,}\\b|[^\\wÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ\\s]', ' ', text)\n",
    "    \n",
    "    # Handle common business abbreviations\n",
    "    replacements = {\n",
    "        r'\\bMio\\b': 'millionen',\n",
    "        r'\\bTsd\\b': 'tausend',\n",
    "        r'\\bca\\.': 'circa',\n",
    "        r'\\bz\\.B\\.': 'zum beispiel'\n",
    "    }\n",
    "    for pattern, repl in replacements.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    \n",
    "    return unidecode(text).lower().strip()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Core Matching Logic\n",
    "# -------------------------------------------------------------------------\n",
    "# Load data\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')[:100]\n",
    "\n",
    "# Process locations\n",
    "logging.info(\"Processing locations...\")\n",
    "for df in [buyers_df, sellers_df]:\n",
    "    df['location_parts'] = df['location'].apply(_extract_location_parts)\n",
    "    df['primary_location'] = df['location_parts'].apply(\n",
    "        lambda x: x[-1] if x else None\n",
    "    )\n",
    "\n",
    "# Geocode locations\n",
    "unique_locations = set(\n",
    "    buyers_df['primary_location'].dropna().tolist() +\n",
    "    sellers_df['primary_location'].dropna().tolist()\n",
    ")\n",
    "geocode_locations(unique_locations)\n",
    "\n",
    "# Load geocodes\n",
    "with shelve.open('geocode_cache.db') as cache:\n",
    "    sellers_df['coords'] = sellers_df['primary_location'].apply(\n",
    "        lambda x: cache.get(x, (None, None))\n",
    "    )\n",
    "    buyers_df['coords'] = buyers_df['primary_location'].apply(\n",
    "        lambda x: cache.get(x, (None, None))\n",
    "    )\n",
    "\n",
    "# Combine text fields\n",
    "def combine_text(row):\n",
    "    return ' '.join([\n",
    "        str(row.get('title', '')),\n",
    "        str(row.get('description', '')),\n",
    "        str(row.get('long_description', ''))\n",
    "    ])\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "def initialize_models():\n",
    "    \"\"\"Explicit model/tokenizer configuration\"\"\"\n",
    "    # Configure transformer with separate tokenizer args\n",
    "    bi_encoder= SentenceTransformer(\n",
    "    'T-Systems-onsite/german-roberta-sentence-transformer-v2',\n",
    "    device='cuda',  # Use GPU if available\n",
    "    tokenizer_kwargs={\n",
    "        'use_fast': True,\n",
    "        'model_max_length': 512,\n",
    "        'truncation': True\n",
    "    }\n",
    ")\n",
    "    # pooling = models.Pooling(transformer.get_word_embedding_dimension())\n",
    "    \n",
    "    # bi_encoder = SentenceTransformer(modules=[transformer, pooling])\n",
    "    cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "    return bi_encoder, cross_encoder\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text, axis=1)\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text, axis=1)\n",
    "\n",
    "# Clean text\n",
    "logging.info(\"Cleaning text...\")\n",
    "sellers_df['clean_text'] = sellers_df['combined_text'].apply(clean_german_text)\n",
    "buyers_df['clean_text'] = buyers_df['combined_text'].apply(clean_german_text)\n",
    "sellers_df['id'] = sellers_df.index\n",
    "# Initialize models\n",
    "logging.info(\"Initializing models...\")\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/german-roberta-sentence-transformer-v2')\n",
    "# cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "bi_encoder, cross_encoder = initialize_models()\n",
    "# Encode sellers\n",
    "logging.info(\"Encoding sellers...\")\n",
    "seller_embeddings = bi_encoder.encode(\n",
    "    sellers_df['clean_text'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = seller_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(seller_embeddings)\n",
    "\n",
    "# Matching parameters\n",
    "SEMANTIC_THRESHOLD = 0.60\n",
    "CROSS_ENCODER_THRESHOLD = 0.65\n",
    "MAX_DISTANCE_KM = 50\n",
    "\n",
    "# Process buyers\n",
    "matches = []\n",
    "for buyer_idx, buyer_row in buyers_df.iterrows():\n",
    "    try:\n",
    "        # Semantic search\n",
    "        buyer_embedding = bi_encoder.encode([buyer_row['clean_text']])\n",
    "        distances, indices = index.search(buyer_embedding, 100)\n",
    "        # Filter candidates\n",
    "        candidates = []\n",
    "        for seller_idx, score in zip(indices[0], distances[0]):\n",
    "            if score >= SEMANTIC_THRESHOLD:\n",
    "                seller = sellers_df.iloc[seller_idx]\n",
    "                candidates.append((seller, score))\n",
    "        \n",
    "        # Location filtering\n",
    "        valid_matches = []\n",
    "        for seller, score in candidates:\n",
    "            # Hierarchical location match\n",
    "            location_match = any(\n",
    "                loc in buyer_row['location_parts']\n",
    "                for loc in seller['location_parts']\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Distance check\n",
    "            distance = None\n",
    "            if not location_match and None not in [buyer_row['coords'], seller['coords']]:\n",
    "                buyer_latitude, buyer_longitude= buyer_row['coords'].get('latitude', None), buyer_row['coords'].get('longitude', None)\n",
    "                seller_latitude, seller_longitude = seller['coords'].get('latitude', None), seller['coords'].get('longitude', None)\n",
    "                distance = geodesic((buyer_latitude, buyer_longitude),(seller_latitude, seller_longitude)).km\n",
    "                location_match = distance <= MAX_DISTANCE_KM\n",
    "\n",
    "            if location_match:\n",
    "                valid_matches.append({\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    \n",
    "                    # 'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    # 'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    # 'buyer_location': buyer_row.get('location', ''),\n",
    "                    # 'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller.get('id', ''),\n",
    "                    # 'seller_date': seller.get('date', ''),\n",
    "                    'seller_title': seller.get('title', ''),\n",
    "                    'seller_description': seller.get('description', ''),\n",
    "                    # 'seller_long_description': seller.get('long_description', ''),\n",
    "                    # 'seller_location': seller.get('location', ''),\n",
    "                    'seller_source': seller.get('url', ''),\n",
    "                    # 'seller_nace_code': seller.get('nace_code', ''),\n",
    "\n",
    "\n",
    "                    'semantic_score': score,\n",
    "                    'distance_km': round(distance, 2) if distance else 'Hierarchical match',\n",
    "                    'buyer_location': ' > '.join(buyer_row['location_parts']),\n",
    "                    'seller_location': ' > '.join(seller['location_parts'])\n",
    "                })\n",
    "\n",
    "        # Cross-encoder reranking\n",
    "        if valid_matches:\n",
    "            pairs = [(buyer_row['clean_text'], sellers_df.iloc[m['seller_id']]['clean_text']) \n",
    "                    for m in valid_matches]\n",
    "            cross_scores = cross_encoder.predict(pairs)\n",
    "            \n",
    "            for match, cross_score in zip(valid_matches, cross_scores):\n",
    "                if cross_score >= CROSS_ENCODER_THRESHOLD:\n",
    "                    match['confidence_score'] = cross_score\n",
    "                    matches.append(match)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing buyer {buyer_idx}:{buyer_row['coords']}: {e}\")\n",
    "\n",
    "# # Save results\n",
    "# if matches:\n",
    "#     matches_df = pd.DataFrame(matches)\n",
    "#     matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "#     matches_df.to_csv('./business_matches.csv', index=False)\n",
    "#     logging.info(f\"Saved {len(matches_df)} matches to business_matches.csv\")\n",
    "# else:\n",
    "#     logging.info(\"No matches found\")\n",
    "\n",
    "# logging.info(\"Creating matches DataFrame...\")\n",
    "# matches_df = pd.DataFrame(matches)\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "del seller_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:32:35,500 - INFO - Use pytorch device_name: mps\n",
      "2025-01-31 11:32:35,501 - INFO - Load pretrained SentenceTransformer: T-Systems-onsite/cross-en-de-roberta-sentence-transformer\n",
      "2025-01-31 11:32:35,984 - WARNING - No sentence-transformers model found with name T-Systems-onsite/cross-en-de-roberta-sentence-transformer. Creating a new one with mean pooling.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5c0cafdc1c47ad89f562dc10291c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:44:34,207 - INFO - EmbeddingSimilarityEvaluator: Evaluating the model on the val-eval dataset after epoch 1.0:\n",
      "2025-01-31 11:44:37,764 - INFO - Cosine-Similarity :\tPearson: 0.1114\tSpearman: 0.1311\n",
      "2025-01-31 11:44:37,769 - INFO - Save model to ./fine_tuned_models/fine_tuned_T-Systems-onsite/cross-en-de-roberta-sentence-transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b93d9f11844d0492272533ac7fc88f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:54:00,422 - INFO - EmbeddingSimilarityEvaluator: Evaluating the model on the val-eval dataset after epoch 2.0:\n",
      "2025-01-31 11:54:03,125 - INFO - Cosine-Similarity :\tPearson: 0.0354\tSpearman: 0.0481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1279.2536, 'train_samples_per_second': 0.285, 'train_steps_per_second': 0.019, 'train_loss': 548081581948928.0, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:54:03,448 - INFO - Use pytorch device_name: mps\n",
      "2025-01-31 11:54:03,449 - INFO - Load pretrained SentenceTransformer: ./fine_tuned_models/fine_tuned_T-Systems-onsite/cross-en-de-roberta-sentence-transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7847d079f6f4a0984569907d60b85c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a93ab8583614a029834cbe86246f59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.55793786\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "# Add negative examples (random pairs)\n",
    "\n",
    "# Combine text fields\n",
    "def combine_text(row):\n",
    "    return ' '.join([\n",
    "        str(row.get('title', '')),\n",
    "        str(row.get('description', '')),\n",
    "        str(row.get('long_description', ''))\n",
    "    ])\n",
    "buyer_text=buyers_df.apply(combine_text, axis=1).sample(69)\n",
    "seller_text=sellers_df.apply(combine_text, axis=1).sample(50)\n",
    "# Generate negative pairs\n",
    "negative_pairs = []\n",
    "for buyer, seller in zip(buyer_text, seller_text):\n",
    "    negative_pairs.append({\n",
    "        'buyer_text': buyer,\n",
    "        'seller_text': seller,\n",
    "        'similarity_score': np.random.uniform(0.1, 0.3)\n",
    "    })\n",
    "\n",
    "negative_pairs_df = pd.DataFrame(negative_pairs)\n",
    "\n",
    "# Combine known matches and negative pairs\n",
    "# negative_pairs['similarity_score'] = np.random.uniform(0.1, 0.3)\n",
    "# Generate synthetic training pairs\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load Data\n",
    "# Read the Excel file into a pandas dataframe\n",
    "known_matches = pd.read_excel(\"./data/nlp_business_all_matches_15_13-11-JN 2 -revised.xlsx\")\n",
    "\n",
    "# remove row with missing values in buyer_title or seller_title col \n",
    "known_matches = known_matches.dropna(subset=['buyer_title', 'seller_title'])\n",
    "# fill missing values or 'not in db' in similarity_score with 0.85 to 0.99 uniformly \n",
    "known_matches['similarity_score'] = known_matches['similarity_score'].apply(\n",
    "    lambda x: np.random.uniform(0.85, 0.99) if pd.isnull(x) or x == 'not in db' else x\n",
    ")\n",
    "# known_matches = pd.read_csv(\"./matches/nlp_business_all_matches_21_13-50.csv\")\n",
    "# known_matches = known_matches[['buyer_title', 'buyer_description', 'seller_title', 'seller_description', 'similarity_score']]\n",
    "known_matches['buyer_text'] = known_matches['buyer_title'] + ' ' + known_matches['buyer_description'] + ' ' + known_matches['buyer_long_description']\n",
    "known_matches['seller_text'] = known_matches['seller_title'] + ' ' + known_matches['seller_description'] + ' ' + known_matches['seller_long_description']\n",
    "\n",
    "combined_pairs = pd.concat([known_matches[['buyer_text', 'seller_text', 'similarity_score']], negative_pairs_df])\n",
    "\n",
    "# 2. Build Examples\n",
    "train_examples = []\n",
    "for idx, row in combined_pairs.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row['buyer_text'], row['seller_text']], label=row['similarity_score']))\n",
    "\n",
    "# 3. Create Dataloader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "# fine_tuned_all-MiniLM-L12-v2\n",
    "# 4. Load Pretrained Model\n",
    "model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "\n",
    "# 5. Define Loss (CosineSimilarityLoss if label in [0..1])\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# 6. Train\n",
    "# model_save_path = \"./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2\"\n",
    "model_save_path = \"./fine_tuned_models/fine_tuned_T-Systems-onsite/cross-en-de-roberta-sentence-transformer\"\n",
    "# Split data into training and validation sets\n",
    "train_size = int(0.8 * len(train_examples))\n",
    "train_examples, val_examples = train_examples[:train_size], train_examples[train_size:]\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name='val-eval')\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=2,\n",
    "    evaluation_steps=100,\n",
    "    optimizer_params={'lr': 2e-5},\n",
    "    warmup_steps=100,\n",
    "    output_path=model_save_path\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 11:58:48,734 - INFO - Use pytorch device_name: mps\n",
      "2025-01-31 11:58:48,734 - INFO - Load pretrained SentenceTransformer: ./fine_tuned_models/fine_tuned_T-Systems-onsite/cross-en-de-roberta-sentence-transformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f42c98d21f5488998c158b8f9d0cc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3964995155c94b2e943914cafcc7c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6069496\n"
     ]
    }
   ],
   "source": [
    "# 7. Use the fine-tuned model\n",
    "fine_tuned_model = SentenceTransformer(model_save_path)\n",
    "\n",
    "# Example inference\n",
    "text1 = \"Elektroinstallationsfirma oder ein IngenieurbÃ¼ro fÃ¼r GebÃ¤udetechnik gesucht Matthias ist SachverstÃ¤ndiger fÃ¼r GebÃ¤udetechnik & Brandschutz und sucht in Baden WÃ¼rttemberg oder Bayern eine Elektroinstallationsfirma oder ein IngenieurbÃ¼ro fÃ¼r GebÃ¤udetechnik.\"\n",
    "text2 = \"Innovativer Elektrobetrieb im Albtal sucht Nachfolger #Pforzheim #Karlsruhe #Service #Wartung #Elektroinstallationen #Beleuchtungstechnik #gleitender Ãœbergang\"\n",
    "embed1 = fine_tuned_model.encode(text1)\n",
    "embed2 = fine_tuned_model.encode(text2)\n",
    "\n",
    "cos_sim = np.dot(embed1, embed2) / (np.linalg.norm(embed1)*np.linalg.norm(embed2))\n",
    "print(\"Cosine Similarity:\", cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import json\n",
    "\n",
    "# Load the small spaCy model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text (remove domain-specific terms, URLs, emails, etc.)\n",
    "    text = re.sub(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Token processing: lemmatization and POS filtering\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[:500]\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields for both buyers and sellers\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# Load fine-tuned bi-encoder and cross-encoder models\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('deepset/gbert-large-sts')\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# Build BM25 index for sellers\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# Set similarity and cross-encoder thresholds\n",
    "similarity_threshold = 0.80  # Adjusted similarity threshold for better precision\n",
    "cross_encoder_threshold = 0.9  # Adjusted cross-encoder threshold\n",
    "\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "matches = []\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "\n",
    "    # BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # Bi-encoder stage (calculate similarity scores)\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ONNX-optimized inference for cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # Location-aware scoring: calculate geodesic distance between buyer and seller locations\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km proximity\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "\n",
    "            if score >= 0.65 and location_match:  # Final matching criteria\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row['location'],\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row['location'],\n",
    "                    'similarity_score': score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# Save results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "matches_df.to_csv('./matches/nlp_business_high_conf_matches.csv', index=False)\n",
    "logging.info(f\"Matches saved: {len(matches_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load the model (replace with correct path if needed)\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Semantic_STS_V2')\n",
    "# model = AutoModel.from_pretrained('aari1995/German_Semantic_STS_V2')\n",
    "\n",
    "# matryoshka_dim = 1024 # How big your embeddings should be, choose from: 64, 128, 256, 512, 768, 1024\n",
    "# model = SentenceTransformer(\"aari1995/German_Semantic_V3\", trust_remote_code=True, truncate_dim=matryoshka_dim)\n",
    "\n",
    "model = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\")\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "sellers_combined_texts = sellers_df['combined_text'].tolist()\n",
    "sellers_embeddings = model.encode(sellers_combined_texts, convert_to_tensor=True)\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sellers_combined_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Define sentences\n",
    "# buyers_combined_texts = buyers_df['combined_text'].tolist()\n",
    "\n",
    "# Compute embeddings for sellers\n",
    "# print(\"ðŸš€ Encoded sellers' text.\", seller_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set similarity threshold\n",
    "similarity_threshold = 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each buyer's combined text\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_title = buyer_row['title']\n",
    "    buyer_description = buyer_row['description']\n",
    "    logging.info(f\"Processing buyer {i+1}/{len(buyers_df)}: {buyer_title}\")\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "    buyer_locations = buyer_row['location']\n",
    "        # Tokenize sentences\n",
    "    # encoded_input = tokenizer(buyer_text, padding=True, truncation=True, return_tensors='pt')\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_tensor=True)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    # with torch.no_grad():\n",
    "    #     model_output = model(**encoded_input)\n",
    "    # buyer_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # Compute embedding for the current buyer's text\n",
    "    \n",
    "    # # Calculate cosine similarities\n",
    "    cosine_scores = util.cos_sim(buyer_embedding, sellers_embeddings)[0]\n",
    "    # Print cosine scores if greater than similarity threshold\n",
    "    for seller_idx, score in enumerate(cosine_scores):\n",
    "        if score >= similarity_threshold:\n",
    "            # Store the results in a DataFrame\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            # if match_locations(seller_locations, buyer_locations):\n",
    "            #     location_match = True\n",
    "            # else:\n",
    "            # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km threshold\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "            \n",
    "                # print(f\"Cosine Score: {score:.4f} (Index: {seller_idx}) sellers_title: {sellers_df.iloc[seller_idx]['title']}, distance: {distance_km} km\")\n",
    "            if location_match:\n",
    "                results.append({\n",
    "                    'buyer_title': buyer_title,\n",
    "                    'buyer_description': buyer_description,\n",
    "                    'seller_title': sellers_df.iloc[seller_idx]['title'],\n",
    "                    'seller_description': sellers_df.iloc[seller_idx]['description'],\n",
    "                    'distance': distance_km,\n",
    "                    'seller_location': seller_locations,\n",
    "                    'buyer_location': buyer_locations,\n",
    "                    'buyer_combined_text': buyer_text,\n",
    "                    'seller_combined_text': sellers_combined_texts[seller_idx],\n",
    "                    'similarity_score': score.item()\n",
    "                })\n",
    "\n",
    "            # Convert results to DataFrame\n",
    "            results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find indices where cosine scores are greater than the similarity threshold\n",
    "    # matching_indices = torch.where(cosine_scores >= similarity_threshold)[0].tolist()\n",
    "    # print(matching_indices)\n",
    "    # results = []\n",
    "    # for seller_idx in matching_indices:\n",
    "    #     seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "    # # # Store results in a DataFrame\n",
    "    #     if cosine_scores >= similarity_threshold:\n",
    "    #         logging.info(f\"Match found: Buyer text: {buyer_text}, Seller text: {seller_text}, Similarity: {score:.4f}\")\n",
    "\n",
    "    #         # results.append({\n",
    "    #         #     'buyer_text': buyer_text,\n",
    "    #         #     'buyer_title': buyer_title,\n",
    "    #         #     'buyer_description': buyer_description,\n",
    "    #         #     'seller_text': seller_text,\n",
    "    #         #     'seller_title': sellers_df.iloc[seller_idx]['title'],\n",
    "    #         #     'seller_description': sellers_df.iloc[seller_idx]['description'],\n",
    "    #         #     'similarity_score': score.item()\n",
    "    #         # })\n",
    "    # # for seller_idx, (seller_text, score) in enumerate(zip(sellers_combined_texts, cosine_scores)):\n",
    "    # #         logging.info(f\"Match found: Buyer text: {buyer_text}, Seller text: {seller_text}, Similarity: {score:.4f}\")\n",
    "            \n",
    "\n",
    "    # results_df = pd.DataFrame(results)\n",
    "# # Print results\n",
    "# for sentence, score in zip(sentences_to_compare, cosine_scores):\n",
    "#     print(f\"Sentence: {sentence}\")\n",
    "#     print(f\"Similarity Score: {score.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./matches/resultsdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# ðŸ”‘ 1. Enhanced Preprocessing\n",
    "# --------------------------\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# nlp = spacy.load(\"de_core_news_lg\", disable=[\"ner\"])\n",
    "# nlp = spacy.load(\n",
    "#     \"de_core_news_sm\",\n",
    "#     disable=[\"ner\"],\n",
    "#     exclude=[\"vectors\"]  # Disable vector subsystem\n",
    "# )\n",
    "# ðŸ”§ Switch to small model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = re.sub(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "    locations = set()\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split location string by \" > \", handling the hierarchical structure\n",
    "        parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.split(\">\")\n",
    "            locations.add(part[-1].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return list(locations)\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "# #Location processing\n",
    "# logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# # Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 2. Hybrid Retrieval Setup\n",
    "# --------------------------\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Precompute BM25 index\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 3. Enhanced Model Loading\n",
    "# --------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ðŸ”‘ Load fine-tuned bi-encoder (pretrain on business texts)\n",
    "bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2')\n",
    "\n",
    "# ðŸ”‘ Optimized cross-encoder with ONNX\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 4. Optimized Matching Loop\n",
    "# --------------------------\n",
    "# ðŸ”‘ Dynamic threshold calculation (precompute from validation data)\n",
    "similarity_threshold = 0.58  # Calculated from validation set\n",
    "cross_encoder_threshold = 0.72\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "    # ðŸ”‘ Hybrid retrieval: BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # ðŸ”‘ Bi-encoder stage\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    \n",
    "    # ðŸ”‘ Batch cosine similarity calculation\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # ðŸ”‘ Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # ðŸ”‘ Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ðŸ”‘ ONNX-optimized inference\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # ðŸ”‘ Location-aware scoring\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            \n",
    "            # ðŸ”‘ Combined score calculation\n",
    "            geo_score = calculate_geo_score(buyer_row, seller_row)  # Implement geo-scoring\n",
    "            final_score = 0.7 * score + 0.3 * geo_score\n",
    "            \n",
    "            if final_score >= 0.65:\n",
    "                # ... rest of match processing ...\n",
    "                print(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 5. Post-processing\n",
    "# --------------------------\n",
    "def calculate_geo_score(buyer, seller):\n",
    "    \"\"\"Calculate normalized geographic compatibility score (0-1)\"\"\"\n",
    "    # Implement sophisticated location matching\n",
    "    return min(1.0, 1 / (1 + geodesic_distance_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import json\n",
    "\n",
    "# Load the small spaCy model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text (remove domain-specific terms, URLs, emails, etc.)\n",
    "    text = re.sub(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Token processing: lemmatization and POS filtering\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[:500]\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields for both buyers and sellers\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# Load fine-tuned bi-encoder and cross-encoder models\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('deepset/gbert-large-sts')\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# Build BM25 index for sellers\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# Set similarity and cross-encoder thresholds\n",
    "similarity_threshold = 0.80  # Adjusted similarity threshold for better precision\n",
    "cross_encoder_threshold = 0.9  # Adjusted cross-encoder threshold\n",
    "\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "matches = []\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "\n",
    "    # BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # Bi-encoder stage (calculate similarity scores)\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ONNX-optimized inference for cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # Location-aware scoring: calculate geodesic distance between buyer and seller locations\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km proximity\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "\n",
    "            if score >= 0.65 and location_match:  # Final matching criteria\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row['location'],\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row['location'],\n",
    "                    'similarity_score': score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# Save results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "matches_df.to_csv('./matches/nlp_business_high_conf_matches.csv', index=False)\n",
    "logging.info(f\"Matches saved: {len(matches_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Convert to ONNX\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-12-v2\",\n",
    "    export=True\n",
    ")\n",
    "cross_encoder.save_pretrained(\"./onnx_models/cross-encoder-de\")\n",
    "\n",
    "# Load optimized version\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\"./onnx_models/cross-encoder-de\")\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
