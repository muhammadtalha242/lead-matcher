{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= 1. requirements matching  > 0.85\n",
    "            1. semantic analysis > 0.9\n",
    "                a. similarity keywords\n",
    "            2. Nace Code match > 1.0\n",
    "        2. Location matching\n",
    "            1. exact location matching\n",
    "            2. Geocoding matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shelve\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from geopy.distance import geodesic \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORTS\n",
    "dataDIR = './data/'\n",
    "originalSalesNexxtChangeData = f'{dataDIR}branche_nexxt_change_sales_listings_scrape.csv'\n",
    "dejunaPurchases = './data/dejuna_buyer_latest.csv'\n",
    "\n",
    "nacecode_josn =  './data/nace_codes.json' \n",
    "nacecode_array_josn =  './data/nace_codes_array.json' \n",
    "nacecode_array_obj =  './data/nace_codes_object.json' \n",
    "nacecode_array_obj_ext =  './data/nace_codes_object_ext.json' \n",
    "nacecode_array_obj_du =  './data/nace_codes_object_du.json'\n",
    " \n",
    "dataFile =  './data/nexxt_change_sales_listings_geocoded_short_test.csv' \n",
    "# sales_file_nace =  './data/nexxt_change_sales_listings_geocoded.csv' \n",
    "sales_file_brachen =  './data/branche_nexxt_change_sales_listings.csv' \n",
    "sales_file_nace =  './data/dub_listings_geo.csv'\n",
    "buyer_file_nace =  './data/nexxt_change_purchase_listings_geocoded.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/talha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/talha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load SpaCy's German model (for tokenization, NER, POS tagging)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_lg')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_lg')\n",
    "    nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. Preprocessing function (with German NER, POS, etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    - Lowercases the text.\n",
    "    - Removes URLs, emails, & large digit sequences.\n",
    "    - Filters out non-alphabetic chars except German Umlauts/ÃŸ.\n",
    "    - Uses SpaCy to keep only NOUN, PROPN, VERB tokens not in stopwords.\n",
    "    - Applies Snowball stemming on remaining tokens.\n",
    "    - Also includes certain named entities (ORG, PRODUCT, GPE).\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, emails, large numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    \n",
    "    # Keep only letters and German characters\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s]', '', text)\n",
    "    \n",
    "    # Compact multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "\n",
    "    # # Initialize German stopwords and Snowball stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Keep nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities (ORG, PRODUCT, GPE) and include them\n",
    "    entities = [\n",
    "        ent.text for ent in doc.ents \n",
    "        if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}\n",
    "    ]\n",
    "    # Stem and remove stopwords from entities\n",
    "    entities = [\n",
    "        stemmer.stem(ent.lower()) \n",
    "        for ent in entities \n",
    "        if ent.lower() not in stop_words\n",
    "    ]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def split_compounds_regex(text):\n",
    "    \"\"\"\n",
    "    Basic German compound word splitting using regex rules.\n",
    "    Example: \"Kundenzufriedenheit\" â†’ [\"kunden\", \"zufriedenheit\"]\n",
    "    \"\"\"\n",
    "    # Split at common compound connectors (e.g., -s-, -en-, -n-)\n",
    "    parts = re.split(r'(s\\b|en\\b|n\\b|e\\b)(?=\\w{3,})', text)\n",
    "    return [p for p in parts if p and len(p) > 2]\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# Load SpaCy model CORRECTLY (without disabling components during initial load)\n",
    "nlp = spacy.load(\"de_core_news_lg\")  # Load first\n",
    "nlp.disable_pipes(\"parser\", \"ner\")   # Disable components AFTER loading\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Preprocesses German text for similarity tasks.\n",
    "    - Disables static vectors to avoid RuntimeError.\n",
    "    - Uses regex-based compound splitting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    custom_stopwords = {\"unternehmen\", \"firma\", \"dienstleistung\", \"kunde\"}\n",
    "    stop_words = STOP_WORDS.union(custom_stopwords)\n",
    "    \n",
    "    # Token processing (lemmatization + POS filtering)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    # Regex-based compound splitting (no external dependencies)\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and len(token.text) > 8:\n",
    "            parts = re.findall(r'\\b\\w{4,}(?=\\w{4,})', token.text)  # Split long nouns\n",
    "            compounds.extend([p.lower() for p in parts])\n",
    "    tokens.extend(compounds)\n",
    "    \n",
    "    # Filter short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:20:08,464 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-03 01:20:08,465 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10cd9122cc140d589d52244acba622c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69234bec7ed048bcb90fffb7f2ba46e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7448be677b8f424488dbbfb35eb16e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b5470889c64aeab1ffdff192533d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b602ddddf54c75b0d63d8615f79361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bb0a0cc45e4f7cbee50b693b3f1476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "Avg Similarity (Matches): 0.50\n",
      "Avg Similarity (Non-Matches): 0.36\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# # Load SpaCy model CORRECTLY (without disabling components during initial load)\n",
    "# nlp = spacy.load(\"de_core_news_lg\")  # Load first\n",
    "# nlp.disable_pipes(\"parser\", \"ner\")   # Disable components AFTER loading\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Preprocesses German text for similarity tasks.\n",
    "    - Disables static vectors to avoid RuntimeError.\n",
    "    - Uses regex-based compound splitting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    custom_stopwords = {\"unternehmen\", \"firma\", \"dienstleistung\", \"kunde\"}\n",
    "    stop_words = STOP_WORDS.union(custom_stopwords)\n",
    "    \n",
    "    # Token processing (lemmatization + POS filtering)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    # Regex-based compound splitting (no external dependencies)\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and len(token.text) > 8:\n",
    "            parts = re.findall(r'\\b\\w{4,}(?=\\w{4,})', token.text)  # Split long nouns\n",
    "            compounds.extend([p.lower() for p in parts])\n",
    "    tokens.extend(compounds)\n",
    "    \n",
    "    # Filter short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_similarity(test_cases, nlp_model, embedding_model):\n",
    "    similarities = []\n",
    "    labels = []\n",
    "    \n",
    "    for text1, text2, is_similar in test_cases:\n",
    "        preprocessed1 = preprocess_text(text1, nlp_model)\n",
    "        preprocessed2 = preprocess_text(text2, nlp_model)\n",
    "        \n",
    "        embedding1 = embedding_model.encode(preprocessed1)\n",
    "        embedding2 = embedding_model.encode(preprocessed2)\n",
    "        \n",
    "        similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "        similarities.append(similarity)\n",
    "        labels.append(is_similar)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    threshold = 0.7\n",
    "    true_positives = np.sum((similarities > threshold) & (labels == True))\n",
    "    false_positives = np.sum((similarities > threshold) & (labels == False))\n",
    "    false_negatives = np.sum((similarities <= threshold) & (labels == True))\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives + 1e-8)  # Avoid division by zero\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    \n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n",
    "    print(f\"Avg Similarity (Matches): {np.mean(similarities[labels]):.2f}\")\n",
    "    print(f\"Avg Similarity (Non-Matches): {np.mean(similarities[~labels]):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    (\"Industriemaschinen\", \"Maschinenbau\", True),\n",
    "    (\"Kundenbetreuung\", \"Autoreparatur\", False),\n",
    "    (\"Logistikdienstleister\", \"Transportunternehmen\", True),\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "evaluate_similarity(test_cases, nlp, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _extract_location_parts(location):\n",
    "#     \"\"\"Extract and categorize location parts into states and cities.\"\"\"\n",
    "#     locations = set()\n",
    "#     german_states = {\n",
    "#         'baden-wÃ¼rttemberg', 'bayern', 'berlin', 'brandenburg', 'bremen',\n",
    "#         'hamburg', 'hessen', 'mecklenburg-vorpommern', 'niedersachsen',\n",
    "#         'nordrhein-westfalen', 'rheinland-pfalz', 'saarland', 'sachsen',\n",
    "#         'sachsen-anhalt', 'schleswig-holstein', 'thÃ¼ringen'\n",
    "#     }\n",
    "\n",
    "\n",
    "#     if not location or not isinstance(location, str):\n",
    "#         return locations\n",
    "\n",
    "#     try:\n",
    "#         # Split on common delimiters\n",
    "#         parts = re.split(r'[>/\\n]\\s*', location)\n",
    "#         split_locations = []\n",
    "\n",
    "#         for part in parts:\n",
    "#             part = part.strip().lower()\n",
    "#             if part:\n",
    "#                 # Further split by space if multiple states are concatenated\n",
    "#                 words = part.split()\n",
    "#                 temp = []\n",
    "#                 current = \"\"\n",
    "#                 for word in words:\n",
    "#                     if word.lower() in german_states:\n",
    "#                         if current:\n",
    "#                             temp.append(current.strip())\n",
    "#                         current = word\n",
    "#                     else:\n",
    "#                         current += \" \" + word if current else word\n",
    "#                 if current:\n",
    "#                     temp.append(current.strip())\n",
    "#                 split_locations.extend(temp)\n",
    "\n",
    "#         for loc in split_locations:\n",
    "#             loc = loc.strip().lower()\n",
    "#             if loc:\n",
    "#                 if loc in german_states:\n",
    "#                     locations.add(loc.title())  # Capitalize for better geocoding\n",
    "#                 else:\n",
    "#                     # Clean up common prefixes like \"region\"\n",
    "#                     clean_part = re.sub(r'^region\\s+', '', loc)\n",
    "#                     if clean_part:\n",
    "#                         locations.add(clean_part.title())\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "#     return locations\n",
    "\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "    locations = set()\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split location string by \" > \", handling the hierarchical structure\n",
    "        parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.split(\">\")\n",
    "            locations.add(part[-1].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return list(locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _extract_location_parts('''Sachsen / Leipzig / Leipzig, Stadt''')\n",
    "_extract_location_parts('''Berlin\n",
    "Sachsen > Leipzig\n",
    "Sachsen > Dresden\n",
    "Brandenburg\n",
    "Niedersachsen > Hannover\n",
    "Hessen > Frankfurt am Main\n",
    "                        Hamburg\n",
    "                        Bayern > MÃ¼nchen\n",
    "                        Bayern > NÃ¼rnberg\n",
    "                        Bayern > Augsburg\n",
    "                        A\n",
    "                        B > C''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Load NACE codes from JSON\n",
    "# -------------------------------------------------------------------------\n",
    "def load_nace_codes(filepath):\n",
    "    \"\"\"\n",
    "    Expects a JSON file where keys = NACE code, values = textual descriptions.\n",
    "    Example:\n",
    "      {\n",
    "        \"01.1\": \"Growing of non-perennial crops\",\n",
    "        \"01.2\": \"Growing of perennial crops\",\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings with sentence-transformers\n",
    "# -------------------------------------------------------------------------\n",
    "def get_embedding_batch(texts, model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode texts in batches to optimize memory usage.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, \n",
    "                              convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embeddings.astype('float32')  # Use float32 to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = pd.read_csv(sellers_filepath) \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"ðŸš€ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "#     lambda x: ' '.join(x.split('>'))\n",
    "# )\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"ðŸš€ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another NACE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings using Hugging Face\n",
    "# -------------------------------------------------------------------------\n",
    "def create_hf_embeddings(texts, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "            embeddings.append(cls_embedding.squeeze().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load Sellers and NACE Data\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "# Filepaths (update to your actual paths)\n",
    "# sellers_filepath = './data/dejuna_buyer_latest.csv'\n",
    "sellers_filepath = originalSalesNexxtChangeData\n",
    "nace_codes_filepath = './data/nace_codes_object_du.json'\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize Hugging Face model and tokenizer\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"ðŸš€ Loaded Hugging Face model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "nace_embeddings = create_hf_embeddings(nace_descriptions, tokenizer, model)\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#  lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "branchen_embeddings = create_hf_embeddings(sellers_df['preprocessed_branchen'].tolist(), tokenizer, model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "\n",
    "similarity_threshold = 0.7\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] if row['assigned_nace_similarity'] >= similarity_threshold else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_hf_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[[ 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEOCODING LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_unique_locations(buyers_df, sellers_df):\n",
    "    \"\"\"Extract all unique locations from buyers and sellers dataframes.\"\"\"\n",
    "    unique_locations = set()\n",
    "\n",
    "    for df, name in [(buyers_df, 'buyers'), (sellers_df, 'sellers')]:\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            # logging.info(f'Extracted locations: {location.lower().split('\\n')}')\n",
    "            unique_locations.update(locations)\n",
    "            \n",
    "\n",
    "    logging.info(f'Total unique locations found: {len(unique_locations)}')\n",
    "    return unique_locations\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode unique locations with caching.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"buyer_seller_matching\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3, error_wait_seconds=10.0)\n",
    "\n",
    "    # Ensure cache directory exists\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if cache_dir and not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        for location in unique_locations:\n",
    "\n",
    "            if location in geocode_cache:\n",
    "                continue  # Already cached\n",
    "            try:\n",
    "                logging.info(f'Geocoding location: {location}')\n",
    "                loc = geocode(location + \", Germany\")\n",
    "                if loc:\n",
    "                    geocode_cache[location] = {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "                    logging.info(f'Geocoded {location}: ({loc.latitude}, {loc.longitude})')\n",
    "                else:\n",
    "                    geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "                    logging.warning(f'Geocoding failed for location: {location}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Geocoding error for location '{location}': {e}\")\n",
    "                geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "\n",
    "def update_dataframe_with_geocodes(df, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Add latitude and longitude columns to the dataframe based on locations.\"\"\"\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            lat_list = []\n",
    "            lon_list = []\n",
    "            for loc in locations:\n",
    "                geocode_info = geocode_cache.get(loc, {'latitude': None, 'longitude': None})\n",
    "                if geocode_info['latitude'] is not None and geocode_info['longitude'] is not None:\n",
    "                    lat_list.append(geocode_info['latitude'])\n",
    "                    lon_list.append(geocode_info['longitude'])\n",
    "                else:\n",
    "                    # If geocoding failed, append None\n",
    "                    lat_list.append(None)\n",
    "                    lon_list.append(None)\n",
    "            # Convert lists to JSON strings for CSV compatibility\n",
    "            latitudes.append(json.dumps(lat_list))\n",
    "            longitudes.append(json.dumps(lon_list))\n",
    "\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    return df\n",
    "\n",
    "# Paths to input and output files\n",
    "buyer_filepath = './data/dejuna_buyer_latest_hf_nace.csv'\n",
    "sellers_filepath = './data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv'\n",
    "\n",
    "cache_path = './geocode_cache.db'\n",
    "\n",
    "# Load buyer and seller datasets\n",
    "\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "unique_locations = get_all_unique_locations(buyers_df, sellers_df)\n",
    "unique_locations\n",
    "# Geocode locations with caching\n",
    "geocode_locations(unique_locations, cache_path=cache_path)\n",
    "\n",
    "# Update dataframes with geocodes\n",
    "\n",
    "# # logging.info('Updating sellers dataframe with geocodes...')\n",
    "sellers_df = update_dataframe_with_geocodes(sellers_df, cache_path=cache_path)\n",
    "buyers_df = update_dataframe_with_geocodes(buyers_df, cache_path=cache_path)\n",
    "\n",
    "# # Save updated dataframes to new CSV files\n",
    "logging.info('Saving updated sellers dataframe...')\n",
    "sellers_output_file = sellers_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "buyers_output_file = buyer_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "sellers_df.to_csv(sellers_output_file, index=False)\n",
    "buyers_df.to_csv(buyers_output_file, index=False)\n",
    "logging.info('Geocoding process completed successfully.')\n",
    "print(f\"ðŸš€ Saved sellers data with assigned geocodes to: {buyers_output_file}\\n {sellers_output_file}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "'''Hamburg\n",
    "Schleswig-Holstein\n",
    "Berlin\n",
    "\n",
    "'''\n",
    "location1 = 'Berlin'\n",
    "location2 = 'Brandenburg'\n",
    "\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[10:30]\n",
    "purchase = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')[10:30]\n",
    "\n",
    "\n",
    "sales['processed_location'] = sales['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "purchase['processed_location'] = purchase['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "\n",
    "# # Check if any element of sales processed_location is in purchase processed_location\n",
    "purchase['latitude'] = purchase['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "purchase['longitude'] = purchase['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "sales['latitude'] = sales['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sales['longitude'] = sales['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "\n",
    "def match_locations(sales_locations, purchase_locations):\n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "\n",
    "# Create a dataframe to store matched locations\n",
    "matched_locations = []\n",
    "\n",
    "for idx, row in sales.iterrows():\n",
    "    for idx2, row2 in purchase.iterrows():\n",
    "        if match_locations(row.processed_location, row2.processed_location):\n",
    "            matched_locations.append({\n",
    "                'sales_id': idx,\n",
    "                'sales_location': row.location,\n",
    "                'sales_processed_location': row.processed_location,\n",
    "                'purchase_id': idx2,\n",
    "                'purchase_location': row2.location,\n",
    "                'sales_longitude': s_lon,\n",
    "                'sales_latitude': s_lat,\n",
    "                'purchase_longitude': p_lon,\n",
    "                'purchase_latitude': p_lat,\n",
    "                'purchase_processed_location': row2.processed_location,\n",
    "                'distance_km': None\n",
    "            })\n",
    "        else:\n",
    "            # Calculate distance between sales and purchase locations\n",
    "            sales_lat_lon = zip(row.latitude, row.longitude)\n",
    "            purchase_lat_lon = zip(row2.latitude, row2.longitude)\n",
    "            for s_lat, s_lon in sales_lat_lon:\n",
    "                for p_lat, p_lon in purchase_lat_lon:\n",
    "                    if s_lat is not None and s_lon is not None and p_lat is not None and p_lon is not None:\n",
    "                        print(f\"Calculating distance between ({s_lat}, {s_lon}) and ({p_lat}, {p_lon})\")\n",
    "                        distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                        print(f\"Distance: {distance}\")\n",
    "                        if distance <= 50:\n",
    "                            matched_locations.append({\n",
    "                                'sales_id': idx,\n",
    "                                'sales_processed_location': row.processed_location,\n",
    "                                'sales_location': row.location,\n",
    "                                'sales_longitude': s_lon,\n",
    "                                'sales_latitude': s_lat,\n",
    "                                'purchase_longitude': p_lon,\n",
    "                                'purchase_latitude': p_lat,\n",
    "                                'purchase_id': idx2,\n",
    "                                'purchase_location': row2.location,\n",
    "                                'purchase_processed_location': row2.processed_location,\n",
    "                                'distance_km': distance\n",
    "                            })\n",
    "matched_locations_df = pd.DataFrame(matched_locations)\n",
    "print(matched_locations_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Load synonyms CSV\n",
    "synonyms_df = pd.read_csv('./data/Updated_Keywords_and_Synonyms.csv')\n",
    "synonym_dict = {}\n",
    "for _, row in synonyms_df.iterrows():\n",
    "    keyword = row['Keyword'].lower()\n",
    "    synonyms = row.dropna().tolist()[1:]\n",
    "    synonyms = [syn.lower() for syn in synonyms]\n",
    "    synonym_dict[keyword] = synonyms\n",
    "\n",
    "# Define augmentation functions\n",
    "def extract_keywords(text, top_n=5):\n",
    "    # vectorizer = TfidfVectorizer(stop_words='german', max_features=top_n)\n",
    "    vectorizer = CountVectorizer(stop_words = german_stop_words) # Now use this in your pipeline\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "def augment_text_with_synonyms(text, top_n=5):\n",
    "    keywords = extract_keywords(text, top_n)\n",
    "    synonyms = []\n",
    "    for word in keywords:\n",
    "        synonyms.extend(synonym_dict.get(word, []))\n",
    "    synonyms = ' '.join(synonyms)\n",
    "    return f\"{text} {synonyms}\"\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text_de(text):\n",
    "    doc = nlp_de(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "def analyze_matches(matches_df, buyers_df, sellers_df):\n",
    "    \"\"\"Analyze matching results and print key metrics.\"\"\"\n",
    "    logging.info(\"\\n=== Matching Analysis ===\")\n",
    "    \n",
    "    total_buyers = len(buyers_df)\n",
    "    total_sellers = len(sellers_df)\n",
    "    total_matches = len(matches_df)\n",
    "    \n",
    "    logging.info(f\"Total buyers: {total_buyers}\")\n",
    "    logging.info(f\"Total sellers: {total_sellers}\") \n",
    "    logging.info(f\"Total matches found: {total_matches}\")\n",
    "    if total_buyers > 0:\n",
    "        logging.info(f\"Average matches per buyer: {total_matches/total_buyers:.2f}\")\n",
    "    else:\n",
    "        logging.info(\"No buyers to match against.\")\n",
    "\n",
    "    # Save top matches for manual review\n",
    "    top_matches = matches_df.head(10)\n",
    "    top_matches.to_csv('./matches/top_matches_for_review.csv', index=False)\n",
    "    logging.info(\"Saved top 10 matches for manual review\")\n",
    "    \n",
    "    return {\n",
    "        'total_matches': total_matches,\n",
    "        'matches_per_buyer': total_matches/total_buyers if total_buyers else 0,\n",
    "        'buyer_match_rate': len(matches_df['buyer_title'].unique())/total_buyers if total_buyers else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "# buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "# sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "\n",
    "# Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "logging.info('Loading the Sentence Transformer model...')\n",
    "# model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "# model_name = 'all-MiniLM-L12-v2'\n",
    "model_name ='aari1995/German_Semantic_STS_V2'\n",
    "\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model {model_name}: {e}\")\n",
    "\n",
    "# Encode sellers' combined_text\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = get_embedding_batch(seller_texts, model, batch_size=64)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "# Set similarity threshold\n",
    "similarity_threshold = 0.93\n",
    "# Optional text length filter\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info('Starting matching process...')\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarity scores to all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "\n",
    "    # Indices above threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "\n",
    "    for seller_idx in matching_indices:\n",
    "        seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "        confidence_score = sim_scores[seller_idx]\n",
    "        if confidence_score < similarity_threshold:\n",
    "            continue\n",
    "\n",
    "        confidence_scores.append(confidence_score)\n",
    "        \n",
    "        match = {\n",
    "            'buyer_date': buyer_row.get('date', ''),\n",
    "            'buyer_title': buyer_row.get('title', ''),\n",
    "            'buyer_description': buyer_row.get('description', ''),\n",
    "            'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "            'buyer_location': buyer_row.get('location', ''),\n",
    "            'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "            'seller_date': seller_row.get('date', ''),\n",
    "            'seller_title': seller_row.get('title', ''),\n",
    "            'seller_description': seller_row.get('description', ''),\n",
    "            'seller_long_description': seller_row.get('long_description', ''),\n",
    "            'seller_location': seller_row.get('location', ''),\n",
    "            'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "            \n",
    "            'similarity_score': confidence_score\n",
    "        }\n",
    "        matches.append(match)\n",
    "\n",
    "    # Progress logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "logging.info('Creating matches DataFrame...')\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Sort by confidence score\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "\n",
    "    # Save all matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    logging.info(f'Saved all matches: {len(matches_df)} records => {output_all}')\n",
    "\n",
    "    # Analyze results\n",
    "    metrics = analyze_matches(matches_df, buyers_df, sellers_df)\n",
    "    \n",
    "    # Optionally filter for high confidence\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= 0.95]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f'Saved high confidence matches: {len(high_conf_df)} records => {output_high_conf}')\n",
    "else:\n",
    "    logging.info('No matches found.')\n",
    "\n",
    "# Final memory cleanup\n",
    "del seller_embeddings\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:22:48,845 - INFO - Loading datasets...\n",
      "2025-02-03 01:22:48,983 - INFO - Preprocessing buyers' text fields...\n",
      "2025-02-03 01:22:51,031 - INFO - Preprocessing sellers' text fields...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].str.replace('>', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:29:01,865 - INFO - Processing locations...\n",
      "2025-02-03 01:29:01,931 - INFO - Combining text fields...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_locations(sales_locations, purchase_locations):\n",
    "    \"\"\"\n",
    "    Check if any element of sales_locations is in purchase_locations.\n",
    "    \"\"\"\n",
    "    print(f'purchase_locations: {purchase_locations}, sales_locations: {sales_locations}, {any(loc in purchase_locations for loc in sales_locations)}')\n",
    "    \n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "# def match_locations(locations_input, target_location):\n",
    "#     # Step 1: Normalize and parse the locations into a list of location parts\n",
    "#     def parse_location(location):\n",
    "#         return [part.strip().lower() for part in location.split(\">\")]\n",
    "\n",
    "#     # Parse the target location\n",
    "#     target_parts = parse_location(target_location)\n",
    "    \n",
    "#     # Split the multiple input locations by line breaks and parse them\n",
    "#     locations = locations_input.strip().split(\"\\n\")\n",
    "#     parsed_locations = [parse_location(loc) for loc in locations]\n",
    "\n",
    "#     # Step 2: Function to compare the parsed locations with the target\n",
    "#     def compare_location(loc1_parts, loc2_parts):\n",
    "#         max_level = max(len(loc1_parts), len(loc2_parts))\n",
    "#         match_score = 0\n",
    "\n",
    "#         for level in range(max_level):\n",
    "#             loc1_value = loc1_parts[level] if level < len(loc1_parts) else None\n",
    "#             loc2_value = loc2_parts[level] if level < len(loc2_parts) else None\n",
    "            \n",
    "#             if loc1_value and loc2_value:\n",
    "#                 # Exact match at this level\n",
    "#                 if loc1_value == loc2_value:\n",
    "#                     match_score += 1\n",
    "#                 else:\n",
    "#                     break  # If any level does not match, break early\n",
    "#             elif loc1_value is None and loc2_value is None:\n",
    "#                 continue  # Both are missing, no issue\n",
    "#             else:\n",
    "#                 match_score += 0.5  # Partial match (one location is more specific)\n",
    "\n",
    "#         return match_score\n",
    "\n",
    "#     # Step 3: Check if any location matches (either exact or partial)\n",
    "#     # for loc_parts in parsed_locations:\n",
    "#     #     match_score = compare_location(loc_parts, target_parts)\n",
    "#     #     if match_score > 0:  # If there's a partial match or exact match\n",
    "#     #         return True\n",
    "#     for loc_parts in parsed_locations:\n",
    "#         match_score = compare_location(loc_parts, target_parts)\n",
    "#         print(\" > \".join(loc_parts))\n",
    "#         if match_score > 0:  \n",
    "#             if target_coords and location_coordinates.get(\" > \".join(loc_parts)):\n",
    "#                 loc_coords = location_coordinates.get(\" > \".join(loc_parts))\n",
    "#                 if loc_coords:\n",
    "#                     lat1, lon1 = target_coords\n",
    "#                     lat2, lon2 = loc_coords\n",
    "#                     distance = haversine(lat1, lon1, lat2, lon2)\n",
    "#                     if distance <= max_distance_km:\n",
    "#                         return True \n",
    "#             else:\n",
    "#                 return True\n",
    "\n",
    "#     # If no matches found, return False\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:29:11,732 - INFO - Loading SentenceTransformer and CrossEncoder models...\n",
      "2025-02-03 01:29:11,743 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-03 01:29:11,743 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n",
      "2025-02-03 01:29:14,118 - INFO - Use pytorch device: cpu\n",
      "2025-02-03 01:29:14,134 - INFO - Encoding sellers' text...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a0102b9b6b47f0b8857d2c165d1cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:37:32,960 - INFO - Sellers' embeddings generated.\n"
     ]
    }
   ],
   "source": [
    "# # Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Load Models\n",
    "# -----------------------------------\n",
    "logging.info(\"Loading SentenceTransformer and CrossEncoder models...\")\n",
    "# bi_encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer(\"xlm-roberta-base\")\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer('aari1995/German_Semantic_STS_V2')\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Encode Sellers' Text\n",
    "# -----------------------------------\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:37:33,803 - INFO - Starting matching process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad3bfae8e0646d8a7d756d948bc073f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239f235bc8614a588f5a9ba911fdf2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:41:14,620 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Baden-WÃ¼rttemberg > Stuttgart > Stuttgart\n",
      "2025-02-03 01:41:14,644 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Baden-WÃ¼rttemberg > Karlsruhe > Calw\n",
      "2025-02-03 01:41:14,652 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Bayern > Mittelfranken > Roth\n",
      "2025-02-03 01:41:14,661 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Baden-WÃ¼rttemberg > Stuttgart > Esslingen\n",
      "2025-02-03 01:41:14,673 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Baden-WÃ¼rttemberg > Stuttgart > Esslingen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6941920624cbca729228cb64015ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcdcc31c66946b8bc954961f956ca36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4832998ce7dd424b8b61deb96b2ffd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afb9baa07bf41ed8b80e63e8f7d8d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201b3de726324241a4f30c3755777ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:44:43,384 - INFO - Match found: Buyer location: Nordrhein-Westfalen > DÃ¼sseldorf, Seller location: Nordrhein-Westfalen > KÃ¶ln > KÃ¶ln\n",
      "2025-02-03 01:44:43,392 - INFO - Match found: Buyer location: Nordrhein-Westfalen > DÃ¼sseldorf, Seller location: Nordrhein-Westfalen > DÃ¼sseldorf > Rhein-Kreis Neuss\n",
      "2025-02-03 01:44:43,401 - INFO - Match found: Buyer location: Nordrhein-Westfalen > DÃ¼sseldorf, Seller location: Nordrhein-Westfalen > DÃ¼sseldorf > DÃ¼sseldorf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fc2e5396dd4cf0a308ca384daf9fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803695c1723b4c84b054b98497a2bece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca7a7003d364ddfbdab794fc71604cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:46:45,840 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen\n",
      "2025-02-03 01:46:45,845 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Niedersachsen\n",
      "2025-02-03 01:46:45,851 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Hagen\n",
      "2025-02-03 01:46:45,853 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen\n",
      "2025-02-03 01:46:45,855 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen\n",
      "2025-02-03 01:46:45,860 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Arnsberg > MÃ¤rkischer Kreis\n",
      "2025-02-03 01:46:45,870 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Arnsberg > MÃ¤rkischer Kreis\n",
      "2025-02-03 01:46:45,873 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Detmold > Lippe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb160896e90048e291ba9750321caea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea9f460d0e4b4180218d904baeb9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e0765241be4720acca883124f61a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8300f2a8ebb54ba587773edc15974def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7655ccf170004533a5a1c1f066122dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:48:23,764 - INFO - Match found: Buyer location: Niedersachsen > LÃ¼neburg\n",
      "Hamburg\n",
      "Schleswig-Holstein\n",
      "Mecklenburg-Vorpommern, Seller location: Schleswig-Holstein\n",
      "2025-02-03 01:48:23,773 - INFO - Match found: Buyer location: Niedersachsen > LÃ¼neburg\n",
      "Hamburg\n",
      "Schleswig-Holstein\n",
      "Mecklenburg-Vorpommern, Seller location: Schleswig-Holstein > Schleswig-Holstein\n",
      "2025-02-03 01:48:23,775 - INFO - Match found: Buyer location: Niedersachsen > LÃ¼neburg\n",
      "Hamburg\n",
      "Schleswig-Holstein\n",
      "Mecklenburg-Vorpommern, Seller location: Schleswig-Holstein > Schleswig-Holstein\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0aaa33b24224d61b103c26f30073aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41065467b7e94063887bca87b7faab3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eab2a1b0f904c8cb5619d48966812dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2bf908c2b94b248c1281df695678f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd3f65a0994cbcb1b2f54ac12a0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc06212eefa40c5ac490ae3fceb0dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:53:04,001 - INFO - Match found: Buyer location: Bremen\n",
      "Niedersachsen, Seller location: Niedersachsen\n",
      "2025-02-03 01:53:04,014 - INFO - Match found: Buyer location: Bremen\n",
      "Niedersachsen, Seller location: Niedersachsen > LÃ¼neburg > Verden\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccb1f853da3487fa716e3679e6aed67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0ad7c9c969424da0dd79d19c0db2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:53:46,616 - INFO - Match found: Buyer location: Berlin\n",
      "Sachsen > Leipzig\n",
      "Sachsen > Dresden\n",
      "Brandenburg\n",
      "Niedersachsen > Hannover, Seller location: Berlin > Berlin > Berlin\n",
      "2025-02-03 01:53:46,633 - INFO - Match found: Buyer location: Berlin\n",
      "Sachsen > Leipzig\n",
      "Sachsen > Dresden\n",
      "Brandenburg\n",
      "Niedersachsen > Hannover, Seller location: Brandenburg > Brandenburg > Potsdam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3479d8c5a7a445d984fdc3b104bfc5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2c08cb2e254ecc8e89c74ec057168a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7ff91a2ea44ca99003d3ea20327c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a02fdec9949d69901bb9bde9d7b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaddcbb07254fa5886cf0c503f72dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594e5f2e1c444ebba808a0eb9d52825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a951ffa3748fda15539da0e69a323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68a7dc89cb94d17a36da1107b7eb312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e91011c68ee4c6581bdb133a9110fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfc035294ed492ba6d163a9ebeaf76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ea4c1cc1674d19a40828190331e7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9317d19876a64104a99a486fc5e73a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f3e0aa0994cd780f18f9a3128a6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fe0881c53946aca6ba5fc439d58b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199991a08357420d83b7cefd45a5c084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933035a96a314fe9b14dcbe74b3004a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c862638f6b46359d482aecb4aef65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5893fb351b94e329be6feb7af07db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:00:50,616 - INFO - Match found: Buyer location: Hessen\n",
      "Sachsen\n",
      "Baden-WÃ¼rttemberg, Seller location: Baden-WÃ¼rttemberg\n",
      "2025-02-03 02:00:50,619 - INFO - Match found: Buyer location: Hessen\n",
      "Sachsen\n",
      "Baden-WÃ¼rttemberg, Seller location: Baden-WÃ¼rttemberg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3837da8c459b4977a0da916efe04bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce02a277571746f3867f2fb5a6630f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34e49c373284b3aa5b6cce74dfdbbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7917eeff86436bb1b6eae80ba8531b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12de27bb071461eb1f0d7c95d5c70eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32c776c635442d6b03466d2fb23cba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0bd0fbbc7c4f669a5ba1ca790b7a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750115b323b04634a4f3dc9eb813aa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbae5fe7c65e4497bc67cab274770063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b092fac944a05ba0f6a3ca218342f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fb583892944c86836a5c306d00667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d8daf15fc6498da60ffe0c4d4f9d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c884aadbbf4ae3b403cb476b77247a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3261b83871ca4e28947514f9d0ad0001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a439634054314b7aa80ba2ee6823f958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9437595219634845adefeba89f49826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dafd2126a84b12b685e5c2731c76a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af283fca2f34eb3b46295c02fb1ce82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7accdf4fa098434286fd924f797e4af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a50ca0e9e74e6a8379c29e412f7c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4f2e5ee9d7470094ad620f91873261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:08:09,296 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Baden-WÃ¼rttemberg > Stuttgart\n",
      "2025-02-03 02:08:09,300 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Bayern\n",
      "2025-02-03 02:08:09,309 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Bayern\n",
      "2025-02-03 02:08:09,311 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg\n",
      "Bayern, Seller location: Bayern\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555d463716df4dec9d6507472ddc20fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a05bd8f444d47ae902f3cb6777a8cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6111e5434b4f1185967bd454a09de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db92db204b486a909403a66b00b677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d956c226ceb494a84fc8174830e8839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739baf5c5b904c6e80730fba1bbc0031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ebf44cde3e4a15816401829bfe1ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:09:36,655 - INFO - Match found: Buyer location: Hamburg\n",
      "Schleswig-Holstein\n",
      "Baden-WÃ¼rttemberg > Freiburg\n",
      "Bayern > Oberbayern, Seller location: Baden-WÃ¼rttemberg > Freiburg > Emmendingen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78d7708405e4e30a91fb70fb7445f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716990c04de34989a37a407cad6a021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:10:33,765 - INFO - Match found: Buyer location: Baden-WÃ¼rttemberg, Seller location: Baden-WÃ¼rttemberg > Karlsruhe > Enzkreis\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d9e4b69a984cd7b8605fc2fd304f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1669f3b489a46889412ca13d91c635a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adddff5510e4fc7af944f41e6599729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78207126103c4d2bb55208ff26090278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ee3130ea3f4034bea3df9143b9863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024277f81e524ee58afaa668d6c0e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acefee834534a88952e9a19740b400c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501c98557b5c4ff5abc98036728fad6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0968f2ae8ba749ba9f3696e633c7f3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d28a079c6a24474b818f84f56e36ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002004255ead48ebbb2b1ae56887c632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec953acb6bc40fc90a2de0b28888283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4126c6bd1a89492d9927a2052af2905b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34634ec2dd84e93ae6f5b8da67039da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1119219f4a24f16b95435e6d34a1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c00091f3f14aa38b779e6ffda695f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1222eff20b1c412182827d9f816a9427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:13:32,616 - INFO - Match found: Buyer location: Hamburg\n",
      "Schleswig-Holstein\n",
      "Berlin, Seller location: Berlin > Berlin > Berlin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4710dd7a5c145d084bf8d6258a4b748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a57ffc566841eaa3cf3f17c9209c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9dd3e72f1c4525bc3faec2de9541fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38514f8954747c08379f54b5cce614e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa7d8e83d424b739214f98ee3371422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:15:23,744 - INFO - Match found: Buyer location: Nordrhein-Westfalen > MÃ¼nster, Seller location: Nordrhein-Westfalen > MÃ¼nster > Steinfurt\n",
      "2025-02-03 02:15:23,753 - INFO - Match found: Buyer location: Nordrhein-Westfalen > MÃ¼nster, Seller location: Nordrhein-Westfalen > MÃ¼nster > MÃ¼nster\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85cc15631584b628641650800a33416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d81b6eb75594afa9ab8422e65ea5e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53a60e04d4c480288e2ed3703398a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd40eb65efc34c4f8792a4516e597470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84ce37ad6ff452baa95abb841f1b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c85d6eb0404b099ff61675b1afd57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e177250ea7d4a49a831eb1b9bbb7c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3681395fbd4649aaac717809698c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59caed0ad44e416c881f03a7bedf2abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3eaf1150bcf4229ab07ae08d8cfcdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d42f36c69044018b74a89aa082602a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87b55535bac4d39b329c999edd70fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d31072219c94699805e6cd3353d60aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6cec3931e044cc9c0586b334bcac63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffd013fdf6842bb981dac47e88452b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:17:54,028 - INFO - Match found: Buyer location: Bayern > Oberbayern > MÃ¼nchen, Seller location: Bayern > Oberbayern\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9616efca695646419f8a3d37a5b867f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7220e4bbcb6247f18b87592b0978d78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:19:36,175 - INFO - Creating matches DataFrame...\n",
      "2025-02-03 02:19:36,888 - INFO - Saved all matches: 35 records => ./matches/nlp_business_all_matches_03_02-19.csv\n",
      "2025-02-03 02:19:36,903 - INFO - Saved high confidence matches: 35 => ./matches/nlp_business_high_conf_03_02-19.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 6. Matching Parameters\n",
    "# -----------------------------------\n",
    "similarity_threshold = 0.60\n",
    "cross_encoder_threshold = 0.65\n",
    "top_n = 100  # Number of top candidates to re-rank per buyer\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info(\"Starting matching process...\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 7. Matching Loop\n",
    "# -----------------------------------\n",
    "matched_locations = []\n",
    "\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyers_df.iloc[i]['latitude']\n",
    "    buyer_longitudes = buyers_df.iloc[i]['longitude']\n",
    "    buyer_locations = buyers_df.iloc[i]['location']\n",
    "\n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute cosine similarities with all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "    \n",
    "    # Get indices of sellers with similarity >= threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue  # No matches above threshold\n",
    "    \n",
    "    # Select top N matches based on similarity scores\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:top_n]]\n",
    "    \n",
    "    # Prepare pairs for cross-encoder\n",
    "    buyer_texts = [buyer_text] * len(top_indices)\n",
    "    seller_texts_top = [sellers_df.iloc[idx]['combined_text'] for idx in top_indices]\n",
    "    pairs = list(zip(buyer_texts, seller_texts_top))\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    cross_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Filter based on cross-encoder threshold\n",
    "    for seller_idx, cross_score in zip(top_indices, cross_scores):\n",
    "\n",
    "\n",
    "        if cross_score >= cross_encoder_threshold:\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            # if match_locations(seller_locations, buyer_locations):\n",
    "            #     location_match = True\n",
    "            # else:\n",
    "            # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km threshold\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "            if location_match:\n",
    "                logging.info(f\"Match found: Buyer location: {buyer_locations}, Seller location: {seller_locations}\")\n",
    "                match = {\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller_row.get('id', ''),\n",
    "                    'seller_date': seller_row.get('date', ''),\n",
    "                    'seller_title': seller_row.get('title', ''),\n",
    "                    'seller_description': seller_row.get('description', ''),\n",
    "                    'seller_long_description': seller_row.get('long_description', ''),\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'seller_source': seller_row.get('url', ''),\n",
    "                    'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "                    \n",
    "                    'similarity_score': cross_score,\n",
    "                    'distance_km': distance_km if distance_km else 'Within processed locations'\n",
    "\n",
    "                }\n",
    "                matches.append(match)\n",
    "                confidence_scores.append(cross_score)\n",
    "    \n",
    "    # Progress Logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 8. Create Matches DataFrame\n",
    "# -----------------------------------\n",
    "logging.info(\"Creating matches DataFrame...\")\n",
    "matches_df = pd.DataFrame(matches)\n",
    "        \n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "# del seller_embeddings\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# 2025-01-21 12:59:12,950 - INFO - Match found: Buyer location: ['Baden-WÃ¼rttemberg', 'Bayern'], Seller location: ['Rhein-Neckar-Kreis']\n",
    "\n",
    "# purchase_locations: ['Baden-WÃ¼rttemberg', 'Bayern'], sales_locations: ['Rhein-Neckar-Kreis'], False\n",
    "location1 = 'Brandenburg > Brandenburg > Potsdam'\n",
    "location2 = 'OsnabrÃ¼ck'\n",
    "#  {'Berlin',\n",
    "#  'Brandenburg',\n",
    "#  'Dresden',\n",
    "#  'Hannover',\n",
    "#  'Leipzig',\n",
    "#  'Niedersachsen',\n",
    "#  'Sachsen'}\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.distance import geodesic\n",
    "# import math\n",
    "\n",
    "# # Function to geocode a location string into latitude and longitude\n",
    "# def geocode(location):\n",
    "#     \"\"\"\n",
    "#     Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "#     Args:\n",
    "#         location (str): The location string to geocode.\n",
    "    \n",
    "#     Returns:\n",
    "#         tuple: Latitude and longitude of the location, or None if not found.\n",
    "#     \"\"\"\n",
    "#     geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "#     location_data = geolocator.geocode(location)\n",
    "#     if location_data:\n",
    "#         return location_data.latitude, location_data.longitude\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # Haversine formula to calculate distance between two points (lat1, lon1) and (lat2, lon2)\n",
    "# def haversine(lat1, lon1, lat2, lon2):\n",
    "#     R = 6371  # Radius of the Earth in kilometers\n",
    "#     phi1 = math.radians(lat1)\n",
    "#     phi2 = math.radians(lat2)\n",
    "#     delta_phi = math.radians(lat2 - lat1)\n",
    "#     delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "#     a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "#     c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "#     distance = R * c  # Distance in kilometers\n",
    "#     return distance\n",
    "\n",
    "\n",
    "# def match_locations(locations_input, target_location, max_distance_km=50):\n",
    "#     # Step 1: Normalize and parse the locations into a list of location parts\n",
    "#     def parse_location(location):\n",
    "#         return [part.strip().lower() for part in location.split(\">\")]\n",
    "\n",
    "#     # Parse the target location\n",
    "#     target_parts = parse_location(target_location)\n",
    "#     target_coords = geocode(target_location)  # Get coordinates of the target location\n",
    "    \n",
    "#     if not target_coords:\n",
    "#         return False  # If target location couldn't be geocoded, return False\n",
    "\n",
    "#     # Split the multiple input locations by line breaks and parse them\n",
    "#     locations = locations_input.strip().split(\"\\n\")\n",
    "#     parsed_locations = [parse_location(loc) for loc in locations]\n",
    "\n",
    "#     # Step 2: Function to compare the parsed locations with the target\n",
    "#     def compare_location(loc1_parts, loc2_parts):\n",
    "#         max_level = max(len(loc1_parts), len(loc2_parts))\n",
    "#         match_score = 0\n",
    "\n",
    "#         for level in range(max_level):\n",
    "#             loc1_value = loc1_parts[level] if level < len(loc1_parts) else None\n",
    "#             loc2_value = loc2_parts[level] if level < len(loc2_parts) else None\n",
    "            \n",
    "#             if loc1_value and loc2_value:\n",
    "#                 # Exact match at this level\n",
    "#                 if loc1_value == loc2_value:\n",
    "#                     match_score += 1\n",
    "#                 else:\n",
    "#                     break  # If any level does not match, break early\n",
    "#             elif loc1_value is None and loc2_value is None:\n",
    "#                 continue  # Both are missing, no issue\n",
    "#             else:\n",
    "#                 match_score += 0.5  # Partial match (one location is more specific)\n",
    "\n",
    "#         return match_score>0\n",
    "\n",
    "    # # Step 3: Check if any location matches (either exact or partial) and is within proximity\n",
    "    # for loc_parts in parsed_locations:\n",
    "    #     match_score = compare_location(loc_parts, target_parts)\n",
    "\n",
    "    #     # If there's a partial or exact match, we also check the geographical proximity\n",
    "    #     if match_score > 0:  # If there's a name match\n",
    "    #         location_str = \" > \".join(loc_parts)\n",
    "    #         loc_coords = geocode(location_str)  # Get coordinates of the input location\n",
    "            \n",
    "    #         if loc_coords:\n",
    "    #             lat1, lon1 = target_coords\n",
    "    #             lat2, lon2 = loc_coords\n",
    "    #             distance=geodesic(target_coords, loc_coords).kilometers\n",
    "    #             logging.info(f\"Distance between '{target_location}' and '{location_str}': {distance:.2f} km\")\n",
    "    #             # distance = haversine(lat1, lon1, lat2, lon2)\n",
    "    #             if distance <= max_distance_km:\n",
    "    #                 return True  # Match found within the proximity\n",
    "    #         else:\n",
    "    #             continue  # If geocoding failed for the input location, skip it\n",
    "\n",
    "    # # If no matches found, return False\n",
    "    # return False\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# locations_input = '''Berlin\n",
    "# Sachsen > Leipzig\n",
    "# Sachsen > Dresden\n",
    "# Brandenburg\n",
    "# Niedersachsen > Hannover'''\n",
    "\n",
    "# target_location = '''Brandenburg > Brandenburg'''\n",
    "\n",
    "# # Call the function\n",
    "# result = match_locations(locations_input, target_location)\n",
    "# print(result)  # True if any match (name + proximity), False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sellers and buyers files into dataframes\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "\n",
    "# Add origin column to both dataframes\n",
    "sellers_df['origin'] = 'seller'\n",
    "buyers_df['origin'] = 'buyer'\n",
    "buyers_df['branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} > {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Concatenate both dataframes\n",
    "combined_df = pd.concat([sellers_df, buyers_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE Labels for Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = combined_df \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"ðŸš€ Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"ðŸš€ Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"ðŸš€ Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "    lambda x: ' '.join(x.split('>'))\n",
    ")\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        # row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"ðŸš€ Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"ðŸš€ Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = \"buyers_sellers_combined_nace.csv\"\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"ðŸš€ Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1= pd.read_csv(\"./matches/nlp_business_all_matches_16_19-31.csv\")\n",
    "temp2= pd.read_csv(\"./matches/nlp_business_all_matches_15_13-11.csv\")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_df = pd.concat([temp1, temp2])\n",
    "# Get all duplicates based on specific columns\n",
    "# Get all rows that only exist once in the dataframe\n",
    "unique_rows = combined_df.drop_duplicates(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)\n",
    "duplicates = unique_rows[unique_rows.duplicated(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)]\n",
    "\n",
    "# unique_rows.to_csv('./matches/unique_rows.csv', index=False)\n",
    "# Display the unique rows\n",
    "# print(unique_rows)\n",
    "print(duplicates)\n",
    "# Display the duplicates\n",
    "# print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def extract_locations(location_string):\n",
    "    \"\"\"\n",
    "    Extracts locations from a string with nested structure.\n",
    "    \n",
    "    Args:\n",
    "        location_string (str): Input string containing locations in nested format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted locations.\n",
    "    \"\"\"\n",
    "    locations = []\n",
    "\n",
    "    # Split the input into lines\n",
    "    lines = location_string.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Strip whitespace from the line\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Ignore empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # If the line contains '>', it indicates nested locations\n",
    "        if '>' in line:\n",
    "            # Extract the specific location after the last '>'\n",
    "            nested_location = line.split('>')[-1].strip()\n",
    "            locations.append(nested_location)\n",
    "        else:\n",
    "            # Append the standalone location\n",
    "            locations.append(line)\n",
    "\n",
    "    return locations\n",
    "\n",
    "def get_location_coordinates(location):\n",
    "    \"\"\"\n",
    "    Mock function to return coordinates for a given location.\n",
    "    Replace this with an actual geocoding API in production.\n",
    "\n",
    "    Args:\n",
    "        location (str): The location name.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location.\n",
    "    \"\"\"\n",
    "    # Mock coordinates for demonstration purposes\n",
    "    coordinates = {\n",
    "        \"Celle\": (52.6226, 10.0815),\n",
    "        \"Hannover\": (52.3759, 9.7320),\n",
    "        \"Karlsruhe\": (49.0069, 8.4037),\n",
    "        \"Mannheim\": (49.4875, 8.4660),\n",
    "        \"Goettingen\": (51.5413, 9.9158),\n",
    "    }\n",
    "    return coordinates.get(location, None)\n",
    "\n",
    "def locations_match(s1, s2):\n",
    "    \"\"\"\n",
    "    Checks if the locations extracted from two strings have any matches.\n",
    "    If no exact match, it calculates the distance between locations.\n",
    "\n",
    "    Args:\n",
    "        s1 (str): First input string containing locations.\n",
    "        s2 (str): Second input string containing locations.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if there are matching locations or distance < 50KM, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract locations from both strings\n",
    "    locations1 = set(_extract_location_parts(s1))\n",
    "    locations2 = set(_extract_location_parts(s2))\n",
    "\n",
    "    print(f\"Locations 1: {locations1}\")\n",
    "    print(f\"Locations 2: {locations2}\")\n",
    "    # Check for exact match first\n",
    "    if not locations1.isdisjoint(locations2):\n",
    "        return True\n",
    "\n",
    "    # If no exact match, calculate distances\n",
    "    for loc1 in locations1:\n",
    "        for loc2 in locations2:\n",
    "            coord1 = get_location_coordinates(loc1)\n",
    "            coord2 = get_location_coordinates(loc2)\n",
    "\n",
    "            if coord1 and coord2:\n",
    "                distance = geodesic(coord1, coord2).kilometers\n",
    "                print(f\"Distance between {loc1} and {loc2}: {distance} KM\")\n",
    "                if distance < 50:\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "s1 = \"Niedersachen > Celle\"\n",
    "s2 = \"Niedersachsen > Goettingen\"\n",
    "\n",
    "output = locations_match(s1, s2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, CrossEncoder\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshelve\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import os\n",
    "import shelve\n",
    "from unidecode import unidecode\n",
    "import gc\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Location Processing Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract hierarchical location components\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return []\n",
    "    try:\n",
    "        return [part.strip() for part in re.split(r'>|\\n', location) if part.strip()]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Location parsing error: {e}\")\n",
    "        return []\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode locations with hierarchical fallback\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"business_matcher\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "    \n",
    "    with shelve.open(cache_path) as cache:\n",
    "        for loc in unique_locations:\n",
    "            if loc not in cache:\n",
    "                try:\n",
    "                    # Try full location first, then fallback through hierarchy\n",
    "                    parts = _extract_location_parts(loc)\n",
    "                    for i in range(len(parts)):\n",
    "                        query = \", \".join(parts[-i-1:]) + \", Germany\"\n",
    "                        result = geocode(query)\n",
    "                        if result:\n",
    "                            cache[loc] = (result.latitude, result.longitude)\n",
    "                            break\n",
    "                    if loc not in cache:\n",
    "                        cache[loc] = (None, None)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Geocoding failed for {loc}: {e}\")\n",
    "                    cache[loc] = (None, None)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Text Processing Functions (German-optimized)\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_german_text(text):\n",
    "    \"\"\"Basic cleaning for German business text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Remove URLs and special characters\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\b\\d{10,}\\b|[^\\wÃ¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ\\s]', ' ', text)\n",
    "    \n",
    "    # Handle common business abbreviations\n",
    "    replacements = {\n",
    "        r'\\bMio\\b': 'millionen',\n",
    "        r'\\bTsd\\b': 'tausend',\n",
    "        r'\\bca\\.': 'circa',\n",
    "        r'\\bz\\.B\\.': 'zum beispiel'\n",
    "    }\n",
    "    for pattern, repl in replacements.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    \n",
    "    return unidecode(text).lower().strip()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Core Matching Logic\n",
    "# -------------------------------------------------------------------------\n",
    "# Load data\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')[:100]\n",
    "\n",
    "# Process locations\n",
    "logging.info(\"Processing locations...\")\n",
    "for df in [buyers_df, sellers_df]:\n",
    "    df['location_parts'] = df['location'].apply(_extract_location_parts)\n",
    "    df['primary_location'] = df['location_parts'].apply(\n",
    "        lambda x: x[-1] if x else None\n",
    "    )\n",
    "\n",
    "# Geocode locations\n",
    "unique_locations = set(\n",
    "    buyers_df['primary_location'].dropna().tolist() +\n",
    "    sellers_df['primary_location'].dropna().tolist()\n",
    ")\n",
    "geocode_locations(unique_locations)\n",
    "\n",
    "# Load geocodes\n",
    "with shelve.open('geocode_cache.db') as cache:\n",
    "    sellers_df['coords'] = sellers_df['primary_location'].apply(\n",
    "        lambda x: cache.get(x, (None, None))\n",
    "    )\n",
    "    buyers_df['coords'] = buyers_df['primary_location'].apply(\n",
    "        lambda x: cache.get(x, (None, None))\n",
    "    )\n",
    "\n",
    "# Combine text fields\n",
    "def combine_text(row):\n",
    "    return ' '.join([\n",
    "        str(row.get('title', '')),\n",
    "        str(row.get('description', '')),\n",
    "        str(row.get('long_description', ''))\n",
    "    ])\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "def initialize_models():\n",
    "    \"\"\"Explicit model/tokenizer configuration\"\"\"\n",
    "    # Configure transformer with separate tokenizer args\n",
    "    bi_encoder= SentenceTransformer(\n",
    "    'T-Systems-onsite/german-roberta-sentence-transformer-v2',\n",
    "    device='cuda',  # Use GPU if available\n",
    "    tokenizer_kwargs={\n",
    "        'use_fast': True,\n",
    "        'model_max_length': 512,\n",
    "        'truncation': True\n",
    "    }\n",
    ")\n",
    "    # pooling = models.Pooling(transformer.get_word_embedding_dimension())\n",
    "    \n",
    "    # bi_encoder = SentenceTransformer(modules=[transformer, pooling])\n",
    "    cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "    return bi_encoder, cross_encoder\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text, axis=1)\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text, axis=1)\n",
    "\n",
    "# Clean text\n",
    "logging.info(\"Cleaning text...\")\n",
    "sellers_df['clean_text'] = sellers_df['combined_text'].apply(clean_german_text)\n",
    "buyers_df['clean_text'] = buyers_df['combined_text'].apply(clean_german_text)\n",
    "sellers_df['id'] = sellers_df.index\n",
    "# Initialize models\n",
    "logging.info(\"Initializing models...\")\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/german-roberta-sentence-transformer-v2')\n",
    "# cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "bi_encoder, cross_encoder = initialize_models()\n",
    "# Encode sellers\n",
    "logging.info(\"Encoding sellers...\")\n",
    "seller_embeddings = bi_encoder.encode(\n",
    "    sellers_df['clean_text'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = seller_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(seller_embeddings)\n",
    "\n",
    "# Matching parameters\n",
    "SEMANTIC_THRESHOLD = 0.60\n",
    "CROSS_ENCODER_THRESHOLD = 0.65\n",
    "MAX_DISTANCE_KM = 50\n",
    "\n",
    "# Process buyers\n",
    "matches = []\n",
    "for buyer_idx, buyer_row in buyers_df.iterrows():\n",
    "    try:\n",
    "        # Semantic search\n",
    "        buyer_embedding = bi_encoder.encode([buyer_row['clean_text']])\n",
    "        distances, indices = index.search(buyer_embedding, 100)\n",
    "        # Filter candidates\n",
    "        candidates = []\n",
    "        for seller_idx, score in zip(indices[0], distances[0]):\n",
    "            if score >= SEMANTIC_THRESHOLD:\n",
    "                seller = sellers_df.iloc[seller_idx]\n",
    "                candidates.append((seller, score))\n",
    "        \n",
    "        # Location filtering\n",
    "        valid_matches = []\n",
    "        for seller, score in candidates:\n",
    "            # Hierarchical location match\n",
    "            location_match = any(\n",
    "                loc in buyer_row['location_parts']\n",
    "                for loc in seller['location_parts']\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Distance check\n",
    "            distance = None\n",
    "            if not location_match and None not in [buyer_row['coords'], seller['coords']]:\n",
    "                buyer_latitude, buyer_longitude= buyer_row['coords'].get('latitude', None), buyer_row['coords'].get('longitude', None)\n",
    "                seller_latitude, seller_longitude = seller['coords'].get('latitude', None), seller['coords'].get('longitude', None)\n",
    "                distance = geodesic((buyer_latitude, buyer_longitude),(seller_latitude, seller_longitude)).km\n",
    "                location_match = distance <= MAX_DISTANCE_KM\n",
    "\n",
    "            if location_match:\n",
    "                valid_matches.append({\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    \n",
    "                    # 'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    # 'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    # 'buyer_location': buyer_row.get('location', ''),\n",
    "                    # 'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller.get('id', ''),\n",
    "                    # 'seller_date': seller.get('date', ''),\n",
    "                    'seller_title': seller.get('title', ''),\n",
    "                    'seller_description': seller.get('description', ''),\n",
    "                    # 'seller_long_description': seller.get('long_description', ''),\n",
    "                    # 'seller_location': seller.get('location', ''),\n",
    "                    'seller_source': seller.get('url', ''),\n",
    "                    # 'seller_nace_code': seller.get('nace_code', ''),\n",
    "\n",
    "\n",
    "                    'semantic_score': score,\n",
    "                    'distance_km': round(distance, 2) if distance else 'Hierarchical match',\n",
    "                    'buyer_location': ' > '.join(buyer_row['location_parts']),\n",
    "                    'seller_location': ' > '.join(seller['location_parts'])\n",
    "                })\n",
    "\n",
    "        # Cross-encoder reranking\n",
    "        if valid_matches:\n",
    "            pairs = [(buyer_row['clean_text'], sellers_df.iloc[m['seller_id']]['clean_text']) \n",
    "                    for m in valid_matches]\n",
    "            cross_scores = cross_encoder.predict(pairs)\n",
    "            \n",
    "            for match, cross_score in zip(valid_matches, cross_scores):\n",
    "                if cross_score >= CROSS_ENCODER_THRESHOLD:\n",
    "                    match['confidence_score'] = cross_score\n",
    "                    matches.append(match)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing buyer {buyer_idx}:{buyer_row['coords']}: {e}\")\n",
    "\n",
    "# # Save results\n",
    "# if matches:\n",
    "#     matches_df = pd.DataFrame(matches)\n",
    "#     matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "#     matches_df.to_csv('./business_matches.csv', index=False)\n",
    "#     logging.info(f\"Saved {len(matches_df)} matches to business_matches.csv\")\n",
    "# else:\n",
    "#     logging.info(\"No matches found\")\n",
    "\n",
    "# logging.info(\"Creating matches DataFrame...\")\n",
    "# matches_df = pd.DataFrame(matches)\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "del seller_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:07:25,063 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-03 01:07:25,064 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1dc800cb7e4f03b1cbea3dfad13e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c653e50b91d4a288fc40c3eb0978850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:11:38,563 - INFO - Save model to ./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 249.9783, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.096, 'train_loss': 572286407016448.0, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "# Add negative examples (random pairs)\n",
    "\n",
    "# Combine text fields\n",
    "def combine_text(row):\n",
    "    return ' '.join([\n",
    "        str(row.get('title', '')),\n",
    "        str(row.get('description', '')),\n",
    "        str(row.get('long_description', ''))\n",
    "    ])\n",
    "buyer_text=buyers_df.apply(combine_text, axis=1).sample(69)\n",
    "seller_text=sellers_df.apply(combine_text, axis=1).sample(50)\n",
    "# Generate negative pairs\n",
    "negative_pairs = []\n",
    "for buyer, seller in zip(buyer_text, seller_text):\n",
    "    negative_pairs.append({\n",
    "        'buyer_text': buyer,\n",
    "        'seller_text': seller,\n",
    "        'similarity_score': np.random.uniform(0.1, 0.3)\n",
    "    })\n",
    "\n",
    "negative_pairs_df = pd.DataFrame(negative_pairs)\n",
    "\n",
    "# Combine known matches and negative pairs\n",
    "# negative_pairs['similarity_score'] = np.random.uniform(0.1, 0.3)\n",
    "# Generate synthetic training pairs\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load Data\n",
    "# Read the Excel file into a pandas dataframe\n",
    "known_matches = pd.read_excel(\"./data/nlp_business_all_matches_15_13-11-JN 2 -revised.xlsx\")\n",
    "\n",
    "# remove row with missing values in buyer_title or seller_title col \n",
    "known_matches = known_matches.dropna(subset=['buyer_title', 'seller_title'])\n",
    "# fill missing values or 'not in db' in similarity_score with 0.85 to 0.99 uniformly \n",
    "known_matches['similarity_score'] = known_matches['similarity_score'].apply(\n",
    "    lambda x: np.random.uniform(0.85, 0.99) if pd.isnull(x) or x == 'not in db' else x\n",
    ")\n",
    "# known_matches = pd.read_csv(\"./matches/nlp_business_all_matches_21_13-50.csv\")\n",
    "# known_matches = known_matches[['buyer_title', 'buyer_description', 'seller_title', 'seller_description', 'similarity_score']]\n",
    "known_matches['buyer_text'] = known_matches['buyer_title'] + ' ' + known_matches['buyer_description'] + ' ' + known_matches['buyer_long_description']\n",
    "known_matches['seller_text'] = known_matches['seller_title'] + ' ' + known_matches['seller_description'] + ' ' + known_matches['seller_long_description']\n",
    "\n",
    "combined_pairs = pd.concat([known_matches[['buyer_text', 'seller_text', 'similarity_score']], negative_pairs_df])\n",
    "\n",
    "# 2. Build Examples\n",
    "train_examples = []\n",
    "for idx, row in combined_pairs.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row['buyer_text'], row['seller_text']], label=row['similarity_score']))\n",
    "\n",
    "# 3. Create Dataloader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "# fine_tuned_all-MiniLM-L12-v2\n",
    "# 4. Load Pretrained Model\n",
    "# model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# 5. Define Loss (CosineSimilarityLoss if label in [0..1])\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# 6. Train\n",
    "model_save_path = \"./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2\"\n",
    "# model_save_path = \"./fine_tuned_models/fine_tuned_T-Systems-onsite/cross-en-de-roberta-sentence-transformer\"\n",
    "# Split data into training and validation sets\n",
    "train_size = int(0.8 * len(train_examples))\n",
    "train_examples, val_examples = train_examples[:train_size], train_examples[train_size:]\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name='val-eval')\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    # evaluator=evaluator,\n",
    "    epochs=2,\n",
    "    # evaluation_steps=100,\n",
    "    optimizer_params={'lr': 2e-5},\n",
    "    # warmup_steps=100,\n",
    "    output_path=model_save_path\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Use the fine-tuned model\n",
    "fine_tuned_model = SentenceTransformer(model_save_path)\n",
    "\n",
    "# Example inference\n",
    "text1 = \"Elektroinstallationsfirma oder ein IngenieurbÃ¼ro fÃ¼r GebÃ¤udetechnik gesucht Matthias ist SachverstÃ¤ndiger fÃ¼r GebÃ¤udetechnik & Brandschutz und sucht in Baden WÃ¼rttemberg oder Bayern eine Elektroinstallationsfirma oder ein IngenieurbÃ¼ro fÃ¼r GebÃ¤udetechnik.\"\n",
    "text2 = \"Innovativer Elektrobetrieb im Albtal sucht Nachfolger #Pforzheim #Karlsruhe #Service #Wartung #Elektroinstallationen #Beleuchtungstechnik #gleitender Ãœbergang\"\n",
    "embed1 = fine_tuned_model.encode(text1)\n",
    "embed2 = fine_tuned_model.encode(text2)\n",
    "\n",
    "cos_sim = np.dot(embed1, embed2) / (np.linalg.norm(embed1)*np.linalg.norm(embed2))\n",
    "print(\"Cosine Similarity:\", cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import json\n",
    "\n",
    "# Load the small spaCy model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text (remove domain-specific terms, URLs, emails, etc.)\n",
    "    text = re.sub(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Token processing: lemmatization and POS filtering\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[:500]\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields for both buyers and sellers\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# Load fine-tuned bi-encoder and cross-encoder models\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('deepset/gbert-large-sts')\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# Build BM25 index for sellers\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# Set similarity and cross-encoder thresholds\n",
    "similarity_threshold = 0.80  # Adjusted similarity threshold for better precision\n",
    "cross_encoder_threshold = 0.9  # Adjusted cross-encoder threshold\n",
    "\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "matches = []\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "\n",
    "    # BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # Bi-encoder stage (calculate similarity scores)\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ONNX-optimized inference for cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # Location-aware scoring: calculate geodesic distance between buyer and seller locations\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km proximity\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "\n",
    "            if score >= 0.65 and location_match:  # Final matching criteria\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row['location'],\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row['location'],\n",
    "                    'similarity_score': score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# Save results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "matches_df.to_csv('./matches/nlp_business_high_conf_matches.csv', index=False)\n",
    "logging.info(f\"Matches saved: {len(matches_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load the model (replace with correct path if needed)\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Semantic_STS_V2')\n",
    "# model = AutoModel.from_pretrained('aari1995/German_Semantic_STS_V2')\n",
    "\n",
    "# matryoshka_dim = 1024 # How big your embeddings should be, choose from: 64, 128, 256, 512, 768, 1024\n",
    "# model = SentenceTransformer(\"aari1995/German_Semantic_V3\", trust_remote_code=True, truncate_dim=matryoshka_dim)\n",
    "\n",
    "model = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\")\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "sellers_combined_texts = sellers_df['combined_text'].tolist()\n",
    "sellers_embeddings = model.encode(sellers_combined_texts, convert_to_tensor=True)\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sellers_combined_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Define sentences\n",
    "# buyers_combined_texts = buyers_df['combined_text'].tolist()\n",
    "\n",
    "# Compute embeddings for sellers\n",
    "# print(\"ðŸš€ Encoded sellers' text.\", seller_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set similarity threshold\n",
    "similarity_threshold = 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each buyer's combined text\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_title = buyer_row['title']\n",
    "    buyer_description = buyer_row['description']\n",
    "    logging.info(f\"Processing buyer {i+1}/{len(buyers_df)}: {buyer_title}\")\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "    buyer_locations = buyer_row['location']\n",
    "        # Tokenize sentences\n",
    "    # encoded_input = tokenizer(buyer_text, padding=True, truncation=True, return_tensors='pt')\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_tensor=True)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    # with torch.no_grad():\n",
    "    #     model_output = model(**encoded_input)\n",
    "    # buyer_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # Compute embedding for the current buyer's text\n",
    "    \n",
    "    # # Calculate cosine similarities\n",
    "    cosine_scores = util.cos_sim(buyer_embedding, sellers_embeddings)[0]\n",
    "    # Print cosine scores if greater than similarity threshold\n",
    "    for seller_idx, score in enumerate(cosine_scores):\n",
    "        if score >= similarity_threshold:\n",
    "            # Store the results in a DataFrame\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            # if match_locations(seller_locations, buyer_locations):\n",
    "            #     location_match = True\n",
    "            # else:\n",
    "            # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km threshold\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "            \n",
    "                # print(f\"Cosine Score: {score:.4f} (Index: {seller_idx}) sellers_title: {sellers_df.iloc[seller_idx]['title']}, distance: {distance_km} km\")\n",
    "            if location_match:\n",
    "                results.append({\n",
    "                    'buyer_title': buyer_title,\n",
    "                    'buyer_description': buyer_description,\n",
    "                    'seller_title': sellers_df.iloc[seller_idx]['title'],\n",
    "                    'seller_description': sellers_df.iloc[seller_idx]['description'],\n",
    "                    'distance': distance_km,\n",
    "                    'seller_location': seller_locations,\n",
    "                    'buyer_location': buyer_locations,\n",
    "                    'buyer_combined_text': buyer_text,\n",
    "                    'seller_combined_text': sellers_combined_texts[seller_idx],\n",
    "                    'similarity_score': score.item()\n",
    "                })\n",
    "\n",
    "            # Convert results to DataFrame\n",
    "            results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find indices where cosine scores are greater than the similarity threshold\n",
    "    # matching_indices = torch.where(cosine_scores >= similarity_threshold)[0].tolist()\n",
    "    # print(matching_indices)\n",
    "    # results = []\n",
    "    # for seller_idx in matching_indices:\n",
    "    #     seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "    # # # Store results in a DataFrame\n",
    "    #     if cosine_scores >= similarity_threshold:\n",
    "    #         logging.info(f\"Match found: Buyer text: {buyer_text}, Seller text: {seller_text}, Similarity: {score:.4f}\")\n",
    "\n",
    "    #         # results.append({\n",
    "    #         #     'buyer_text': buyer_text,\n",
    "    #         #     'buyer_title': buyer_title,\n",
    "    #         #     'buyer_description': buyer_description,\n",
    "    #         #     'seller_text': seller_text,\n",
    "    #         #     'seller_title': sellers_df.iloc[seller_idx]['title'],\n",
    "    #         #     'seller_description': sellers_df.iloc[seller_idx]['description'],\n",
    "    #         #     'similarity_score': score.item()\n",
    "    #         # })\n",
    "    # # for seller_idx, (seller_text, score) in enumerate(zip(sellers_combined_texts, cosine_scores)):\n",
    "    # #         logging.info(f\"Match found: Buyer text: {buyer_text}, Seller text: {seller_text}, Similarity: {score:.4f}\")\n",
    "            \n",
    "\n",
    "    # results_df = pd.DataFrame(results)\n",
    "# # Print results\n",
    "# for sentence, score in zip(sentences_to_compare, cosine_scores):\n",
    "#     print(f\"Sentence: {sentence}\")\n",
    "#     print(f\"Similarity Score: {score.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./matches/resultsdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# ðŸ”‘ 1. Enhanced Preprocessing\n",
    "# --------------------------\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# nlp = spacy.load(\"de_core_news_lg\", disable=[\"ner\"])\n",
    "# nlp = spacy.load(\n",
    "#     \"de_core_news_sm\",\n",
    "#     disable=[\"ner\"],\n",
    "#     exclude=[\"vectors\"]  # Disable vector subsystem\n",
    "# )\n",
    "# ðŸ”§ Switch to small model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = re.sub(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "    locations = set()\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split location string by \" > \", handling the hierarchical structure\n",
    "        parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.split(\">\")\n",
    "            locations.add(part[-1].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return list(locations)\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "# #Location processing\n",
    "# logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# # Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 2. Hybrid Retrieval Setup\n",
    "# --------------------------\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Precompute BM25 index\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 3. Enhanced Model Loading\n",
    "# --------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ðŸ”‘ Load fine-tuned bi-encoder (pretrain on business texts)\n",
    "bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2')\n",
    "\n",
    "# ðŸ”‘ Optimized cross-encoder with ONNX\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 4. Optimized Matching Loop\n",
    "# --------------------------\n",
    "# ðŸ”‘ Dynamic threshold calculation (precompute from validation data)\n",
    "similarity_threshold = 0.58  # Calculated from validation set\n",
    "cross_encoder_threshold = 0.72\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "    # ðŸ”‘ Hybrid retrieval: BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # ðŸ”‘ Bi-encoder stage\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    \n",
    "    # ðŸ”‘ Batch cosine similarity calculation\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # ðŸ”‘ Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # ðŸ”‘ Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ðŸ”‘ ONNX-optimized inference\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # ðŸ”‘ Location-aware scoring\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            \n",
    "            # ðŸ”‘ Combined score calculation\n",
    "            geo_score = calculate_geo_score(buyer_row, seller_row)  # Implement geo-scoring\n",
    "            final_score = 0.7 * score + 0.3 * geo_score\n",
    "            \n",
    "            if final_score >= 0.65:\n",
    "                # ... rest of match processing ...\n",
    "                print(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# --------------------------\n",
    "# ðŸ”‘ 5. Post-processing\n",
    "# --------------------------\n",
    "def calculate_geo_score(buyer, seller):\n",
    "    \"\"\"Calculate normalized geographic compatibility score (0-1)\"\"\"\n",
    "    # Implement sophisticated location matching\n",
    "    return min(1.0, 1 / (1 + geodesic_distance_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import json\n",
    "\n",
    "# Load the small spaCy model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text (remove domain-specific terms, URLs, emails, etc.)\n",
    "    text = re.sub(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Token processing: lemmatization and POS filtering\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[:500]\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields for both buyers and sellers\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# Load fine-tuned bi-encoder and cross-encoder models\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('deepset/gbert-large-sts')\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# Build BM25 index for sellers\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# Set similarity and cross-encoder thresholds\n",
    "similarity_threshold = 0.80  # Adjusted similarity threshold for better precision\n",
    "cross_encoder_threshold = 0.9  # Adjusted cross-encoder threshold\n",
    "\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "matches = []\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "\n",
    "    # BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # Bi-encoder stage (calculate similarity scores)\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ONNX-optimized inference for cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # Location-aware scoring: calculate geodesic distance between buyer and seller locations\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km proximity\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "\n",
    "            if score >= 0.65 and location_match:  # Final matching criteria\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row['location'],\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row['location'],\n",
    "                    'similarity_score': score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# Save results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "matches_df.to_csv('./matches/nlp_business_high_conf_matches.csv', index=False)\n",
    "logging.info(f\"Matches saved: {len(matches_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = buyers_df['combined_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "from geopy.distance import geodesic\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# -------------------------------\n",
    "# Logging & spaCy Model Setup\n",
    "# -------------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Synonyms Setup\n",
    "# -------------------------------\n",
    "SYNONYM_GROUPS = [\n",
    "    # 1. Company / Enterprise / Business Entities\n",
    "    {\"unternehmen\", \"firma\", \"betrieb\", \"geschÃ¤ft\", \"organisation\", \"konzern\", \"gesellschaft\", \"unternehmung\", \"institution\"},\n",
    "    # 2. Acquisition / Succession\n",
    "    {\"Ã¼bernahme\", \"nachfolge\", \"akquisition\", \"Ã¼bertragung\", \"nachfolgeregelung\", \"Ã¼bergabe\"},\n",
    "    # 3. Building Technology & Engineering\n",
    "    {\"gebÃ¤udetechnik\", \"installation\", \"elektroinstallation\", \"ingenieurbÃ¼ro\", \"bau\", \"architektur\", \"technik\", \"techniker\"},\n",
    "    # 4. Dental & Zahntechnik\n",
    "    {\"zahntechnik\", \"zahntechniker\", \"dentallabor\", \"dentallabore\", \"zahnlabor\", \"zahntechnikerin\"},\n",
    "    # 5. Health, Fitness & Physiotherapy\n",
    "    {\"physiopraxis\", \"physio\", \"fitness-studio\", \"fitnessstudio\", \"praxis\", \"gesundheit\", \"physiotherapie\"},\n",
    "    # 6. Machinery / Machine Construction\n",
    "    {\"maschinenbau\", \"maschinenkonstrukteur\", \"maschinenbauunternehmen\", \"maschinenbaufirma\", \"maschinenkonstruktion\"},\n",
    "    # 7. Gastronomy / Hospitality\n",
    "    {\"gastronomie\", \"caf\", \"cafÃ©\", \"gastgewerbe\", \"restaurant\", \"gasthof\", \"pachtÃ¼bernahme\"},\n",
    "    # 8. Consulting & Advisory\n",
    "    {\"beratung\", \"consulting\", \"unternehmensberatung\", \"managementberatung\", \"beratungsgesprÃ¤ch\", \"berater\"},\n",
    "    # 9. Electrotechnology\n",
    "    {\"elektrotechnik\", \"elektroinstallation\", \"elektroinstallationsfirma\", \"elektrotechniker\", \"bauinstallation\", \"elektro\"},\n",
    "    # 10. Stone Processing / Gravure\n",
    "    {\"sandstrahl\", \"gravure\", \"stein\", \"steingravur\", \"steinbearbeitung\", \"steinverarbeitung\"},\n",
    "    # 11. Creative / Design / Handicraft\n",
    "    {\"start-up\", \"design\", \"kunst\", \"handwerk\", \"kreativ\", \"unikate\", \"patent\", \"designschutz\", \"kunsthandwerk\"},\n",
    "    # 12. IT / Software / Digital\n",
    "    {\"it\", \"informationstechnologie\", \"it-dienst\", \"it-service\", \"edv\", \"software\", \"programm\", \"applikation\", \"app\", \"softwarelÃ¶sung\"},\n",
    "    # 13. Production / Manufacturing\n",
    "    {\"produktion\", \"fertigung\", \"herstellung\", \"produktionsverfahren\", \"fertigungsprozess\"},\n",
    "    # 14. Logistics / Distribution\n",
    "    {\"logistik\", \"transport\", \"distribution\", \"versand\", \"lagerhaltung\", \"lieferkette\"},\n",
    "    # 15. Trade / Sales / Commerce\n",
    "    {\"handel\", \"verkauf\", \"einzelhandel\", \"groÃŸhandel\", \"grosshandel\", \"vertrieb\", \"commerce\"},\n",
    "    # 16. Finance\n",
    "    {\"finanzen\", \"geld\", \"kapital\", \"bankwesen\", \"finanzdienstleistung\", \"investment\", \"beteiligung\", \"vermÃ¶gen\", \"finanzierung\"},\n",
    "    # 17. Marketing & Communication\n",
    "    {\"marketing\", \"werbung\", \"promotion\", \"vertriebsfÃ¶rderung\", \"marktforschung\", \"public relations\", \"kommunikation\", \"marketingstrategie\"},\n",
    "    # 18. Human Resources / Personnel\n",
    "    {\"personal\", \"hr\", \"human resources\", \"mitarbeiter\", \"rekrutierung\", \"personaldienstleistung\", \"personalvermittlung\", \"arbeitskrÃ¤fte\"},\n",
    "    # 19. Research, Development & Innovation\n",
    "    {\"entwicklung\", \"forschung\", \"innovation\", \"forschungs- und entwicklungsabteilung\", \"innovationsmanagement\"},\n",
    "    # 20. Legal / Regulatory\n",
    "    {\"recht\", \"jurisprudenz\", \"gesetz\", \"legal\", \"rechtsschutz\", \"anwaltskanzlei\", \"rechtsberatung\", \"jurist\", \"gesetzgebung\"},\n",
    "    # 21. Real Estate & Construction\n",
    "    {\"immobilien\", \"grundstÃ¼cke\", \"liegenschaften\", \"immobilienverwaltung\", \"bauprojekt\", \"immobilienentwicklung\"},\n",
    "    # 22. Energy / Utilities\n",
    "    {\"energie\", \"strom\", \"elektrizitÃ¤t\", \"gas\", \"erneuerbare energie\", \"energieversorgung\", \"energiewirtschaft\"},\n",
    "    # 23. E-Commerce / Online Trade\n",
    "    {\"e-commerce\", \"onlinehandel\", \"internetverkauf\", \"digitaler verkauf\", \"onlineshop\", \"webshop\"},\n",
    "    # 24. Digital Transformation\n",
    "    {\"digitalisierung\", \"digital\", \"digitaler wandel\", \"it-transformation\", \"digital transformierung\", \"digitalisierungslÃ¶sungen\", \"digitale transformation\"},\n",
    "    # 25. Management & Leadership\n",
    "    {\"management\", \"fÃ¼hrung\", \"leitung\", \"geschÃ¤ftsfÃ¼hrung\", \"managementberatung\", \"organisationsentwicklung\", \"betriebsfÃ¼hrung\"}\n",
    "]\n",
    "\n",
    "def expand_with_synonyms(keywords):\n",
    "    \"\"\"Expand the keyword set using the synonym groups.\"\"\"\n",
    "    expanded = set(keywords)\n",
    "    for group in SYNONYM_GROUPS:\n",
    "        if group.intersection(keywords):\n",
    "            expanded.update(group)\n",
    "    return expanded\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Text Preprocessing and Keyword Extraction\n",
    "# -------------------------------\n",
    "def extract_keywords(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract keywords from text using spaCy. We keep nouns, proper nouns, verbs, and adjectives.\n",
    "    Then, we expand the keyword set using our synonym groups.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return set()\n",
    "    doc = nlp_model(text)\n",
    "    keywords = set()\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if lemma not in stop_words and len(lemma) > 2:\n",
    "                keywords.add(lemma)\n",
    "    return expand_with_synonyms(keywords)\n",
    "\n",
    "def combine_text_fields(row, fields):\n",
    "    \"\"\"Combine multiple fields from a row into one string.\"\"\"\n",
    "    return \" \".join([str(row[field]) for field in fields if pd.notnull(row[field])])\n",
    "\n",
    "# -------------------------------\n",
    "# 2.1. Industry Tag Extraction\n",
    "# -------------------------------\n",
    "# Define an industry dictionary with indicative keywords.\n",
    "INDUSTRY_KEYWORDS = {\n",
    "    \"SHK\": {\"shk\", \"heizung\", \"sanitÃ¤r\", \"elektro\", \"installateur\", \"elektrotechnik\", \"heizungsbaumeister\"},\n",
    "    \"Friseur\": {\"friseur\", \"friseursalon\", \"haarschnitt\", \"kosmetik\"},\n",
    "    \"BÃ¤ckerei\": {\"bÃ¤ckerei\", \"brot\", \"backwaren\"},\n",
    "    \"Maler\": {\"maler\", \"malerbetrieb\", \"anstrich\"},\n",
    "    \"Schlosserei\": {\"schlosser\", \"schlosserei\"},\n",
    "    \"Dentallabor\": {\"zahntechnik\", \"dentallabor\", \"zahnlabor\"},\n",
    "    \"Physio\": {\"physiopraxis\", \"physio\", \"fitness\", \"fitnessstudio\", \"physiotherapie\"},\n",
    "    \"Maschinenbau\": {\"maschinenbau\", \"maschinenkonstrukteur\", \"maschinen\"},\n",
    "    \"Gastronomie\": {\"gastronomie\", \"caf\", \"cafÃ©\", \"restaurant\", \"gastgewerbe\"},\n",
    "    \"Elektro\": {\"elektroinstallation\", \"elektrotechnik\", \"elektroinstallationsfirma\"},\n",
    "    \"Bau\": {\"bau\", \"architektur\", \"bauunternehmen\"},\n",
    "    \"Tankstelle\": {\"tankstelle\", \"waschstraÃŸe\", \"kfzbetrieb\"},\n",
    "    \"KFZ\": {\"werkstatt\", \"kfz\", \"automobil\", \"auto\"}\n",
    "}\n",
    "\n",
    "def extract_industry_tags(text):\n",
    "    \"\"\"\n",
    "    Extract industry tags by scanning the text for any of the indicative keywords.\n",
    "    Returns a set of matching industry tags.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tags = set()\n",
    "    for tag, kws in INDUSTRY_KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            if kw in text:\n",
    "                tags.add(tag)\n",
    "                break\n",
    "    return tags\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load and Preprocess the Datasets\n",
    "# -------------------------------\n",
    "logging.info(\"Loading datasets...\")\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "buyer_fields = [\"title\", \"description\", \"long_description\", \"Industrie\", \"Sub-Industrie\"]\n",
    "seller_fields = [\"title\", \"description\", \"long_description\", \"branchen\"]\n",
    "\n",
    "logging.info(\"Processing buyer data...\")\n",
    "buyers_df[\"combined_text\"] = buyers_df.apply(lambda row: combine_text_fields(row, buyer_fields), axis=1)\n",
    "buyers_df[\"keywords\"] = buyers_df[\"combined_text\"].apply(lambda text: extract_keywords(text, nlp))\n",
    "buyers_df[\"industry_tags\"] = buyers_df[\"combined_text\"].apply(lambda text: extract_industry_tags(text))\n",
    "\n",
    "logging.info(\"Processing seller data...\")\n",
    "sellers_df[\"combined_text\"] = sellers_df.apply(lambda row: combine_text_fields(row, seller_fields), axis=1)\n",
    "sellers_df[\"keywords\"] = sellers_df[\"combined_text\"].apply(lambda text: extract_keywords(text, nlp))\n",
    "sellers_df[\"industry_tags\"] = sellers_df[\"combined_text\"].apply(lambda text: extract_industry_tags(text))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Parse Geographic Coordinates\n",
    "# -------------------------------\n",
    "def parse_coordinates(coord_str):\n",
    "    try:\n",
    "        return json.loads(coord_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "buyers_df[\"latitude\"] = buyers_df[\"latitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df[\"longitude\"] = buyers_df[\"longitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df[\"latitude\"] = sellers_df[\"latitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df[\"longitude\"] = sellers_df[\"longitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Similarity and Geographic Functions\n",
    "# -------------------------------\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "def within_distance(buyer_latitudes, buyer_longitudes, seller_latitudes, seller_longitudes, max_distance_km=50):\n",
    "    \"\"\"\n",
    "    Return (True, distance) if any buyer-seller coordinate pair is within max_distance_km.\n",
    "    If no coordinate information is available, assume a geographic match.\n",
    "    \"\"\"\n",
    "    if not buyer_latitudes or not buyer_longitudes or not seller_latitudes or not seller_longitudes:\n",
    "        return True, None\n",
    "    buyer_coord = (np.mean(buyer_latitudes), np.mean(buyer_longitudes))\n",
    "    for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "        if s_lat is None or s_lon is None:\n",
    "            continue\n",
    "        distance = geodesic(buyer_coord, (s_lat, s_lon)).km\n",
    "        if distance <= max_distance_km:\n",
    "            return True, distance\n",
    "    return False, None\n",
    "\n",
    "# -------------------------------\n",
    "# 5.1. Additional Domainâ€Specific Helper Functions\n",
    "# -------------------------------\n",
    "def domain_match_required(buyer_title, seller_industries):\n",
    "    \"\"\"\n",
    "    If the buyer title explicitly mentions a specific domain keyword,\n",
    "    require that the seller's industry tags include a matching domain.\n",
    "    \"\"\"\n",
    "    buyer_title_lower = buyer_title.lower()\n",
    "    seller_ind_lower = {tag.lower() for tag in seller_industries}\n",
    "    # For example, if buyer mentions \"shk\", require seller to have \"shk\"\n",
    "    if \"shk\" in buyer_title_lower and \"shk\" not in seller_ind_lower:\n",
    "        return False\n",
    "    if \"friseur\" in buyer_title_lower and \"friseur\" not in seller_ind_lower:\n",
    "        return False\n",
    "    if \"maler\" in buyer_title_lower and \"maler\" not in seller_ind_lower:\n",
    "        return False\n",
    "    if (\"dentallabor\" in buyer_title_lower or \"zahntechniker\" in buyer_title_lower) and \\\n",
    "       not ((\"dentallabor\" in seller_ind_lower) or (\"zahnlabor\" in seller_ind_lower)):\n",
    "        return False\n",
    "    # Additional rules can be added here\n",
    "    return True\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Improved Rule-Based Matching with Refined Industry Filtering\n",
    "# -------------------------------\n",
    "# Base similarity threshold and bonus constants.\n",
    "keyword_similarity_threshold = 0.3\n",
    "INDUSTRY_BONUS = 0.15  # full bonus if specific match; lower if only generic \"Bau\" overlaps\n",
    "\n",
    "logging.info(\"Starting improved rule-based matching with refined industry filtering...\")\n",
    "matches = []\n",
    "\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_keywords = buyer_row[\"keywords\"]\n",
    "    buyer_industries = buyer_row[\"industry_tags\"]\n",
    "    buyer_title = buyer_row[\"title\"]\n",
    "    buyer_latitudes = buyer_row[\"latitude\"]\n",
    "    buyer_longitudes = buyer_row[\"longitude\"]\n",
    "    \n",
    "    for j, seller_row in sellers_df.iterrows():\n",
    "        seller_keywords = seller_row[\"keywords\"]\n",
    "        seller_industries = seller_row[\"industry_tags\"]\n",
    "        seller_title = seller_row[\"title\"]\n",
    "        \n",
    "        # Require at least one common industry tag.\n",
    "        common_industries = buyer_industries.intersection(seller_industries)\n",
    "        if not common_industries:\n",
    "            continue\n",
    "        \n",
    "        # If the only shared tag is the generic \"Bau\", reduce the bonus.\n",
    "        if common_industries == {\"Bau\"}:\n",
    "            industry_bonus = 0.05  # lower bonus for generic match\n",
    "        else:\n",
    "            industry_bonus = INDUSTRY_BONUS\n",
    "        \n",
    "        # If buyer title explicitly requires a specific domain, enforce it.\n",
    "        # if not domain_match_required(buyer_title, seller_industries):\n",
    "        #     continue\n",
    "        \n",
    "        sim = jaccard_similarity(buyer_keywords, seller_keywords)\n",
    "        sim += industry_bonus\n",
    "        \n",
    "        if sim >= keyword_similarity_threshold:\n",
    "            location_match, distance_km = within_distance(\n",
    "                buyer_latitudes, buyer_longitudes,\n",
    "                seller_row[\"latitude\"], seller_row[\"longitude\"],\n",
    "                max_distance_km=50\n",
    "            )\n",
    "            if location_match:\n",
    "                matches.append({\n",
    "                    \"buyer_id\": buyer_row.get(\"id\", i),\n",
    "                    \"seller_id\": seller_row.get(\"id\", j),\n",
    "                    \"buyer_title\": buyer_title,\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'distance_km': distance_km,\n",
    "                    \"seller_title\": seller_title,\n",
    "                    \"keyword_similarity\": sim,\n",
    "                    \"industry_buyer\": list(buyer_industries),\n",
    "                    \"industry_seller\": list(seller_industries),\n",
    "                    \"distance_km\": distance_km\n",
    "                })\n",
    "                logging.info(f\"Match: Buyer '{buyer_title}' & Seller '{seller_title}' | Sim: {sim:.2f} | Industries: {buyer_industries} vs. {seller_industries}\")\n",
    "\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values(\"keyword_similarity\", ascending=False)\n",
    "output_path = \"./matches/rule_based_matches_with_synonyms_and_industries.csv\"\n",
    "matches_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"Improved rule-based matching completed. Total matches found: {len(matches_df)}\")\n",
    "logging.info(f\"Matches saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elektroinstallationsfirma oder ein IngenieurbÃ¼ro fÃ¼r GebÃ¤udetechnik gesucht Matthias ist SachverstÃ¤ndiger fÃ¼r GebÃ¤udetechnik & Brandschutz und sucht in Baden WÃ¼rttemberg oder Bayern eine Elektroinstallationsfirma oder ein IngenieurbÃ¼ro fÃ¼r GebÃ¤udetechnik. Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=207804\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=212714\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=211934\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210882\\n\\nMatthias Datensatz gibt es 2x!\\nIch bin SachverstÃ¤ndiger fÃ¼r GebÃ¤udetechnik und Vorbeugender Brandschutz, SiGe, Sicherheitsbeauftragter, Gefahrgutbeauftragter, SiFa und Suche ein geeignetes Unternehmen in der Region Baden - WÃ¼rttemberg und Bayern zu Ã¼bernehmen. IngenieurbÃ¼ro oder Elektroinstallationsfirma. Beteiligung oder Ãœbernahme. Bis 1 Mio â‚¬ Ingenieurdienstleistungen Elektroinstallation, Elektrofirma, GebÃ¤udetechnik, Brandschutztechnik',\n",
       " 'Heizung SanitÃ¤rbetrieb im Raum Celle - Hannover gesucht Felix ist Meister im Bereih Heizung SanitÃ¤r und sucht einen betrieb zur Ãœbernahme zwischen Celle und Hannover Ich Felix Heinze habe einen Meistertitel im Bereich Heizung und SanitÃ¤r. Und suche einen Betrieb zur Ãœbernahme. In der Region Celle und Hannover. 5-6 Angestellte. Ca. 100T Handwerk Heizung, SanitÃ¤r, Klima (SHK)',\n",
       " 'Physiotherapeut sucht Praxis in NÃ¼rnberg Daniel ist seit 2014 Physiotherapeut sowie Heilpraktiker fÃ¼r Physiotherapie und sucht eine Physiotherapie-Praxis im Raum NÃ¼rnberg zur Ãœbernahme. - Physiotherapeut seit 2014\\n- Bachelorabschluss in Gesundheitsmanagement und GesundheitsfÃ¶rderung\\n- Heilpraktiker fÃ¼r Physiotherapie\\n- 1 Jahr Fachliche Leitung einer Physiopraxis\\n- 4 Jahre Erfahrung als Freiberufler ( NebentÃ¤tigkeit ) Gesundheitswesen Physiotherapie',\n",
       " 'Jungunternehmer sucht Physio-Praxis o. Fitness-Studio in DÃ¼sseldorf o. Ratingen Sam ist Jungunternehmer und sucht Physiotherapie-Praxen + Fitnesstudios zur Ãœbernahme 02.12.24: Mail, ob Suche noch aktuell ist\\n19.05.24: Anfrage: Er schrieb: Ich schaue generell nach Standorten zur Ãœbernahme indem sich eine Praxis+ Fitnessstudio kombinieren lassen. Deshalb sind kleinere Immobilien <400qm wahrscheinlich wenig interessant aber generell bin ich erstmal fÃ¼r alles offen.  Gesundheitswesen Physiotherapie, Fitnessstudios',\n",
       " 'Schreinermeister sucht Tischlerei zur Ãœbernahme Peter ist Schreinermeister und sucht einen Betrieb zur Ãœbernahme in SÃ¼d-Hessen bis Nord-Baden-WÃ¼rttemberg mit vorwiegender TÃ¤tigkeit und Kundengruppe im individuellen MÃ¶belbau nach MaÃŸ UrsprÃ¼nglich wg. Tischlerei Esslingen angeschrieben. Inserat .\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=208447\\n\\nSchreinerei mit bestehenden Mitarbeiterstamm und guter Auftragslage erwÃ¼nscht.\\nPrivatkunden, gewerbliche Kunden im hochwertigen MÃ¶bel- und Innenausbau. \\nKommt aus Siedelsbrunn (35 km nordÃ¶stlich von Mannheim)\\nSitzt nÃ¶rdlich von Mannheim / Heidelberg Handwerk Schreinerei, MÃ¶belbau',\n",
       " 'GeschÃ¤ftsfÃ¼hrer sucht Transportfirma zum Kauf Andreas ist Inhaber einer Spedition und mÃ¶chte seinen Betrieb erweitern. Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208011\\n\\nGeschÃ¤ftsfÃ¼hrer einer Spedition aus NRW. Sucht passende Spedition zur Ãœbernahme. War ursprÃ¼nglich Interessent einer Spedition in Bodenwerder Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'War ursprÃ¼nglich Kauf-Interessent einer Spedition in Bodenwerder. Schrieb:\\nHallo habe Interesse an ihrer Anzeige brauche mehr Details zur Spedition Unbekannt Unbekannt',\n",
       " 'MBI-Kandidat sucht Logistik-Firma Jan ist Betriebswirt und sucht eine Spedition / Transportunternehmen im Raum Niedersachsen zur Unternehmensnachfolge. War ursprÃ¼nglich Kauf-Interessent einer Spedition in Bodenwerder. Er schrieb:\\nich interessiere mich fÃ¼r eine Unternehmensnachfolge in der Transport-/ Speditionsbranche im Raum Niedersachsen.\\nKurz zu meiner Person: ich bin Anfang 30, Betriebswirt und klassischer MBI-Kandidat.\\nEine vorherige Anstellung im Unternehmen, fÃ¼r eine mÃ¶glichst glatte Ãœbergabe, wÃ¤re fÃ¼r mich denkbar. Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'FÃ¼hrungskraft mÃ¶chte Logistik-Firma Ã¼bernehmen Lasse hat 20 Jahre Erfahrung im StraÃŸen-Transport / Speditionsgewerbe und sucht eine passende Firma zum Kauf in Nord-Deutschland 02.12.24: fÃ¼r Lasse dieses Inserat angecshrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208188\\n\\n20 Jahre StraÃŸen-Transport / Speditionsgewerbe in FÃ¼hrungsverantwortung tÃ¤tig. beschÃ¤ftigt sich schon lÃ¤nger mit Nachfolge/SelbststÃ¤ndigkeit.\\nVerkehrsfachwirt\\nVerkehrsleiter\\nSilo / KÃ¼hler / Plane. War ursprÃ¼nglich Kauf-Interessent einer Spedition in Bodenwerder Transport und Logistik StraÃŸentransport, Spedition',\n",
       " 'Logistik-Berater sucht Firma zur Ãœbernahme Fabian sucht Spedition in: SÃ¼dliches Niedersachsen oder Nord-Hessen War Interessent der Spedition in Bodenwerder / Niedersachsen\\nscheint Berater fÃ¼r Logistikunternehmen zu sein. war Interessent der Spedtition SchÃ¤fer in Bodenwerder, schrieb: \"Guten Abend, ich bin seit mehr als 20 Jahren im Logistikbereich tÃ¤tig, verfÃ¼ge Ã¼ber Kenntnisse in der UnternehmensfÃ¼hrung, Digitalisierung etc. und wÃ¼rde gerne mehr Ã¼ber das Unternehmen und die Rahmenbedingungen erfahren. Viele GrÃ¼ÃŸe Fabian Rogalla Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Logistik-GeschÃ¤ftsfÃ¼hrer sucht Transportunternehmen JÃ¼rgen ist Speditions- / Logistik-GeschÃ¤ftsfÃ¼hrer und sucht eine gesunde Spedition zur Ãœbernahme Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208188\\n\\nGuten Tag,\\n\\nals Speditions/Logistik-GeschÃ¤ftsfÃ¼hrer mit breiter Erfahrung, Fachkundenachweis und Verkehrsleiter mit mehrjÃ¤hriger Erfahrung und akademischem Background besitze ich Ã¼bergreifende Fach- und FÃ¼hrungskompetenz in allen Bereichen der ergebnisorientierten UnternehmensfÃ¼hrung und Entwicklung eines Logistik-/ Serviceunternehmens mit verschiedenen Standorten und eigenem Fuhrpark. Meine PersÃ¶nlichkeit und FÃ¼hrung ist mittelstands-/ergebnisorientiert, pragmatisch, authentisch, empathisch, werteorientiert, hands- on, stark integrationsfÃ¤hig und in hÃ¶chstem MaÃŸe von LoyalitÃ¤t/Vertrauen geprÃ¤gt. Kernaufgabe war immer Aufbau und Steuerung sÃ¤mtlicher Strukturen, Entwicklung und Steuerung mittels KPIÂ´s und die Unternehmensentwicklung.\\n\\nIn meiner mehrjÃ¤hrigen beruflichen TÃ¤tigkeit als GeschÃ¤ftsfÃ¼hrer -nach einer von mir erfolgreich durchgefÃ¼hrten Restrukturierung- eines erfolgreichen, international tÃ¤tigen mittelstÃ¤ndischen Logistikers mit kundenspezifischen Dienstleistungen und breiter WertschÃ¶pfungskette, habe ich unternehmerisches Denken und DurchsetzungsstÃ¤rke bewiesen. Meine Arbeitsweise ist geprÃ¤gt durch eine deutliche marktorientierte Ausrichtung, fachbereichsÃ¼bergreifendes, prozessorientiertes, durchsetzungsstarkes Handeln mit starker NÃ¤he zum operativen GeschÃ¤ft und kontinuierlichen ProduktivitÃ¤tsuntersuchungen, GeschÃ¤ftsprozessoptimierung, Innovations-, QualitÃ¤ts-/Risikoorientierung und eine der strategischen Ausrichtung folgenden Personalentwicklung.\\n\\nEin Start kann nach Vereinbarung erfolgen. Den erforderlichen Umzug in die NÃ¤he des Firmensitzes wÃ¼rde ich kurzfristig durchfÃ¼hren.\\n\\nFÃ¼r ein persÃ¶nliches GesprÃ¤ch stehe ich Ihnen jederzeit sehr gerne zur VerfÃ¼gung. Meinen CV Zeugnisse stelle ich Ihnen selbstverstÃ¤ndlich bei Bedarf zur VerfÃ¼gung. Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Disponent mÃ¶chte sich selbststÃ¤ndig machen Andre mit Erfahrung als Disponent & Verkehrsleiter sucht SchÃ¼ttgut-Transportfirma Selbst bin ich langjÃ¤hrig als Disponent und Verkehrsleiter tÃ¤tig im Raum 26/27/28/49\\nSucht Spedition, die sich mit SchÃ¼ttguttransporte wie Baustoffe oder Agrarprodukte mit z.B. Kipper oder Schubboden oder mit Schwertransporte mit z.B. Tieflader beschÃ¤ftigt. Oder auch Agrar- und Baustoffhandel, Sand- und Kiesgewinnung- und handel, Tiefbau, Abbruch, Landwirtschaftliche Dienstleistungen Transport und Logistik SchÃ¼ttguttransporte, Schwertransporte',\n",
       " 'GeschÃ¤ftsfÃ¼hrer sucht Dienstleistungsunternehmen Christian ist GeschÃ¤ftsfÃ¼hrer im Logistik-Bereich und sucht Spedition zum Kauf in Berlin, Leipzig, Dresden oder Brandenburg. Gesucht wird ein etabliertes und profitables Dienstleistungs- oder Handwerksunternehmen, welches mindestens seit 10 Jahren erfolgreich existiert, optimaler Weise in der Rechtsform einer GmbH. Hat Erfahrung im Bereich Logistik, Spedition.\\nSucht 1-5 Mio Unternehmenswert\\n\\nAusschlusskriterien: Firmen die Qualifikationen benÃ¶tigen, die nicht durch das eingesetzte Personal (bspw. Meister) abgedeckt werden kÃ¶nnen. Sucht eher Investment und mÃ¶chte dann betriebswirtschftlich beraten.\\n\\nDer KÃ¤ufer ist ein erfahrener Manager (Ã¼ber 12 Jahre Erfahrung als GeschÃ¤ftsfÃ¼hrer, 57 Jahre alt) in der Dienstleistungsbranche (Logistik) mit akademischer Qualifikation (Dipl.-Kfm.). Idee ist, meine Expertise im kaufmÃ¤nnischen Bereich und im Bereich UnternehmensfÃ¼hrung einzubringen, die operative FÃ¼hrung sollte aber durch die zweite Ebene abgedeckt sein. Transport und Logistik Logistikdienstleistungen, Spedition',\n",
       " 'Logistiker sucht Spedition oder Kontrakt-Logistik Michael ist FÃ¼hrungskraft und mÃ¶chte Unternehmen im Logistik-Bereich Ã¼bernehmen Als erfahrener Logistiker mit langjÃ¤hriger Erfahrung unter anderem im C-Level Bereich bin ich auf der Suche nach einem Unternehmen im Logistikumfeld. Dies kann eine Spedition sein, aber auch im Bereich Kontraktlogistik oder Multi-Customer-Warehouses. Transport und Logistik Spedition, Kontraktlogistik, Lagerhaltung',\n",
       " 'GeschÃ¤ftsfÃ¼hrer sucht Spedition zur Ãœbernahme Eduard ist Inhaber einer Spedition und mÃ¶chte ein zweites Unternehmen Ã¼bernehmen, um Synergien zu schaffen. War Kauf-Interessent einer Spedition in Nord-Hessen. Etwas Problem-fokussiert, aber freundlicher Typ.\\n\\nSehr geehrte Damen und Herren,\\nich bin durch Ihre Verkaufsanzeige auf die geplante Transaktion aufmerksam geworden.\\n\\nGerne wÃ¼rde ich mehr darÃ¼ber erfahren. UnterstÃ¼tzung erhalte ich von einem externen Unternehmensberater welcher auch auf Finanzierungen spezialisiert ist.\\n\\nMeinerseits bringe ich unternehmerisches Denken und Handeln sowie Kompetenz und Eloquenz in der Kundenbetreuung mit. Zudem verfÃ¼ge ich Ã¼ber ein breites, europaweites Netzwerk hinsichtlich Produzenten jeglicher Erzeugnisse, was sich sicherlich positiv auf die Auftragslage auswirken kÃ¶nnte. Durch diese Vernetzung befinden sind bereits einige Kunden in der Pipeline. Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Investor kauft Speditionen auf GeschÃ¤ftsfÃ¼hrer mehrere Logistik-Unternehmen kauft gÃ¼nstige Transportfirmen. War Kauf-Interessent einer Spedition in Nord-Hessen. Ernsthaftes Interesse war fraglich. Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Inahber von 2 Speditionen sucht weitere Firmen zum Kauf Florian ist Inhaber von 2 Speditionen und sucht GeschÃ¤ftsfelderweiterung im Transportsektor War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\n \"mit groÃŸen Interesse habe ich Ihr Inserat gelesen, da ich immer wieder nach ErweiterungsmÃ¶glichkeiten meiner bisherigen TÃ¤tigkeiten ausschau halte.\", Inhaber und GF zweier Speditionen (https://tide-spedition.de/\\nhttps://ja-spedition.de/\\nInhaber von BBQ Imbiss, und einer Consulting Firma. (?) Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Umzugsunternehmer sucht MÃ¶belspedition Volkan ist Inhaber eines Umzugsunternehmens und sucht MÃ¶belspedition zum Kauf War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nMeine Frau und meine Schwieger Eltern haben ein Umzugsunternehmen MÃ¶beltransport / MÃ¶belspedition und Lagerung in erster Linie wollen wir mit meiner Frau zusammen das Unternehmen ein bisschen vergrÃ¶ÃŸern und eventuell eine neue Sparte ins unternehmen reinholen meine Vorstellung wÃ¤re natÃ¼rlich das Unternehmen selbst zu leiten und wenn der Preis stimmt natÃ¼rlich auch die Immobilie mit zu Ã¼bernehmen.  Transport und Logistik Umzugsdienste, MÃ¶beltransporte',\n",
       " 'Familienunternehmen sucht GeschÃ¤ftsfelderweiterung im Schwerlastverkehr Dominik ist Inhaber einer Transportfirma und sucht Spedition zum Kauf im Bereich Schwerlastverkehr. War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nSehr geehrte Damen und Herren, \\nda wir ein kleines Familienunternehmen seit 42 Jahre aufgebaut haben, suchen wir nun den Einstieg in den Schwerlastverkehr. Auf Grund unserer Familientradition, kann ich mir gut vorstellen einen anderen Familienbetrieb zu Ã¼bernehmen um diesen im Sinne \"Familie\" weiter fortzufÃ¼hren. Der Einstieg ist wenn, geplant als GeschÃ¤ftsfÃ¼hrung inkl. Betriebsleiter vor Ort. Da wir ein Unternehmen in der NÃ¤he von NÃ¼rnberg haben, wÃ¤re dies ein zweites Standbein fÃ¼r uns. Ein Eigenkapital von 10-20% kann durch Privateinlage gestemmt werden, natÃ¼rlich ist dies AbhÃ¤ngig der Kaufpreisvorstellung. Transport und Logistik Schwertransporte, Spezialtransporte',\n",
       " 'Unternehmer sucht Transportunternehmen Serhat ist Inhaber einer Transportfirma und sucht Spedition zum Kauf War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nWir als die Tekmann Transporte GmbH suchen neue GeschÃ¤ftsfelder und sind bereit erfolgreich im der Transport Branche tÃ¤tig. Und haben Interesse zu Ãœbernahme ihres Betriebes.\\nDaher freuen wir uns fÃ¼r eine RÃ¼ckmeldung\\nMfg Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Familienunternehmen sucht Logistik-Firma / Spedition Daniel ist Leiter eines Familienunternehmens im Logistik-Sektor und sucht Spedition zum Kauf in Hessen oder NRW. War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nwir sind eine mittelstÃ¤ndisches Familienunternehmen mit ca. 165 Mitarbeitern und sitzen in NRW. Da wir uns aktuell vergrÃ¶ÃŸern mÃ¶chten, sind wir auf der Suche nach einem geeigneten Partner der sein Unternehmen verkaufen mÃ¶chte und zu uns passen. Transport und Logistik Spedition und Logistikdienstleistungen',\n",
       " 'Speditionsgruppe sucht passende Firma zum Kauf Kevin ist Leiter einer Speditionsgruppe und sucht ein Transportunternehmen zum Kauf in Hessen, Sachsen oder baden-WÃ¼rttemberg War Kauf-Interessent einer Spedition in Nord-Hessen. Er schieb:\\nwir sind an der Nachfolge Ihres Unternehmens interessiert.\\n\\nZu uns:\\nAus einem Zusammenschluss von Speditionsunternehmern entstand in 2024 die Unternehmensgruppe - Core Logistics GmbH.\\nDie Beteiligten sind einerseits erfahrene Spediteure mit 15-25 Jahren Berufserfahrung und auf der anderen Seite Unternehmer die Erfahrung im Bereich des Unternehmenswachstum haben.\\n\\nGemeinsam fÃ¼hren wir bereits heute eine Speditionsgruppe zwischen Darmstadt/Ulm/Zwickau mit aktuell rund 14 Mio Umsatz und fÃ¼hren die Kompetenzen zu einer Firmengruppe zusammen.\\n\\nGerne tauschen wir uns einmal am Telefon aus um grundsÃ¤tzlich zu prÃ¼fen, ob eine Unternehmensnachfolge mit uns gemeinsam mÃ¶glich ist. Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Logistikgruppe sucht MÃ¶belspedition Peter ist in der GeschÃ¤ftsfÃ¼hrung einer Logistik-Gruppe und sucht eine MÃ¶belspedition zur Ãœbernahme. War Kauf-Interessent einer Spedition in Nord-Hessen. Reber ist eine Logistik-Gruppe, die u.a. im MÃ¶beltransport tÃ¤tig ist Transport und Logistik MÃ¶beltransporte, Umzugsdienste',\n",
       " 'Logistik-Immobilien gesucht Investor sucht Logistik-Immobilie oder Firma im Bereich \"Container\" War Kauf-Interessent einer Spedition in Nord-Hessen.\\nWir suchen aktuell nach Logistik-Immobilien aber auch nach Immobiliennahen operativen Gesellschaften, beispielsweise im Logistik Bereich. Hier sind wir vor allem auch an Containern interessiert.  Immobilien und Logistik Logistikimmobilien, Containerdienste',\n",
       " 'GeschÃ¤ftsfÃ¼hrer sucht Pflegedienst zur Ãœbernahme Markus ist GeschÃ¤ftsfÃ¼hrer eines Pflegedienstes und sucht einen weitere Pflegedienst in Raum MÃ¼nchen + 1,5h Fahrtzeit zur Ãœbernahme Anfrage Ã¼ber Purposition am 29.08.24 von Marcus Kerwin, Voli-Pflege.de, bis 0,5 Mio\\nBayern, am liebsten bis 1,5 h rund um MÃ¼nchen\\nAuschluss: keine auÃŸerklinische Intensivpflege\\nIch habe selbst Inserat bei NC verÃ¶ffentlicht:\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=210760 Gesundheitswesen Ambulanter Pflegedienst, Pflegeleistungen',\n",
       " 'Unternehmen sucht Erweiterung: Tank, BehÃ¤lter, Heizung, OberflÃ¤chenschutz etc. Wir (ein Unternehmen im Bereich Tank-, BehÃ¤lterschutz und BehÃ¤lter-Anlagenbau) suchen eine Firma zum Kauf im Bereich BehÃ¤lter / Tanks usw. Wir sind ein Unternehmen aus SÃ¼ddeutschland und bauen, installieren und warten BehÃ¤lteranlagen (Von Ã–l-Tanks, Heizungstanks bis hin zu IndustriebehÃ¤ltern). Wir suchen eine passende Erweiterung fÃ¼r unser KerngeschÃ¤ft. Gesucht werden Unternehmen, auf die eines der folgenden Stichworte zutrifft: \\n\\nRund um Ã–l:\\nBehÃ¤lterbau\\nBehÃ¤lteranlagen\\nTankschutz, Tank-Wartung, Tank-Reinigung, Tank-Instandhaltung, Tank-Reparatur\\nTank-Installation, Tank-Abbau, Tank-Entsorgung\\nIndustriebehÃ¤lter Herstellung Reinigung\\n\\nRund um Wasser:\\nHeizungstanks (Bau, Reinigung, Entkalkung)\\nBoiler\\n\\nRund um OberflÃ¤chenschutz:\\nOberflÃ¤chenbeschichtung\\nOberflÃ¤chenschutz\\nSandstrahlverfahren\\nBehÃ¤lterbeschichtung und Tankbeschichtung\\nOberflÃ¤chensanierung\\n\\nHabe selbst Inserat verÃ¶ffentlicht:\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=210880 Industriedienstleistungen Tankbau, OberflÃ¤chenschutz, Industrieanstriche',\n",
       " 'Unternehmer sucht Hausverwaltung zum Kauf Thomas ist Immobilienmakler und sucht Hausverwaltung zur Ãœbernahme in NRW 05.09.24. RÃ¼ckfragen gestellt\\n04.09.24: KÃ¤ufer hat Anfrage via UB geschickt. Er schrieb\\nSeit Jahren als Immobilienmakler tÃ¤tig.\\nDie Partnerin arbeitet seit Jahren in der Hausverwaltung.\\nDer nÃ¤chste logische Schritt, ist die GrÃ¼ndung bzw. Ãœbernahme einer eigenen Verwaltung.\\nRegion KÃ¶ln / vorzugsweise KÃ¶lner Westen und Rhein-Erft Kreis.\\nDa wir seit Jahren aber auch im Raum DÃ¼sseldorf/Neuss tÃ¤tig sind, ist dort eine Ãœbernahme auch denkbar. Immobilien Hausverwaltung',\n",
       " 'Immobilienunternehmer sucht Hausverwaltung Reza ist Immobilienunternehmner und sucht eine Hausverwaltung zum Kauf 20.09.24: RÃ¼ckmeldung zu Kaufpreis erhalten\\n20.09.24: Habe RÃ¼ckfrage zu Kaufpreis gestellt\\n18.09.24 KÃ¤ufer hat Anfrage via UB geschickt:\\nHi ich bin Reza und bin als Immobilienunternehmer interessiert eine Hausverwaltung bis 350.000 â‚¬ zu Ã¼bernehmen.\\nIch selbst habe Erfahrung in der Vermietung und Verkauf, sowie der Verwaltung von Objekten aus dem Family Office. mÃ¶chte: Umfassende Einarbeitung druch Inhaber Immobilien Hausverwaltung',\n",
       " 'Elektroingenieur sucht Elektrofirma zur Ãœbernahme Mario ist Elektroingenieur und sucht eine Elektrofirma / Elektroinstallatin / Kommunikationstechnik-Firma in MÃ¼nchen +100 km 20.10.24: (Inserat angeschrieben, dann Absage: VerkÃ¤ufer hat keine Lust auf Agentur)\\nhttps://www.hwk-muenchen.de/74,0,bbdetailoffer.html?id=15706\\n\\n\\n05.09.24: habe selbst Kauf-Gesuch bei NC verÃ¶ffentlicht: (mit Purposition account): S-34f2b6\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=210800\\n\\n04.09.24 Anfrage via PP geschickt: \\nIch suche eine Elektrofirma zum Kauf in MÃ¼nchen oder Umgebung. Ich bin selbst Elektroingenieur (FH) und zur Zeit in Anstellung. Der TÃ¤tigkeitsschwerpunkt der Firma kÃ¶nnte klassische Elektroinstallation und / oder Kommunikationstechnik hauptsÃ¤chlich fÃ¼r WohngebÃ¤ude sein. bis 150 Tsd. Elektroindustrie Elektroinstallation, Kommunikationstechnik',\n",
       " 'Hausverwalter sucht HV zur Ãœbernahme Julius hat bereits eine Hausverwaltung und sucht nun eine weitere Hausverwaltung zum Kauf. Guten Tag, wir betreiben seit rund 12 Jahren nebenberuflich eine kleine Hausverwaltung mit rund 50 Einheiten, ausschliesslich aus dem eigenen Grundbesitz in der Mietverwaltung. Wir suchen eine kleine-mittelgroÃŸe HV bis 1 Mio. zur Ãœbernahme aus dem Raum Ruhrgebiet oder Oberbayern um uns zu vergrÃ¶ÃŸern und die AktivitÃ¤ten zu erweitern. Immobilien Hausverwaltung',\n",
       " 'Junger Hausverwalter sucht in Mittelfranken Kim arbeit in einer WEG-Verwaltung und mÃ¶chte sich durch die Ãœbernahme einer Hausverwaltung selbststÃ¤ndig machen. mein Name ist Kim Kevin Burgahrt, arbeite seit lÃ¤ngerem in einer WEG-Verwaltung und suche jetzt im Raum Mittelfranken eine Verwaltung die ich Ã¼bernehmen/kaufen kann. 50 - 250 Tsd. Euro. Immobilien Hausverwaltung',\n",
       " 'Unternehmer sucht Hausverwaltung Tristan ist im Immobilienberich tÃ¤tg und sucht eine Hausverwaltung bis 2.000 Einheiten. wir sind an der Ãœbernahme einer Hausverwaltung interessiert bis 2.000 Einheiten.\\nHat Erfahrung im Bereich der Miet.- und WEG-Verwaltung, aber hat noch keine eigene Hausverwaltung. Am 16.10.24 telefoniert Immobilien Hausverwaltung',\n",
       " 'Heizungsbaumeister sucht SHK Betrieb Heizungsbaumeister und Inhaber eines SHk Betriebs sucht weiteren Betrieb zur Ãœbernahme. Hatten Kontakt wg. Kriegl SanitÃ¤r zum Kauf. Kauf-Interessent hat bereits einen SHK Betrieb und mÃ¶chte sich erweitern. Sucht Heizung SanitÃ¤r Betriebe rund um NÃ¼rnberg FÃ¼rth. Handwerk Heizung, SanitÃ¤r, Klima (SHK)',\n",
       " 'SHK Betrieb in Sachsen gesucht Markus ist gelernter Anlagenmechaniker fÃ¼r SHK-Technik und sucht SHK Betrieb. 16.10.24: Betrieb angeboten, bisher keine RÃ¼ckmeldung\\n02.09.24: Anfrage:\\nSucht SHK Betrieb, Unter 5-10 Mitarbeiter. Raum Sachsen ( Chemnitz/ Zwickau )\\n18jahre Berufserfahrung im Bau und Kundendienst\\nGelernter Anlagenmechaniker fÃ¼r SHK Technik Handwerk Heizung, SanitÃ¤r, Klima (SHK)',\n",
       " 'Heizungsbaumeister sucht SHK Betrieb Felix ist Heizungsbaumeister und mÃ¶chte sich SHK Betrieb zwischen Celle - Hannover Ã¼bernehmen, 21.10.24: Betrieb angeboten ( https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=203445 ), passt aber nicht (Betrieb sitzt auf ElterngrundstÃ¼ck, SOftware veraltet etc.)\\n27.09.24 nettes Telefonat, 3%\\nAnfrage 22.09.24\\nIch Felix Heinze habe einen Meistertitel im Bereich Heizung und SanitÃ¤r. Und suche einen Betrieb zur Ãœbernahme. In der Region Celle und Hannover. 5-6 Angestellte. Ca. 100T Handwerk Heizung, SanitÃ¤r, Klima (SHK)',\n",
       " 'Angehender Heizungsbaumeister sucht SHK Betrieb Angehender Heizungsbaumeister sucht kleinen SHK Betrieb zur Ãœbernahme. Anfrage Ã¼ber UB am 23.09.24:\\nsuche einen Shk SanitÃ¤r-Heizungs-Klimatechnik in Niedersachsen.  Ãœbernahmepreis wÃ¤re mir erstmal nicht so wichtig, ob 3 oder 5 Mitarbeiter. Ich selber besuche gerade die Meisterschule die ich voraussichtlich im Mai beenden werde, In NRW komme aber ursprÃ¼nglich aus Niedersachsen. Ich suche nÃ¤mlich auch einen Inhaber der interesse hÃ¤tte den Meisterkurs bezahlen wÃ¼rde und demnach sofort in die Einarbeitung fÃ¼r die Ãœbernahme seiner Firma anfangen wÃ¼rde .  Handwerk Heizung, SanitÃ¤r, Klima (SHK)',\n",
       " 'Firma gesucht: GebÃ¤udetechnik oder Brandschutz Matthias ist SachverstÃ¤ndiger fÃ¼r GebÃ¤udetechnik und Vorbeugender Brandschutz und mÃ¶chte ein passendes Unternehmen im Bereich GebÃ¤udetechnik oder Brandschutz Ã¼bernehmen. 13.11.24: VerkÃ¤ufer-Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=209981\\n\\nnoch nicht angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210882\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208421\\n\\nInserat angeschrieben (RÃ¼ckmeldung 28.10.24 Christine Schmitt, ist schon in Verhandlung, Update an Kauf-Interessent geschickt)\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210993\\n\\nAnfrage Ã¼ber dejuna am 26.09.24:\\n\"Ich bin SachverstÃ¤ndiger fÃ¼r GebÃ¤udetechnik und Vorbeugender Brandschutz, SiGe, Sicherheitsbeauftragter, Gefahrgutbeauftragter, SiFa und Suche ein geeignetes Unternehmen in der Region Baden - WÃ¼rttemberg und Bayern zu Ã¼bernehmen. IngenieurbÃ¼ro oder Elektroinstallationsfirma (wahrscheinlich passt Brandschutz und gebÃ¤udetechnik am ehesten, weil er keinen Meister hat? nochmal nachfragen). Beteiligung oder Ãœbernahme. Bis 1 Mio â‚¬\"\\nKommt wohl aus Singen / BW? Ingenieurdienstleistungen GebÃ¤udetechnik, Brandschutztechnik',\n",
       " 'Schlossermeister sucht weiteren Schlosserei-Betrieb zur Ãœbernahme. Mehmet ist Schlossermeister und sucht einen weiteren Metallbau-Betrieb zur Ãœbernahme. am 26.09.24 wg. Schlosserei Weber Mannheim per Mail angeschrieben. Ist Schlossermeister, hat bereits eine Schlosserei und sucht weitere Schlosserei zum Kauf Handwerk Metallbau, Schlosserei',\n",
       " 'Schreinermeister sucht Tischlerei Daniel ist Schreinermeister und sucht Tischlerei im Rheingau Ich bin Schreinermeister und schon seit mehreren Jahren selbststÃ¤ndig als Einzelunternehmer.\\nGerne wÃ¼rde ich eine Schreinerei Ã¼bernehmen hier im Rheingau das bedeutet zwischen Wiesbaden und RÃ¼desheim. Handwerk Schreinerei, Tischlerei',\n",
       " 'KFZ-Meister sucht Werkstatt Mohammad ist KFZ-Meister und sucht eine Autowerkstatt zur Nachfolge in Sachsen Telefonat am 23.08.24: sehr nettes Telefonat (freundlciher Typ, sprachlich fÃ¼r nicht nativ speaker absolut ok, klingt gebildet, freundlich. hat sich bereits mit Kaufpreisfinanzierung etc beschÃ¤ftigt)\\nKontakt wg. diesem Inserat gehabt: https://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=205816 Ich bin Kfz-Meister und suche nach einem Betrieb zur Ãœbernahme.\\n\\n2020 habe ich meinen Gesellenabschluss gemacht. AnschlieÃŸend habe ich die Meisterschule besucht und 2021 den Meisterabschluss als Kraftfahrzeugtechniker erfolgreich bestanden.\\nNebenbei habe ich noch den geprÃ¼ften Fachmann fÃ¼r kaufmÃ¤nnische BetriebsfÃ¼hrung absolviert. Hierbei konnte ich meine Fertigkeiten und Kenntnisse optimieren und erweitern.\\n\\nIm Moment befinde ich mich im Studium zum \"Dipl.-Ing. (FH)\\nKraftfahrzeugelektronik, welches ich voraussichtlich 2025 abschlieÃŸen werde.\\n\\nIch suche eine Kfz-Werkstatt mit HebebÃ¼hnen zum Kauf.  Fahrzeugdienstleistungen Kfz-ReparaturwerkstÃ¤tten',\n",
       " 'Historisches Weingut gesucht Konrad ist Unternehmer und sucht ein historisches Weingut in der Region Pfalz, Baden, Breisgau. 25.10.24: Anfrage via dejuna:\\nBin Quereinsteiger und Suche erstklassiges bestens funktionierendes Weingut ab 15 ha in Pfalz, Baden , Breisgau.\\nSchÃ¶n wÃ¤re ein historisches Weingut.\\nGutes Personal und eigene Weinherstellung. Der Zielbetrieb sollte alle Mitarbeiter mitbringen.\\nIch selbst werde im kaufm. Bereich mitarbeiten, meine Frau im Bereich Vinothek und Kundenbetreuung. Frage zu Kaufpreis unbeantwortet Landwirtschaft und Weinbau Weinbau, Weingut',\n",
       " 'GeschÃ¤ftsfÃ¼hrer sucht Logistik-Unternehmen GeschÃ¤ftsfÃ¼hrer sucht Transport / Logistik-Unternehmen im Bereich Industrie, Pharma, Spezialtransporte, Consumer Goods 09.08.24 hat KÃ¤ufer Anfrage via UB gesendet:\\n- Vorzugsweise Transport und Logistik im Bereich Industrie, Pharma, Spezialtransporte, Consumer Goods\\n- Den Kaufpreis lasse ich offen, ist er bei einem Kleinstunternehmen bis 1,5-2 MIO kann ich diesen finanzieren, mir stehen selbst als Einzelinvestor sofort 350k zur VerfÃ¼gung, bei grÃ¶ÃŸeren Finanzierungen mit Partner (habe ich an der Hand falls das Unternehmen passen sollte)\\n- Standort vorzugsweise der Norden, ich selbst lebe mit meiner Familie in Hamburg â€“ schlieÃŸe aber je nach mÃ¶glicher Passgenauigkeit eines Unternehmens natÃ¼rlich andere Standorte nicht aus\\n\\nMoin aus Hamburg,\\n\\nich bin 47 Jahre alt und seit mehr als 25 Jahren in der Transport und Logistikbranche tÃ¤tig. Seit 2011 in GeschÃ¤ftsfÃ¼hrungspositionen. Erfahrung im Bereich Landfracht, Fuhrpark, Luft- und Seefracht sowie Kontraktlogistik.\\nBedingt Gesellschaftlicher Entscheidungen habe ich beschlossen meine Zukunft beruflich neu auszurichten. Nachdem ich immer schon Unternehmer werden wollte, bin ich seit geraumer Zeit auf der Suche nach einem Unternehmen, wo ein Nachffolger gesucht wird oder was direkt Ã¼bernommen werden kann.\\nBevorzugut im Norden v. Deutschland, da ich hier auch meinen Lebensmittelpunkt habe. Grenznah an Ã–sterreich oder die Schweiz wÃ¤re auch in Ordnung. Transport und Logistik Spezialtransporte, Industrielogistik, Pharmalogistik',\n",
       " 'Maschinenkonstrukteur sucht Maschinenbau-Betrieb suche fÃ¼r Uwe (er ist Maschinenbaukonstrukteur) einen Maschinenbau-Betrieb zur Ãœbernahme, der eigene Produkte oder Maschinen herstellt (also keinen reinen Fertigungsbetrieb). 13.11.24: Inserat angeschrieben (mit Makler)\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210167\\n\\n07.11.24: beide Inserate angeschrieben\\n07.11.24: RÃ¼ckmeldung: findet er beides spannend\\n07.11.24: Mail wg. Hydraulikunternehmen + Medizintechnik-Unternehmen geschrieben: https://www.ihk.de/sbh/boersen/exiboerse/unternehmen/vs-ex-a-25-24-6292208\\nhttps://www.ihk.de/sbh/boersen/exiboerse/unternehmen/vs-ex-a-24-21-medizintechnikunternehmen-zum-verkauf-5315142\\n\\n09.09.24: Telefonat mit Kauf-Interessent. Notizen:\\nIst Maschinenkonstrukteur (hat mal Industriemechaniker gelernt, Fachrichtung Maschinensystemtechnik,\\nsucht Maschinenbauer mit eignem Produkt/Maschinen.\\nSondermaschinenbau, Vorrichtung, Maschinenelementen (Pumpe, Getriebe/ Kupplungen), Hochdruckpunpte, SteuergerÃ¤te, mechanische Komponenten, Pharma Maschinenbau \\nreiner Fertigungsbetrieb eher nicht, lieber mit eigenem Produkt | eigener Maschinen-Entwicklung/Bau\\nZeitraum: nÃ¤chstes halbes Jahr\\nStandorte: Sucht in: SchwÃ¤bisch Hall, Heidelberg, Bodenseeregion, Friedrichshafen, HunsrÃ¼ck, Heilbronn\\neher kleinere Unternehmen interessant zB. bis 1 Mio\\n\\n\\n13.08.24: per Inserat angeschrieben\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=199996 Herstellung und Engineering Maschinenbau, Anlagenbau',\n",
       " 'Finanzberaterin sucht Versicherungsagentur zur Ãœbernahme Finanzberaterin sucht Versicherungsagentur zur Ãœbernahme 20.09.24: RÃ¼ckfragen gestellt\\nAnfrage Ã¼ber UB am 17.09.24\\nEine motivierte und frisch angehende Finanzberaterin und suche Deutschlandweit Versicherungsmarkler die eine Nachfolge suche.\\nWohnhaft bin ich NÃ¤he Bodensee aber durch die Digitalisierung unserer Beratung im Dach- Raum suchend. Finanzdienstleistungen Versicherungsmakler',\n",
       " 'Logistik-Firma sucht Spedition zur Ãœbernahme GeschÃ¤ftsfÃ¼hrer einer Spedition sucht weitere Spedition zur GeschÃ¤ftserweiterung Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208011\\n\\n30.10.24: Habe RÃ¼ckfragen gestellt\\n30.10.24: Anfrage via UB:\\nWir sind ein belgisches Transportunternehmen mit insgesamt 100 Fahrzeugen, inklusive Subunternehmern, davon sind 30 in Deutschland zugelassen.\\nUnser Unternehmen konzentriert sich hauptsÃ¤chlich auf Komplett- und Teilladungen mit Tautliner.\\nWir sind auch auf Luftfracht mit Rollerbet- und KofferanhÃ¤ngern spezialisiert.\\nDies betrifft Transporte von und nach Belgien â€“ Deutschland â€“ Ã–sterreich und der Schweiz sowie den Benelux-LÃ¤ndern.\\nWir mieten derzeit eine Unterkunft (Parkplatz â€“ BÃ¼ro ) in DÃ¼ren fÃ¼r unsere deutsche Tochtergesellschaft.\\n \\nWas wir suchen, kann eine komplette Spedition umfassen mit Fuhrpark oder einfach nur Immobilien  (BÃ¼ro + Parkplatz)\\n \\nBei den von uns transportierten Waren handelt es sich in der Regel um Paletten Ware mit oder ohne ADR.\\n \\nWir suchen ein geeignetes Unternehmen, vorzugsweise auf der Achse AACHEN â€“ ESCHWEILER â€“ DÃœREN â€“ KERPEN â€“ mit einem Jahresumsatz zwischen 5 â€“ 10 Millionen Euro.\\n(Laut Websiete machen sie bisher Gefahrguttransport, Nahrungsmittel-Transport, Baugewerbe) Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Hausverwaltungsgruppe sucht weitere Hausverwaltung Junge Hausverwaltung sucht weitere Verwaltung zur Ãœbernahme. 30.10.24: Feedback vom KÃ¤ufer: \"Lassen Sie uns Anfang kommenden Jahres sprechen, wie eine Zusammenarbeit aussehen kÃ¶nnte.\"\\n30.10.24: Follow up per Mail\\n02.10.24: Telefonat, arbeitet schon Hausveraltungsmaklern emorion, tbk, dr adams, der hauslehrer \\n(DUB, nexxt Change brauche ich nicht screenen, die diese Makler dort verÃ¶ffentlichen)\\nist Interessiert an Suchmandat und bespricht sich intern (deal 120/h + 3)\\n30.09.24: hat Interesse an Suchmandat, mÃ¶chte telefonieren\\n25.09.24 Suchmandat angeboten\\n\\nBiz Inserat:\\nWir sind eine junge, innovative Hausverwaltung, welche historisch eigene BestÃ¤nde verwaltet und sich nun auch fÃ¼r Dritte Ã¶ffnet.\\nNeben der Ãœbernahme von Mandaten in der eigenen Hausverwaltung wollen wir auch aktiv durch strategische ZukÃ¤ufe wachsen. Wir schauen deutschlandweit nach spannenden Hausverwaltungen mit Fokus Wohnen jeder GrÃ¶ÃŸe und unterstÃ¼tzen auch bei Nachfolgesituationen.\\nWir suchen insbs. in:\\n\\nRhein-Main: Frankfurt, Mainz, Offenbach, Darmstadt und Region\\nRhein-Ruhr: Gladbach, Wuppertal, DÃ¼sseldorf und Region\\nOberbayern: MÃ¼nchen, Ingolstadt und Region\\nBerlin\\nLeipzig Immobilien Hausverwaltung',\n",
       " 'Jung-Unternehmer sucht Hausverwaltung zum Kauf Jung-Unternehmer sucht Hausverwaltung zum Kauf in Oberbayern 06.11.2024: RÃ¼ckfragen gestellt, direkt Antwort erhalten\\n05.11.2024: Anfrage\\nWir (zwei Jung-Unternehmer) suchen eine Hausverwaltung zum kaufen aus dem Raum Pfaffenhofen a. D. ILM, Ingolstadt und MÃ¼nchen. Bis 200.000 â‚¬ Immobilien Hausverwaltung',\n",
       " 'Ambulanter Pflegedienst in SÃ¼d-Hessen gesucht Pflegedienstleitung sucht ambulanten Pflegedienst zur Ãœbernahme 06.11.2024: RÃ¼ckfragen gestellt, Antworten (s.u.) bekommen\\n06.11.2024: Anfrage\\nIch suche fÃ¼r mich eine Ambulante pflegedienst in SÃ¼d Hessen (Darmstadt, Frankfurt) zu kaufen.\\nEigene Kapital haben wir nicht, aber wir sind EigentÃ¼mer .\\nWegen Kaufpreis, ich weiÃŸ noch nicht, ich mÃ¶chte nicht etwas groÃŸ zu kaufen. \\nIch bin gelernte krankenschwester, und Pflegedienst leitung  Gesundheitswesen Ambulante Pflege, Pflegedienstleistungen',\n",
       " 'Industriemeister Metall sucht Schlosserei zur Ãœbernahme Serkan ist Industriemeister Metall und sucht Schlosserei in Mannheim zur Ãœbernahme. Bis 150 Tsd. â‚¬ Inserat angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=207374\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=204787\\n\\n07.11.24: RÃ¼ckfrage zu Meistertitel + Kaufpreis an Serkan geschickt\\n07.11.24: NC Inserat angeschrieben: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=204787\\n\\n06.11.24: Habe ihm RÃ¼ckfragen gemailt, Tag spÃ¤ter Antwort erhalten\\n06.11.24: Hat sich auf Schlosserei Inserat Mannheim gemeldet. Er schrieb:\\nIch kÃ¶nnte mir eher vorstellen, dass ich eine komplette Schlosserei Ã¼bernehmen kann. Falls mal etwas reinkommen sollte, dÃ¼rfen Sie mich gerne diesbezÃ¼glich kontaktieren.\\nAusbildung Mechatroniker fÃ¼r KÃ¤ltetechnik\\nWeiterbildung Industriemeister Metall (Schlossermeister Weber sagt: das ist kein Handwerksmeister, er brÃ¤uchte also einen MetallbauMeister als Angestellten?!)\\nIch komme aus Hassloch, deshalb bin ich offen fÃ¼r weitere Orte zwischen Mannheim und Hassloch.\\nSucht bis 150 Tsd. â‚¬ Handwerk Metallbau, Schlosserei',\n",
       " 'Versicherungsmakler sucht Versicherungsagentur Martin ist Versicherungsmakler und sucht eine Versicherungsagentur zur Ãœbernahme 06.11.24 Anfrage via UB:\\nIch suche eine zu Ã¼bernehmende Versicherungsagentur in und um Leipzig.\\nDie erforderlichen Kenntnisse und Erlaubnisse besitze ich. Finanzdienstleistungen Versicherungsmakler',\n",
       " 'Physiotherapeutin sucht Physio-Praxis Corinna ist Physiotherapeutin und sucht eine Physio-Praxis mit Mitarbeiten zur Ãœbernahme. 10.11.24: Anfrage via UB:\\nSuche Physiotherapie-Praxis. Gern auch mit angestellten Therapeuten.. Ich suche im Raum Chemnitz und Erzgebirge eine MÃ¶glichkeit meinen Wirkungskreis zu vergrÃ¶ÃŸern. Wer gern sein Pensionsdasein endlich genieÃŸen mÃ¶chte soll sich gern melden! Gesundheitswesen Physiotherapie',\n",
       " 'Jung-Unternehmer sucht Malerbetrieb zum Kauf in Dortmund + 25km Ibrahim hat einen Betrieb fÃ¼r Innenausbau und sucht einen Malerbetrieb zum Kauf Eckpunkte Kauf-Gesuch:\\nSucht Malerbetrieb oder Fliesenleger-Betrieb.\\nDer Betrieb sollte 25km rund um Dortmund sein, mindestens 5 Mitarbeiter umfassen, idealerweise mit einem Malermeister oder Bauleiter und einer BÃ¼rokraft.\\nEin eigener Fuhrpark sowie eine eigene LagerstÃ¤tte mit integriertem BÃ¼ro wÃ¤ren wÃ¼nschenswert.\\nBis 1 Mio â‚¬\\nâ€“â€“\\n20.11.24: VerkÃ¤ufer 2 angeschrieben https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=212252\\n20.11.24: VerkÃ¤ufer 1 angerufen. Betrieb ist schon verkauft\\n20.11.24: Telefonat mit KÃ¤ufer\\n19.11.24: Inserat angeschrieben wg. Malerbetrieb aus Bochum. Provision in Mail erwÃ¤hnt. Nochmal checken, ob er das gelesen hat\\nKÃ¤ufer: https://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=211043 \\nVerkÃ¤ufer 1 kÃ¶nnte sein: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=209176 Handwerk Maler-Betrieb, Fliesenleger-Betrieb',\n",
       " 'Pflegedienste gesucht: Unternehmen mÃ¶chte expandieren Herbert ist Inhaber einer Pflegegruppe und sucht einen Pflegedienst zur Ãœbernahme 20.11.24: Anfrage Ã¼ber dejuna\\nMoin,\\nwir sind ein renovierter Pflegedienst seit 1992 mit rund 150 Mitarbeitern in Ostfriesland. Hier suchen wir zum Ausbau und Erweiterung weitere Pflegedienste. Wir sind komplett digitalisiert und suchen Pflegedienste wo die Nachfolge nicht geregelt ist.\\nWir suchen im Raum Ostfriesland â€“ Friesland- Oldenburg. Das Emsland wÃ¼rde ich schon ausschlieÃŸen wollen.  \\nKaufpreis bis 1.000.000,- Gesundheitswesen Ambulante Pflege, Pflegedienstleistungen',\n",
       " 'Ehepaar sucht Pflegedienst Ehepaar sucht Pflegedienst zum Kauf im Raum KÃ¶ln +50km 02.12.24: RÃ¼ckfrage zum Kaufpreis gestellt\\n24.11.24: Anfrage via dejuna:\\nmein Ehemann und ich sind auf der Suche nach einem ambulanten Pflegedienst zum Kauf. Wir kommen aus KÃ¶ln, im Umkreis von ca 50km wÃ¼rden wir Ã¼ber Angebote sehr freuen. Gesundheitswesen Ambulante Pflege, Pflegedienstleistungen',\n",
       " 'Investor sucht Hausverwaltung Gerrit ist Investor im Immobiliensektor und mÃ¶chte eine Hausverwaltung in Norddeutschland und Berlin Ã¼bernehmen 03.12.24: braucht kein Suchmandat, mÃ¶chte aber Angebote\\n02.12.24: Suchmandat angeboten (120 + 3)\\n\\n25.11.24: Anfrage via dejuna:\\nInvestor sucht wietere Hausverwaltung zum Kauf. Wir suchen:\\n- Eine etablierte Hausverwaltung\\n- AnsÃ¤ssig in Norddeutschland PLZ-Bereich 2**** und Metropolregion Berlin\\n- Ab 1.000 verwaltete Wohneinheiten\\n- Vorrangig Mietverwaltung von Wohnungen, Garagen und Gewerbe\\n- Anteilig auch Verwaltung nach dem WEG-Gesetz\\n- ZuverlÃ¤ssiger Mitarbeiterstamm\\n- Kaufpreis im 6- bis niedrigen 7-steligem Bereich\\nUnser Angebot:\\n- Wir bieten Ihrem Unternehmen und Mitarbeiter*Innen eine langfristige\\nPerspektive\\n- Wir stelen bei einer Nachfolge-LÃ¶sung einen Volzeit geschÃ¤ftsfÃ¼hrenden\\nGeselschafter (MBI â€’ Management Buy In)\\n- Kapitalstarke Unternehmensgruppe fÃ¼r Zukunfts-Investitionen\\n- FortfÃ¼hrung Ihres Lebenswerkes als eigenstÃ¤ndiges Unternehmen Immobilien Hausverwaltung',\n",
       " 'Immobilienkaufmann sucht Hausverwaltung Jan ist Immobilienkaufmann und sucht eine Hausverwaltung in Norddeutschland Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210028\\n\\n02.12.24: RÃ¼ckfrage zu Kaufpreis gestellt, dann telefoniert, sehr nett, Suchmandat erwÃ¤hnt (120, 3)\\n\\n01.12.24 Anfrage via dejuna:\\nSehr geehrte Damen und Herren,\\nich bin gelernter Immobilienkaufmann (IHK) mit mehrjÃ¤hriger Erfahrung in der Immobilenverwaltung und GeschÃ¤ftsfÃ¼hrer einer Hausverwaltung. Suche in der Region zwischen Bremen, Hannover, Hamburg und LÃ¼neburg. Lieber Mitverwaltung, als nur reine WEG-Verwaltungen. Gerne mehr als 200 Einheiten.\\nVon 50 T bis max. 1 Mio.\\n\\nSofern Sie Unternehmen in den betreffenden Regionen im Angebot haben, freue ich mich Ã¼ber Ihre Nachricht.\\n\\nFÃ¼r RÃ¼ckfragen zum Kaufgesuch kontaktieren Sie mich gern, per E-Mail.\\n\\nMit freundlichen GrÃ¼ÃŸen\\nJan-Erik Rohrbeck Immobilien Hausverwaltung',\n",
       " 'Bestattungsunternehmen gesucht Jung-Unternehmer sucht Bestattungsunternehmen zur Ãœbernahme in Niedersachsen 02.12.24\\nDieses Inserat kÃ¶nnte passen: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=200512\\n\\n02.12.24 Hat via dejuna angerufen\\n- sucht Bestattungsunternehmen\\n- hat lange als Bestatter gearbeitet\\n- bis 150 T\\n- nice to have: Unternehmen soll Abrechnungssoftware z.B. mit adelta Finanz o.Ã¤. zusammenarbeiten (um autom. Rechnungen zu versenden)\\n\\nDeal: 2 fÃ¼r Vermittlung, Beratung zusÃ¤tzlich (fragte nach Hilfe bei Businessplan, usw.)\\nSchnack sehr gerne! Handwerk Bestattungsgewerbe, Bestattungsunternehmen',\n",
       " 'Dentallabor gesucht BjÃ¶rn ist Zahntechniker-Meister und sucht ein Dentallabor zur Ãœbernahme 11.12.24: VerkÃ¤ufer 2 angeschrieben:\\nSteinfurt: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208586\\n06.12.24: VerkÃ¤ufer angeschrieben:\\nBorken: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=211131\\n\\n05.12.24 sein Kauf-Gesuch angeschrieben: https://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=211076\\n\\nSein Inserat: \\nZahntechnikermeister und BdH mit langjÃ¤hriger FÃ¼hrungserfahrung sucht zum nÃ¤chstmÃ¶glichen Zeitpunkt ein Dentallabor ohne Investitionsstau im WestmÃ¼nsterland oder nÃ¶rdlichen Ruhrgebiet zur Ãœbernahme oder Beteiligung.\\nBorken / Coesfeld / Recklinghausen / Botrop / Kreis Wesel\\n\\nErgÃ¤nzung am 07.01.25 per Mail:\\nIch bin 45, seit fast 20 Jahren ZTM mit einem Abschluss als BdH. Seit rund 20 Jahren arbeite ich fast immer als Laborleiter. Bei meinem derzeitigen Ag arbeite ich seit nunmehr 14 Jahren und bin eigentlich sehr zufrieden. Nur die Aussicht auf einen eigenen Betrieb kÃ¶nnte mich zu einem Wechsel bewegen. Derzeit fÃ¼hre ich ein Team mit ca. 25 Technikern.\\nMeine schwerpunkte liegen in der Kundenbetreuung und der Ã„sthetischen Frontzahnversorgung.\\nEinen kurzen Lebenslauf fÃ¼ge ich bei. Gesundheits- und Sozialwesen Gesundheitswesen, Dentallabor',\n",
       " 'Hausverwaltung gesucht Vter & Sohn-Gespann sucht eine Hausverwaltung zum Kauf 08.12.24: Anfrage via dejuna:\\nIch bin auf der Suche nach einer zum Kauf anstehenden Hausverwaltung in der Region Soest/ MÃ¶hnesee/ Arnsberg oder Waldeck/ Kassel/ Bad Wildungen. mein Sohn ist seit einiger Zeit in der Hausverwaltung tÃ¤tig und wird sich mittelfristig selbststÃ¤ndig machen. Dabei werde ich ihn unterstÃ¼tzen und die ersten Jahre auch mitarbeiten. Bei einer mÃ¶glichen Finanzierung sehe ich keine Probleme. Auch wenn der bisherige Inhaber sein Unternehmen erst in den nÃ¤chsten Jahren verkaufen mÃ¶chte und bereit wÃ¤re, seinen potentiellen Nachfolger noch sukzessive einzuarbeiten, wÃ¤re es fÃ¼r uns interessant. Immobilien Hausverwaltung',\n",
       " 'Unternehmen sucht Dachdeckerei, Zimmerei, Schreinerei, Tischlerei und Fensterbau-Firma zum Kauf Handwerksbetrieb sucht: Dachdeckerei, Zimmerei, Schreinerei, Tischlerei und Fensterbau-Firma zum Kauf Kleinanzeigen Inserat (noch nicht kontaktiert)\\nWir sind ein etabliertes Unternehmen mit einer starken PrÃ¤senz im Bereich des Dachdeckerhandwerks. Als Teil unserer Wachstumsstrategie sind wir auf der Suche im Raum MÃ¼nchen, Rosenheim und Umgebung nach Unternehmen, die zur Ãœbernahme oder WeiterfÃ¼hrung geeignet sind. Unser Ziel ist es, unser Fachwissen und unsere Ressourcen zu nutzen, um das Erbe Ihres Unternehmens zu bewahren und weiter auszubauen.\\n\\nGesuchtes Unternehmen:\\nBranche: Dachdeckerhandwerk, Spengler, Zimmerer, Schreiner, Fensterbau\\nEigenschaften: Wir suchen nach Unternehmen mit einer soliden Kundenbasis, erfahrenem Fachpersonal und einem guten Ruf in der Branche.\\nMindestumsatz: netto p.a. 1.000.000 â‚¬\\nWir sind an Unternehmen aller GrÃ¶ÃŸenordnungen interessiert, von Einzelunternehmen bis hin zu Betrieben mit Ã¼ber 20 Mitarb. Handwerk Dachdecker, Sprengler, Schreiner, Gensterbau',\n",
       " 'Unternehmer sucht GebÃ¤udereinigung Ali hat eine etablierte Reinigungsfirma und sucht eine zweite GebÃ¤udereinigung zum Kauf 11.12.24: Anruf von Ali, Zusammenfassung (2 und 120, interessiert):\\nich bin auf der Suche nach einem Unternehmen im Kreis BÃ¼rstadt 68642 umkreis 40 KM.   bei Worms , das in den Bereichen GebÃ¤udereinigung, Gartenbau oder Malerbetrieb tÃ¤tig ist und zur Ãœbernahme steht. Besonders interessiert bin ich an einem Betrieb, der gut etabliert ist und Ã¼ber eine solide wirtschaftliche Grundlage sowie bestehende Kundenbeziehungen verfÃ¼gt.\\n\\nMein Ziel ist es, den Betrieb erfolgreich weiterzufÃ¼hren und auszubauen, wobei mir die QualitÃ¤t der Dienstleistungen und ein gutes VerhÃ¤ltnis zu bestehenden Mitarbeitenden und Kunden besonders am Herzen liegen. Ich bin an offenen GesprÃ¤chen mit EigentÃ¼mern interessiert, die ihre Nachfolge planen oder andere Ãœberlegungen zum Verkauf haben. Handwerk GebÃ¤udereinigung',\n",
       " 'Hausverwalter sucht zweite Hausverwaltung zum Kauf Thomas hat bereits eine Hausverwaltung und sucht eine zweite Immobilienverwaltung zum Kauf im Raum Ansbach, NÃ¼rnberg oder MÃ¼nchen. 17.12.24: Anfrage via dejuna\\nIch bin seit Ã¼ber 30 Jahren im Bereich der Hausverwaltung tÃ¤tig.\\nDerzeit vergrÃ¶ÃŸern wir unseren Bestand durch Zukauf, da unser Sohn, 33 Jahre auch mit langjÃ¤hriger Verwaltungserfahrung, das Unternehmen ausbauen mÃ¶chte.\\nWir haben Interesse an den Regionen/Umgebungen: Ansbach, NÃ¼rnberg, MÃ¼nchen. Radius + 50km\\nKaufpreis bis 1 Mio Immobilien Hausverwaltung',\n",
       " 'Bestattungsunternehmen gesucht Matthias sucht ein Bestattungsunternehmen im Raum Stuttgart zur Ãœbernahme. 19.12.24: RÃ¼ckfragen gestellt, direkt Antwort erhalten\\n18.12.24: Anfrage via dejuna\\nBin auf der Suche zur Ãœbernahme eines Bestattungsunternehmen im Raum Stuttgart .Mfg.M.Wei\\nIch suche fÃ¼r mich selbst , hÃ¤tte aber dann mit einem anderen Bestattungsinstiut eine Kooperation. Eigenkapital ist vorhanden. Handwerk Bestattungsgewerbe, Bestattungsunternehmen',\n",
       " 'Gewerbe-Immobilien-Verwaltung / Hausverwaltung gesucht Christian sucht eine Verwaltung fÃ¼r Gewerbeimmobilien zum Kauf (Hausverwaltung) 28.12.24: Anfrage\\nLiebes Dejuna-Team,\\n\\nwir interessieren uns fÃ¼r den Kauf eines kleineren bis mittelgroÃŸen Property Management-Unternehmens/einer Hausverwaltung im GroÃŸraum MÃ¼nchen.\\nIdealerweise hat das Unternehmen primÃ¤r Kunden und einen Bestand, der aus Gewerbeimmobilien besteht. Denn hier liegt auch die Expertise unseres Teams. Die Ãœbernahme einer Hausverwaltung fÃ¼r Wohnimmobilien wollen wir natÃ¼rlich nicht ausschlieÃŸen.\\nVielen Dank fÃ¼r eine Kontaktaufnahme und ggf. erste Ideen im neuen Jahr.\\n\\nViele GrÃ¼ÃŸe nach Hamburg\\n\\nChristian Simanek Immobilien Hausverwaltung',\n",
       " 'SHK Firma gesucht Claas sucht einen SHK-Betrieb zur Ãœbernahme in Hamburg Ich bin interessiert an der Ãœbernahme oder Mehrheitsbeteiligung an einem SHK-Betrieb in Hamburg. Gerne benachrichtigen Sie mich Ã¼ber Betriebe, welche auf mein Gesuch passen. Handwerk Heizung, SanitÃ¤r, Klima (SHK)',\n",
       " 'Makler sucht Hausverwaltung Immobilienmakler sucht Hausverwaltung 01.01.25 Anfrage via dejuna:\\nInteresse an Ãœbernahme einer Hausverwaltung\\n\\nSehr geehrte Damen und Herren,\\n\\nwir sind auf der Suche nach einer bestehenden Hausverwaltung, die in den nÃ¤chsten Jahren â€“ beispielsweise aus AltersgrÃ¼nden â€“ ihren Betrieb verÃ¤uÃŸern mÃ¶chte. Unser Ziel ist es, unser GeschÃ¤ftsfeld strategisch auszubauen und langfristig weiterzuentwickeln.\\n\\nDerzeit liegt unser Fokus auf der Mietverwaltung, doch ab 2025 werden wir unser Angebot um die WEG-Verwaltung erweitern. Im Laufe dieses Jahres Ã¼bernehmen wir bereits einen Bestand und sind offen dafÃ¼r, weitere Verwaltungsobjekte in unseren Einzugsbereich zu integrieren.\\n\\nDabei kÃ¶nnten wir uns vorstellen, vorhandene Mitarbeiter zu Ã¼bernehmen. Zudem wÃ¼rden wir es begrÃ¼ÃŸen, wenn der bisherige Inhaber den Ãœbergabeprozess aktiv begleiten mÃ¶chte â€“ gerne auch in leitender Funktion â€“ und den Betrieb Schritt fÃ¼r Schritt Ã¼bergibt. Sollte der Inhaber im Rentenbezug in Teilzeit oder nach seinen Vorstellungen weiterhin mitarbeiten wollen, wÃ¼rden wir dies ebenfalls gerne ermÃ¶glichen. Es ist jedoch keine Voraussetzung.\\n\\nDie Erfahrung und Expertise des bisherigen Inhabers mÃ¶chten wir nicht verlieren, da wir wissen, wie wertvoll diese fÃ¼r den erfolgreichen Ãœbergang sein kÃ¶nnen. Aus Erfahrung wissen wir auch, dass viele Unternehmer nach dem Verkauf eines GeschÃ¤ftszweigs nicht sofort ganz aufhÃ¶ren mÃ¶chten. Eine gewisse Ãœbergangszeit oder die MÃ¶glichkeit, kÃ¼rzerzutreten, kÃ¶nnen wir flexibel anbieten.\\n\\nUnser primÃ¤res Suchgebiet umfasst Nordrhein-Westfalen, insbesondere das Ruhrgebiet mit StÃ¤dten wie z.B. Wuppertal, DÃ¼sseldorf, Essen, Bochum und Oberhausen usw.\\n\\nSollten Sie Unternehmen betreuen, die an einer NachfolgelÃ¶sung interessiert sein, freuen wir uns Ã¼ber Ihre Kontaktaufnahme.\\n\\nMit freundlichen GrÃ¼ÃŸen\\nMartin Bittscheidt Immobilien Hausverwaltung',\n",
       " 'Unternehmer sucht Hausverwaltung Unternehmer sucht WEG-Hausverwaltung zum Kauf 05.01.25 Anfrage via dejuna\\nZur Abdeckung einer hybriden Marktposition soll ein neues Unternehmen entstehen. PrimÃ¤r sollen hierbei sowohl die Strukturen eines klassischen MaklerbÃ¼ros, als auch die fundierte TÃ¤tigkeit einer Hausverwaltung kombiniert werden.\\nEntsprechend wird eine WEG-Hausverwaltung zur Ãœbernahme/Nachfolgeabdeckung gesucht. Der TÃ¤tigkeitsradius umfasst den Landkreis Pinneberg/Schleswig-Holstein und Teile des Hamburger Westens. Additional wÃ¼rde dieses Inserat auch fÃ¼r den Landkreis Itzehoe/Schleswig-Holstein passend sein. Immobilien Hausverwaltung',\n",
       " 'Bestatter sucht Bestattungsunternehmen zur Ãœbernahme Bestatter sucht Bestattungsinstitut zur Ãœbernahme 07.01.24 Anfrage via dejuna:\\nIch bin Bestatter und suche ein Bestattungsinstitut zur Ãœbernahme im Raum: Zwischen Bonn, Siegburg, Neuwied, Montabaur, Bad Marienberg, Gummersbach.\\nDas wÃ¤re bei Kennzeichen der Handwerk Bestattungsgewerbe, Bestattungsunternehmen',\n",
       " 'Zahntechniker-Meister sucht Dentallabor Dirk ist Zahntechniker-Meister und sucht ein Dentallabor zur Ãœbernahme 07.01.25 Anfrage auf dejuna Auschreibung (Dentallabor bei MÃ¼nchen)\\nich bin gerade gezwungen mich mit meinem Dentallabor zu vergrÃ¶ÃŸern und suche eine geeignete Immobilie. Ich suche etwas zwischen 100 und 140 mÂ², mit Platz fÃ¼r mindestens drei Personen. An besten im Bereich Giesing +5km. Gesundheits- und Sozialwesen Gesundheitswesen, Dentallabor',\n",
       " 'Zahntechniker-Meisterin sucht Denttallabor zur Ãœbernahme Viktoria ist Zahntechniker-Meisterin und sucht ein Dentallabor rund um Hannover (150km) Ich bin Viktoria Pahl, eine Zahntechnikerin-Meisterin aus Hannover. Ich wÃ¼rde gerne ein Labor Ã¼bernehmen in Niedersachen oder von Hannover 150km Umkreis.\\n\\nLiebe GrÃ¼ÃŸe\\nViktoria Pahl Gesundheits- und Sozialwesen Gesundheitswesen, Dentallabor']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyers_df['combined_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the spaCy model (disable NER for speed)\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "# Pre-compile regex patterns for efficiency\n",
    "URL_PATTERN = re.compile(r'http\\S+|www\\.\\S+')\n",
    "EMAIL_PATTERN = re.compile(r'\\S+@\\S+')\n",
    "LONG_NUMBER_PATTERN = re.compile(r'\\b\\d{10,}\\b')\n",
    "DOMAIN_PATTERN = re.compile(r'(geschÃ¤ft|dienstleistung|industrie)\\w*', re.IGNORECASE)\n",
    "NON_ALPHA_PATTERN = re.compile(r'[^a-zA-ZÃ¤Ã¶Ã¼Ã„Ã–ÃœÃŸ\\s\\'\\-]')\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"Clean and preprocess text using regex and spaCy.\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Remove domain-specific terms, URLs, emails, long numbers, and non-alphabetic characters\n",
    "    text = DOMAIN_PATTERN.sub('', text)\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = EMAIL_PATTERN.sub('', text)\n",
    "    text = LONG_NUMBER_PATTERN.sub('', text)\n",
    "    text = NON_ALPHA_PATTERN.sub('', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Keep tokens that are nouns, proper nouns, verbs, or adjectives (using lemmas)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ'} and token.text.lower() not in stop_words:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if len(lemma) > 2:\n",
    "                tokens.append(lemma)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def tokenize_for_bm25(text, nlp_model):\n",
    "    \"\"\"Tokenize text using spaCy to ensure consistency with preprocessing.\"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    return [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "def get_bounding_box(lat, lon, max_distance_km=50):\n",
    "    \"\"\"\n",
    "    Compute an approximate bounding box for a given coordinate.\n",
    "    Note: 1 degree latitude ~ 111 km.\n",
    "    \"\"\"\n",
    "    delta = max_distance_km / 111  # Approximate conversion to degrees\n",
    "    return (lat - delta, lat + delta, lon - delta, lon + delta)\n",
    "\n",
    "def passes_geographic_filter(buyer_latitudes, buyer_longitudes, seller_latitudes, seller_longitudes, max_distance_km=50):\n",
    "    \"\"\"\n",
    "    Check if any pair of buyer and seller coordinates is within the specified maximum distance.\n",
    "    Uses a bounding box pre-filter before calculating the precise geodesic distance.\n",
    "    Returns a tuple: (True/False, distance in km or None)\n",
    "    \"\"\"\n",
    "    if not buyer_latitudes or not buyer_longitudes or not seller_latitudes or not seller_longitudes:\n",
    "        return False, None\n",
    "    \n",
    "    # Use the average buyer coordinate as a reference point\n",
    "    buyer_lat = np.mean(buyer_latitudes)\n",
    "    buyer_lon = np.mean(buyer_longitudes)\n",
    "    min_lat, max_lat, min_lon, max_lon = get_bounding_box(buyer_lat, buyer_lon, max_distance_km)\n",
    "    \n",
    "    for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "        if s_lat is None or s_lon is None:\n",
    "            continue\n",
    "        # Quick check: if seller coordinate is outside the bounding box, skip\n",
    "        if s_lat < min_lat or s_lat > max_lat or s_lon < min_lon or s_lon > max_lon:\n",
    "            continue\n",
    "        # Calculate precise distance\n",
    "        distance = geodesic((buyer_lat, buyer_lon), (s_lat, s_lon)).km\n",
    "        if distance <= max_distance_km:\n",
    "            return True, distance\n",
    "    return False, None\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Preprocess Datasets\n",
    "# -------------------------------\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    \"\"\"Combine preprocessed text fields into one string.\"\"\"\n",
    "    fields = [\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        # Use 'branchen_preprocessed' if available, otherwise 'preprocessed_branchen'\n",
    "        row.get('branchen_preprocessed', '') or row.get('preprocessed_branchen', '')\n",
    "    ]\n",
    "    return ' '.join(fields)\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# Load Models and Build BM25 Index\n",
    "# -------------------------------\n",
    "logging.info('Loading models...')\n",
    "# Load a bi-encoder for sentence embeddings\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# Load the cross-encoder (ONNX-optimized) and its tokenizer\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "logging.info('Building BM25 index for sellers...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "# Use the spaCy tokenizer for BM25\n",
    "tokenized_seller_texts = [tokenize_for_bm25(text, nlp) for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "logging.info(\"Encoding sellers' text with bi-encoder...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Seller embeddings generated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Matching Parameters\n",
    "# -------------------------------\n",
    "similarity_threshold = 0.3      # Bi-encoder cosine similarity threshold\n",
    "cross_encoder_threshold = 0.35   # Cross-encoder threshold for initial filtering\n",
    "final_similarity_threshold = 0.25  # Final similarity threshold after geographic filtering\n",
    "\n",
    "# -------------------------------\n",
    "# Matching Function for a Single Buyer\n",
    "# -------------------------------\n",
    "def process_buyer(buyer_row, seller_embeddings, bm25_index, sellers_df):\n",
    "    \"\"\"Process one buyer and return valid matches as a list of dictionaries.\"\"\"\n",
    "    matches = []\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "    \n",
    "    # BM25 candidate retrieval using the spaCy tokenized text\n",
    "    buyer_tokens = tokenize_for_bm25(buyer_text, nlp)\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    # Retrieve the top 500 candidates\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-1000:][::-1]\n",
    "    \n",
    "    # Compute the buyer embedding using the bi-encoder\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute cosine similarities between buyer and BM25 candidate seller embeddings\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    candidate_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    if len(candidate_indices) == 0:\n",
    "        return matches  # No candidates pass the bi-encoder threshold\n",
    "    \n",
    "    # Use a percentile-based filter (top 90% among candidates)\n",
    "    candidate_scores = sim_scores[candidate_indices]\n",
    "    percentile_cutoff = np.percentile(candidate_scores, 10)\n",
    "    top_candidate_indices = candidate_indices[candidate_scores >= percentile_cutoff]\n",
    "    if len(top_candidate_indices) == 0:\n",
    "        top_candidate_indices = candidate_indices  # Fallback to all candidates if none pass cutoff\n",
    "    \n",
    "    # Prepare candidate pairs for cross-encoder inference\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_candidate_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # Batch inference using the cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = torch.sigmoid(outputs.logits).squeeze().detach().numpy()\n",
    "    # Handle case where cross_scores is a scalar (0-d array)\n",
    "    if cross_scores.ndim == 0:\n",
    "        cross_scores = np.array([cross_scores])\n",
    "    \n",
    "    # Iterate over candidates, apply geographic filter, and record matches\n",
    "    for idx, cross_score in zip(top_candidate_indices, cross_scores):\n",
    "        seller_idx = bm25_candidates[idx]\n",
    "        seller_row = sellers_df.iloc[seller_idx]\n",
    "        seller_latitudes = seller_row['latitude']\n",
    "        seller_longitudes = seller_row['longitude']\n",
    "        \n",
    "        # Check if the seller and buyer are within the desired geographic proximity\n",
    "        location_match, distance_km = passes_geographic_filter(\n",
    "            buyer_latitudes, buyer_longitudes, seller_latitudes, seller_longitudes, max_distance_km=50)\n",
    "        \n",
    "        # Combine cross-encoder score with location match; adjust as needed\n",
    "        if cross_score >= cross_encoder_threshold and location_match:\n",
    "            final_score = cross_score  # You could also weight location here if desired\n",
    "            if final_score >= final_similarity_threshold:\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'similarity_score': final_score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer '{buyer_row['title']}' with Seller '{seller_row['title']}' | Score: {final_score:.2f} | Distance: {distance_km:.2f} km\")\n",
    "    return matches\n",
    "\n",
    "# -------------------------------\n",
    "# Main Matching Loop\n",
    "# -------------------------------\n",
    "all_matches = []\n",
    "logging.info(\"Starting buyer-seller matching...\")\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_matches = process_buyer(buyer_row, seller_embeddings, bm25_index, sellers_df)\n",
    "    if buyer_matches:\n",
    "        all_matches.extend(buyer_matches)\n",
    "\n",
    "# Save matching results to CSV\n",
    "matches_df = pd.DataFrame(all_matches)\n",
    "# matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "output_path = './matches/nlp_business_high_conf_matches.csv'\n",
    "matches_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"Matching completed. Total matches found: {len(matches_df)}\")\n",
    "logging.info(f\"Matches saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"./onnx_models/cross-encoder-de\",\n",
    "    file_name=\"model.onnx\",\n",
    "    provider=\"CPUExecutionProvider\"  # or \"CPUExecutionProvider\"\n",
    ")\n",
    "\n",
    "# Load optimized version\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\"./onnx_models/cross-encoder-de\")\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install required packages\n",
    "# pip install optimum[onnxruntime] torch onnxruntime\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Use AutoModelForSequenceClassification to load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    ")\n",
    "\n",
    "# 2. Export to ONNX format with explicit architecture\n",
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "from optimum.onnxruntime import ORTOptimizer, ORTQuantizer\n",
    "\n",
    "# Save PyTorch model first\n",
    "model.save_pretrained(\"./tmp_model\")\n",
    "\n",
    "# 3. Convert using optimum-cli (recommended)\n",
    "# Run in terminal:\n",
    "# optimum-cli export onnx --model ./tmp_model --task text-classification ./onnx_models/cross-encoder-de\n",
    "\n",
    "# 4. Load the converted ONNX model\n",
    "# Option 2: Use German-optimized model\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"./onnx_models/cross-encoder-de\",\n",
    "    file_name=\"model.onnx\",\n",
    "    provider=\"CPUExecutionProvider\"  # or \"CPUExecutionProvider\"\n",
    ")\n",
    "# 5. Load tokenizer\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained(\"./tmp_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
