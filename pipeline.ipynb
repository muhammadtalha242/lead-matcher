{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======= 1. requirements matching  > 0.85\n",
    "            1. semantic analysis > 0.9\n",
    "                a. similarity keywords\n",
    "            2. Nace Code match > 1.0\n",
    "        2. Location matching\n",
    "            1. exact location matching\n",
    "            2. Geocoding matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import shelve\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from geopy.distance import geodesic \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORTS\n",
    "dataDIR = './data/'\n",
    "originalSalesNexxtChangeData = f'{dataDIR}branche_nexxt_change_sales_listings_scrape.csv'\n",
    "dejunaPurchases = './data/dejuna_buyer_latest.csv'\n",
    "\n",
    "nacecode_josn =  './data/nace_codes.json' \n",
    "nacecode_array_josn =  './data/nace_codes_array.json' \n",
    "nacecode_array_obj =  './data/nace_codes_object.json' \n",
    "nacecode_array_obj_ext =  './data/nace_codes_object_ext.json' \n",
    "nacecode_array_obj_du =  './data/nace_codes_object_du.json'\n",
    " \n",
    "dataFile =  './data/nexxt_change_sales_listings_geocoded_short_test.csv' \n",
    "# sales_file_nace =  './data/nexxt_change_sales_listings_geocoded.csv' \n",
    "sales_file_brachen =  './data/branche_nexxt_change_sales_listings.csv' \n",
    "sales_file_nace =  './data/dub_listings_geo.csv'\n",
    "buyer_file_nace =  './data/nexxt_change_purchase_listings_geocoded.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/talha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/talha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Load SpaCy's German model (for tokenization, NER, POS tagging)\n",
    "# -------------------------------------------------------------------------\n",
    "try:\n",
    "    nlp = spacy.load('de_core_news_lg')\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download('de_core_news_lg')\n",
    "    nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 2. Preprocessing function (with German NER, POS, etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    - Lowercases the text.\n",
    "    - Removes URLs, emails, & large digit sequences.\n",
    "    - Filters out non-alphabetic chars except German Umlauts/ß.\n",
    "    - Uses SpaCy to keep only NOUN, PROPN, VERB tokens not in stopwords.\n",
    "    - Applies Snowball stemming on remaining tokens.\n",
    "    - Also includes certain named entities (ORG, PRODUCT, GPE).\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, emails, large numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    \n",
    "    # Keep only letters and German characters\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s]', '', text)\n",
    "    \n",
    "    # Compact multiple spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "\n",
    "    # # Initialize German stopwords and Snowball stemmer\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer = SnowballStemmer('german')\n",
    "\n",
    "    # Process text with SpaCy\n",
    "    doc = nlp_model(text)\n",
    "\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Keep nouns, proper nouns, and verbs\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB'} and token.text not in stop_words:\n",
    "            stemmed = stemmer.stem(token.text)\n",
    "            tokens.append(stemmed)\n",
    "\n",
    "    # Extract named entities (ORG, PRODUCT, GPE) and include them\n",
    "    entities = [\n",
    "        ent.text for ent in doc.ents \n",
    "        if ent.label_ in {'ORG', 'PRODUCT', 'GPE'}\n",
    "    ]\n",
    "    # Stem and remove stopwords from entities\n",
    "    entities = [\n",
    "        stemmer.stem(ent.lower()) \n",
    "        for ent in entities \n",
    "        if ent.lower() not in stop_words\n",
    "    ]\n",
    "    tokens.extend(entities)\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def split_compounds_regex(text):\n",
    "    \"\"\"\n",
    "    Basic German compound word splitting using regex rules.\n",
    "    Example: \"Kundenzufriedenheit\" → [\"kunden\", \"zufriedenheit\"]\n",
    "    \"\"\"\n",
    "    # Split at common compound connectors (e.g., -s-, -en-, -n-)\n",
    "    parts = re.split(r'(s\\b|en\\b|n\\b|e\\b)(?=\\w{3,})', text)\n",
    "    return [p for p in parts if p and len(p) > 2]\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# Load SpaCy model CORRECTLY (without disabling components during initial load)\n",
    "nlp = spacy.load(\"de_core_news_lg\")  # Load first\n",
    "nlp.disable_pipes(\"parser\", \"ner\")   # Disable components AFTER loading\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Preprocesses German text for similarity tasks.\n",
    "    - Disables static vectors to avoid RuntimeError.\n",
    "    - Uses regex-based compound splitting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s\\'\\-]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    custom_stopwords = {\"unternehmen\", \"firma\", \"dienstleistung\", \"kunde\"}\n",
    "    stop_words = STOP_WORDS.union(custom_stopwords)\n",
    "    \n",
    "    # Token processing (lemmatization + POS filtering)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    # Regex-based compound splitting (no external dependencies)\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and len(token.text) > 8:\n",
    "            parts = re.findall(r'\\b\\w{4,}(?=\\w{4,})', token.text)  # Split long nouns\n",
    "            compounds.extend([p.lower() for p in parts])\n",
    "    tokens.extend(compounds)\n",
    "    \n",
    "    # Filter short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:20:08,464 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-03 01:20:08,465 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10cd9122cc140d589d52244acba622c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69234bec7ed048bcb90fffb7f2ba46e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7448be677b8f424488dbbfb35eb16e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b5470889c64aeab1ffdff192533d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b602ddddf54c75b0d63d8615f79361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bb0a0cc45e4f7cbee50b693b3f1476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n",
      "Avg Similarity (Matches): 0.50\n",
      "Avg Similarity (Non-Matches): 0.36\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# # Load SpaCy model CORRECTLY (without disabling components during initial load)\n",
    "# nlp = spacy.load(\"de_core_news_lg\")  # Load first\n",
    "# nlp.disable_pipes(\"parser\", \"ner\")   # Disable components AFTER loading\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Preprocesses German text for similarity tasks.\n",
    "    - Disables static vectors to avoid RuntimeError.\n",
    "    - Uses regex-based compound splitting.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s\\'\\-]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    custom_stopwords = {\"unternehmen\", \"firma\", \"dienstleistung\", \"kunde\"}\n",
    "    stop_words = STOP_WORDS.union(custom_stopwords)\n",
    "    \n",
    "    # Token processing (lemmatization + POS filtering)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    # Regex-based compound splitting (no external dependencies)\n",
    "    compounds = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN' and len(token.text) > 8:\n",
    "            parts = re.findall(r'\\b\\w{4,}(?=\\w{4,})', token.text)  # Split long nouns\n",
    "            compounds.extend([p.lower() for p in parts])\n",
    "    tokens.extend(compounds)\n",
    "    \n",
    "    # Filter short tokens\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_similarity(test_cases, nlp_model, embedding_model):\n",
    "    similarities = []\n",
    "    labels = []\n",
    "    \n",
    "    for text1, text2, is_similar in test_cases:\n",
    "        preprocessed1 = preprocess_text(text1, nlp_model)\n",
    "        preprocessed2 = preprocess_text(text2, nlp_model)\n",
    "        \n",
    "        embedding1 = embedding_model.encode(preprocessed1)\n",
    "        embedding2 = embedding_model.encode(preprocessed2)\n",
    "        \n",
    "        similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "        similarities.append(similarity)\n",
    "        labels.append(is_similar)\n",
    "    \n",
    "    similarities = np.array(similarities)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    threshold = 0.7\n",
    "    true_positives = np.sum((similarities > threshold) & (labels == True))\n",
    "    false_positives = np.sum((similarities > threshold) & (labels == False))\n",
    "    false_negatives = np.sum((similarities <= threshold) & (labels == True))\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives + 1e-8)  # Avoid division by zero\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    \n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n",
    "    print(f\"Avg Similarity (Matches): {np.mean(similarities[labels]):.2f}\")\n",
    "    print(f\"Avg Similarity (Non-Matches): {np.mean(similarities[~labels]):.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    (\"Industriemaschinen\", \"Maschinenbau\", True),\n",
    "    (\"Kundenbetreuung\", \"Autoreparatur\", False),\n",
    "    (\"Logistikdienstleister\", \"Transportunternehmen\", True),\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "evaluate_similarity(test_cases, nlp, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _extract_location_parts(location):\n",
    "#     \"\"\"Extract and categorize location parts into states and cities.\"\"\"\n",
    "#     locations = set()\n",
    "#     german_states = {\n",
    "#         'baden-württemberg', 'bayern', 'berlin', 'brandenburg', 'bremen',\n",
    "#         'hamburg', 'hessen', 'mecklenburg-vorpommern', 'niedersachsen',\n",
    "#         'nordrhein-westfalen', 'rheinland-pfalz', 'saarland', 'sachsen',\n",
    "#         'sachsen-anhalt', 'schleswig-holstein', 'thüringen'\n",
    "#     }\n",
    "\n",
    "\n",
    "#     if not location or not isinstance(location, str):\n",
    "#         return locations\n",
    "\n",
    "#     try:\n",
    "#         # Split on common delimiters\n",
    "#         parts = re.split(r'[>/\\n]\\s*', location)\n",
    "#         split_locations = []\n",
    "\n",
    "#         for part in parts:\n",
    "#             part = part.strip().lower()\n",
    "#             if part:\n",
    "#                 # Further split by space if multiple states are concatenated\n",
    "#                 words = part.split()\n",
    "#                 temp = []\n",
    "#                 current = \"\"\n",
    "#                 for word in words:\n",
    "#                     if word.lower() in german_states:\n",
    "#                         if current:\n",
    "#                             temp.append(current.strip())\n",
    "#                         current = word\n",
    "#                     else:\n",
    "#                         current += \" \" + word if current else word\n",
    "#                 if current:\n",
    "#                     temp.append(current.strip())\n",
    "#                 split_locations.extend(temp)\n",
    "\n",
    "#         for loc in split_locations:\n",
    "#             loc = loc.strip().lower()\n",
    "#             if loc:\n",
    "#                 if loc in german_states:\n",
    "#                     locations.add(loc.title())  # Capitalize for better geocoding\n",
    "#                 else:\n",
    "#                     # Clean up common prefixes like \"region\"\n",
    "#                     clean_part = re.sub(r'^region\\s+', '', loc)\n",
    "#                     if clean_part:\n",
    "#                         locations.add(clean_part.title())\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "#     return locations\n",
    "\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "    locations = set()\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split location string by \" > \", handling the hierarchical structure\n",
    "        parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.split(\">\")\n",
    "            locations.add(part[-1].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return list(locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _extract_location_parts('''Sachsen / Leipzig / Leipzig, Stadt''')\n",
    "_extract_location_parts('''Berlin\n",
    "Sachsen > Leipzig\n",
    "Sachsen > Dresden\n",
    "Brandenburg\n",
    "Niedersachsen > Hannover\n",
    "Hessen > Frankfurt am Main\n",
    "                        Hamburg\n",
    "                        Bayern > München\n",
    "                        Bayern > Nürnberg\n",
    "                        Bayern > Augsburg\n",
    "                        A\n",
    "                        B > C''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. Load NACE codes from JSON\n",
    "# -------------------------------------------------------------------------\n",
    "def load_nace_codes(filepath):\n",
    "    \"\"\"\n",
    "    Expects a JSON file where keys = NACE code, values = textual descriptions.\n",
    "    Example:\n",
    "      {\n",
    "        \"01.1\": \"Growing of non-perennial crops\",\n",
    "        \"01.2\": \"Growing of perennial crops\",\n",
    "        ...\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        nace_codes = json.load(file)\n",
    "    return nace_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings with sentence-transformers\n",
    "# -------------------------------------------------------------------------\n",
    "def get_embedding_batch(texts, model, batch_size=64):\n",
    "    \"\"\"\n",
    "    Encode texts in batches to optimize memory usage.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, \n",
    "                              convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embeddings.astype('float32')  # Use float32 to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = pd.read_csv(sellers_filepath) \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"🚀 Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"🚀 Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "#     lambda x: ' '.join(x.split('>'))\n",
    "# )\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"🚀 Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"🚀 Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"🚀 Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another NACE code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Create embeddings using Hugging Face\n",
    "# -------------------------------------------------------------------------\n",
    "def create_hf_embeddings(texts, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "            embeddings.append(cls_embedding.squeeze().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load Sellers and NACE Data\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(sellers_filepath, nace_codes_filepath):\n",
    "    sellers_df = pd.read_csv(sellers_filepath)\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return sellers_df, nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "# Filepaths (update to your actual paths)\n",
    "# sellers_filepath = './data/dejuna_buyer_latest.csv'\n",
    "sellers_filepath = originalSalesNexxtChangeData\n",
    "nace_codes_filepath = './data/nace_codes_object_du.json'\n",
    "\n",
    "# Load data\n",
    "sellers_df, nace_codes = load_data(sellers_filepath, nace_codes_filepath)\n",
    "print(\"🚀 Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize Hugging Face model and tokenizer\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "print(f\"🚀 Loaded Hugging Face model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "nace_embeddings = create_hf_embeddings(nace_descriptions, tokenizer, model)\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#  lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "branchen_embeddings = create_hf_embeddings(sellers_df['preprocessed_branchen'].tolist(), tokenizer, model)\n",
    "print(\"🚀 Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "\n",
    "similarity_threshold = 0.7\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] if row['assigned_nace_similarity'] >= similarity_threshold else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = sellers_filepath.replace(\".csv\", \"_hf_nace.csv\")\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"🚀 Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[[ 'assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GEOCODING LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_unique_locations(buyers_df, sellers_df):\n",
    "    \"\"\"Extract all unique locations from buyers and sellers dataframes.\"\"\"\n",
    "    unique_locations = set()\n",
    "\n",
    "    for df, name in [(buyers_df, 'buyers'), (sellers_df, 'sellers')]:\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            # logging.info(f'Extracted locations: {location.lower().split('\\n')}')\n",
    "            unique_locations.update(locations)\n",
    "            \n",
    "\n",
    "    logging.info(f'Total unique locations found: {len(unique_locations)}')\n",
    "    return unique_locations\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode unique locations with caching.\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"buyer_seller_matching\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3, error_wait_seconds=10.0)\n",
    "\n",
    "    # Ensure cache directory exists\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if cache_dir and not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        for location in unique_locations:\n",
    "\n",
    "            if location in geocode_cache:\n",
    "                continue  # Already cached\n",
    "            try:\n",
    "                logging.info(f'Geocoding location: {location}')\n",
    "                loc = geocode(location + \", Germany\")\n",
    "                if loc:\n",
    "                    geocode_cache[location] = {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "                    logging.info(f'Geocoded {location}: ({loc.latitude}, {loc.longitude})')\n",
    "                else:\n",
    "                    geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "                    logging.warning(f'Geocoding failed for location: {location}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Geocoding error for location '{location}': {e}\")\n",
    "                geocode_cache[location] = {'latitude': None, 'longitude': None}\n",
    "\n",
    "def update_dataframe_with_geocodes(df, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Add latitude and longitude columns to the dataframe based on locations.\"\"\"\n",
    "    with shelve.open(cache_path) as geocode_cache:\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "\n",
    "        for idx, location in df['location'].items():\n",
    "            locations = _extract_location_parts(location)\n",
    "            lat_list = []\n",
    "            lon_list = []\n",
    "            for loc in locations:\n",
    "                geocode_info = geocode_cache.get(loc, {'latitude': None, 'longitude': None})\n",
    "                if geocode_info['latitude'] is not None and geocode_info['longitude'] is not None:\n",
    "                    lat_list.append(geocode_info['latitude'])\n",
    "                    lon_list.append(geocode_info['longitude'])\n",
    "                else:\n",
    "                    # If geocoding failed, append None\n",
    "                    lat_list.append(None)\n",
    "                    lon_list.append(None)\n",
    "            # Convert lists to JSON strings for CSV compatibility\n",
    "            latitudes.append(json.dumps(lat_list))\n",
    "            longitudes.append(json.dumps(lon_list))\n",
    "\n",
    "    df['latitude'] = latitudes\n",
    "    df['longitude'] = longitudes\n",
    "    return df\n",
    "\n",
    "# Paths to input and output files\n",
    "buyer_filepath = './data/dejuna_buyer_latest_hf_nace.csv'\n",
    "sellers_filepath = './data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv'\n",
    "\n",
    "cache_path = './geocode_cache.db'\n",
    "\n",
    "# Load buyer and seller datasets\n",
    "\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "unique_locations = get_all_unique_locations(buyers_df, sellers_df)\n",
    "unique_locations\n",
    "# Geocode locations with caching\n",
    "geocode_locations(unique_locations, cache_path=cache_path)\n",
    "\n",
    "# Update dataframes with geocodes\n",
    "\n",
    "# # logging.info('Updating sellers dataframe with geocodes...')\n",
    "sellers_df = update_dataframe_with_geocodes(sellers_df, cache_path=cache_path)\n",
    "buyers_df = update_dataframe_with_geocodes(buyers_df, cache_path=cache_path)\n",
    "\n",
    "# # Save updated dataframes to new CSV files\n",
    "logging.info('Saving updated sellers dataframe...')\n",
    "sellers_output_file = sellers_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "buyers_output_file = buyer_filepath.replace(\".csv\", \"_geocoded.csv\")\n",
    "sellers_df.to_csv(sellers_output_file, index=False)\n",
    "buyers_df.to_csv(buyers_output_file, index=False)\n",
    "logging.info('Geocoding process completed successfully.')\n",
    "print(f\"🚀 Saved sellers data with assigned geocodes to: {buyers_output_file}\\n {sellers_output_file}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "'''Hamburg\n",
    "Schleswig-Holstein\n",
    "Berlin\n",
    "\n",
    "'''\n",
    "location1 = 'Berlin'\n",
    "location2 = 'Brandenburg'\n",
    "\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[10:30]\n",
    "purchase = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')[10:30]\n",
    "\n",
    "\n",
    "sales['processed_location'] = sales['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "purchase['processed_location'] = purchase['location'].apply(lambda x: list(_extract_location_parts(x)))\n",
    "\n",
    "# # Check if any element of sales processed_location is in purchase processed_location\n",
    "purchase['latitude'] = purchase['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "purchase['longitude'] = purchase['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "sales['latitude'] = sales['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sales['longitude'] = sales['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "\n",
    "def match_locations(sales_locations, purchase_locations):\n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "\n",
    "# Create a dataframe to store matched locations\n",
    "matched_locations = []\n",
    "\n",
    "for idx, row in sales.iterrows():\n",
    "    for idx2, row2 in purchase.iterrows():\n",
    "        if match_locations(row.processed_location, row2.processed_location):\n",
    "            matched_locations.append({\n",
    "                'sales_id': idx,\n",
    "                'sales_location': row.location,\n",
    "                'sales_processed_location': row.processed_location,\n",
    "                'purchase_id': idx2,\n",
    "                'purchase_location': row2.location,\n",
    "                'sales_longitude': s_lon,\n",
    "                'sales_latitude': s_lat,\n",
    "                'purchase_longitude': p_lon,\n",
    "                'purchase_latitude': p_lat,\n",
    "                'purchase_processed_location': row2.processed_location,\n",
    "                'distance_km': None\n",
    "            })\n",
    "        else:\n",
    "            # Calculate distance between sales and purchase locations\n",
    "            sales_lat_lon = zip(row.latitude, row.longitude)\n",
    "            purchase_lat_lon = zip(row2.latitude, row2.longitude)\n",
    "            for s_lat, s_lon in sales_lat_lon:\n",
    "                for p_lat, p_lon in purchase_lat_lon:\n",
    "                    if s_lat is not None and s_lon is not None and p_lat is not None and p_lon is not None:\n",
    "                        print(f\"Calculating distance between ({s_lat}, {s_lon}) and ({p_lat}, {p_lon})\")\n",
    "                        distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                        print(f\"Distance: {distance}\")\n",
    "                        if distance <= 50:\n",
    "                            matched_locations.append({\n",
    "                                'sales_id': idx,\n",
    "                                'sales_processed_location': row.processed_location,\n",
    "                                'sales_location': row.location,\n",
    "                                'sales_longitude': s_lon,\n",
    "                                'sales_latitude': s_lat,\n",
    "                                'purchase_longitude': p_lon,\n",
    "                                'purchase_latitude': p_lat,\n",
    "                                'purchase_id': idx2,\n",
    "                                'purchase_location': row2.location,\n",
    "                                'purchase_processed_location': row2.processed_location,\n",
    "                                'distance_km': distance\n",
    "                            })\n",
    "matched_locations_df = pd.DataFrame(matched_locations)\n",
    "print(matched_locations_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "german_stop_words = stopwords.words('german')\n",
    "\n",
    "# Load synonyms CSV\n",
    "synonyms_df = pd.read_csv('./data/Updated_Keywords_and_Synonyms.csv')\n",
    "synonym_dict = {}\n",
    "for _, row in synonyms_df.iterrows():\n",
    "    keyword = row['Keyword'].lower()\n",
    "    synonyms = row.dropna().tolist()[1:]\n",
    "    synonyms = [syn.lower() for syn in synonyms]\n",
    "    synonym_dict[keyword] = synonyms\n",
    "\n",
    "# Define augmentation functions\n",
    "def extract_keywords(text, top_n=5):\n",
    "    # vectorizer = TfidfVectorizer(stop_words='german', max_features=top_n)\n",
    "    vectorizer = CountVectorizer(stop_words = german_stop_words) # Now use this in your pipeline\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "def augment_text_with_synonyms(text, top_n=5):\n",
    "    keywords = extract_keywords(text, top_n)\n",
    "    synonyms = []\n",
    "    for word in keywords:\n",
    "        synonyms.extend(synonym_dict.get(word, []))\n",
    "    synonyms = ' '.join(synonyms)\n",
    "    return f\"{text} {synonyms}\"\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def preprocess_text_de(text):\n",
    "    doc = nlp_de(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "def analyze_matches(matches_df, buyers_df, sellers_df):\n",
    "    \"\"\"Analyze matching results and print key metrics.\"\"\"\n",
    "    logging.info(\"\\n=== Matching Analysis ===\")\n",
    "    \n",
    "    total_buyers = len(buyers_df)\n",
    "    total_sellers = len(sellers_df)\n",
    "    total_matches = len(matches_df)\n",
    "    \n",
    "    logging.info(f\"Total buyers: {total_buyers}\")\n",
    "    logging.info(f\"Total sellers: {total_sellers}\") \n",
    "    logging.info(f\"Total matches found: {total_matches}\")\n",
    "    if total_buyers > 0:\n",
    "        logging.info(f\"Average matches per buyer: {total_matches/total_buyers:.2f}\")\n",
    "    else:\n",
    "        logging.info(\"No buyers to match against.\")\n",
    "\n",
    "    # Save top matches for manual review\n",
    "    top_matches = matches_df.head(10)\n",
    "    top_matches.to_csv('./matches/top_matches_for_review.csv', index=False)\n",
    "    logging.info(\"Saved top 10 matches for manual review\")\n",
    "    \n",
    "    return {\n",
    "        'total_matches': total_matches,\n",
    "        'matches_per_buyer': total_matches/total_buyers if total_buyers else 0,\n",
    "        'buyer_match_rate': len(matches_df['buyer_title'].unique())/total_buyers if total_buyers else 0\n",
    "    }\n",
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "# buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "# sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "\n",
    "# Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "logging.info('Loading the Sentence Transformer model...')\n",
    "# model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "# model_name = 'all-MiniLM-L12-v2'\n",
    "model_name ='aari1995/German_Semantic_STS_V2'\n",
    "\n",
    "# model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading model {model_name}: {e}\")\n",
    "\n",
    "# Encode sellers' combined_text\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = get_embedding_batch(seller_texts, model, batch_size=64)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "# Set similarity threshold\n",
    "similarity_threshold = 0.93\n",
    "# Optional text length filter\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info('Starting matching process...')\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_numpy=True, normalize_embeddings=True).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarity scores to all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "\n",
    "    # Indices above threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "\n",
    "    for seller_idx in matching_indices:\n",
    "        seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "        confidence_score = sim_scores[seller_idx]\n",
    "        if confidence_score < similarity_threshold:\n",
    "            continue\n",
    "\n",
    "        confidence_scores.append(confidence_score)\n",
    "        \n",
    "        match = {\n",
    "            'buyer_date': buyer_row.get('date', ''),\n",
    "            'buyer_title': buyer_row.get('title', ''),\n",
    "            'buyer_description': buyer_row.get('description', ''),\n",
    "            'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "            'buyer_location': buyer_row.get('location', ''),\n",
    "            'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "            'seller_date': seller_row.get('date', ''),\n",
    "            'seller_title': seller_row.get('title', ''),\n",
    "            'seller_description': seller_row.get('description', ''),\n",
    "            'seller_long_description': seller_row.get('long_description', ''),\n",
    "            'seller_location': seller_row.get('location', ''),\n",
    "            'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "            \n",
    "            'similarity_score': confidence_score\n",
    "        }\n",
    "        matches.append(match)\n",
    "\n",
    "    # Progress logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "logging.info('Creating matches DataFrame...')\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "\n",
    "    # Sort by confidence score\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "\n",
    "    # Save all matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    logging.info(f'Saved all matches: {len(matches_df)} records => {output_all}')\n",
    "\n",
    "    # Analyze results\n",
    "    metrics = analyze_matches(matches_df, buyers_df, sellers_df)\n",
    "    \n",
    "    # Optionally filter for high confidence\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= 0.95]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f'Saved high confidence matches: {len(high_conf_df)} records => {output_high_conf}')\n",
    "else:\n",
    "    logging.info('No matches found.')\n",
    "\n",
    "# Final memory cleanup\n",
    "del seller_embeddings\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:22:48,845 - INFO - Loading datasets...\n",
      "2025-02-03 01:22:48,983 - INFO - Preprocessing buyers' text fields...\n",
      "2025-02-03 01:22:51,031 - INFO - Preprocessing sellers' text fields...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].str.replace('>', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:29:01,865 - INFO - Processing locations...\n",
      "2025-02-03 01:29:01,931 - INFO - Combining text fields...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Location processing\n",
    "logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_locations(sales_locations, purchase_locations):\n",
    "    \"\"\"\n",
    "    Check if any element of sales_locations is in purchase_locations.\n",
    "    \"\"\"\n",
    "    print(f'purchase_locations: {purchase_locations}, sales_locations: {sales_locations}, {any(loc in purchase_locations for loc in sales_locations)}')\n",
    "    \n",
    "    return any(loc in purchase_locations for loc in sales_locations)\n",
    "# def match_locations(locations_input, target_location):\n",
    "#     # Step 1: Normalize and parse the locations into a list of location parts\n",
    "#     def parse_location(location):\n",
    "#         return [part.strip().lower() for part in location.split(\">\")]\n",
    "\n",
    "#     # Parse the target location\n",
    "#     target_parts = parse_location(target_location)\n",
    "    \n",
    "#     # Split the multiple input locations by line breaks and parse them\n",
    "#     locations = locations_input.strip().split(\"\\n\")\n",
    "#     parsed_locations = [parse_location(loc) for loc in locations]\n",
    "\n",
    "#     # Step 2: Function to compare the parsed locations with the target\n",
    "#     def compare_location(loc1_parts, loc2_parts):\n",
    "#         max_level = max(len(loc1_parts), len(loc2_parts))\n",
    "#         match_score = 0\n",
    "\n",
    "#         for level in range(max_level):\n",
    "#             loc1_value = loc1_parts[level] if level < len(loc1_parts) else None\n",
    "#             loc2_value = loc2_parts[level] if level < len(loc2_parts) else None\n",
    "            \n",
    "#             if loc1_value and loc2_value:\n",
    "#                 # Exact match at this level\n",
    "#                 if loc1_value == loc2_value:\n",
    "#                     match_score += 1\n",
    "#                 else:\n",
    "#                     break  # If any level does not match, break early\n",
    "#             elif loc1_value is None and loc2_value is None:\n",
    "#                 continue  # Both are missing, no issue\n",
    "#             else:\n",
    "#                 match_score += 0.5  # Partial match (one location is more specific)\n",
    "\n",
    "#         return match_score\n",
    "\n",
    "#     # Step 3: Check if any location matches (either exact or partial)\n",
    "#     # for loc_parts in parsed_locations:\n",
    "#     #     match_score = compare_location(loc_parts, target_parts)\n",
    "#     #     if match_score > 0:  # If there's a partial match or exact match\n",
    "#     #         return True\n",
    "#     for loc_parts in parsed_locations:\n",
    "#         match_score = compare_location(loc_parts, target_parts)\n",
    "#         print(\" > \".join(loc_parts))\n",
    "#         if match_score > 0:  \n",
    "#             if target_coords and location_coordinates.get(\" > \".join(loc_parts)):\n",
    "#                 loc_coords = location_coordinates.get(\" > \".join(loc_parts))\n",
    "#                 if loc_coords:\n",
    "#                     lat1, lon1 = target_coords\n",
    "#                     lat2, lon2 = loc_coords\n",
    "#                     distance = haversine(lat1, lon1, lat2, lon2)\n",
    "#                     if distance <= max_distance_km:\n",
    "#                         return True \n",
    "#             else:\n",
    "#                 return True\n",
    "\n",
    "#     # If no matches found, return False\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:29:11,732 - INFO - Loading SentenceTransformer and CrossEncoder models...\n",
      "2025-02-03 01:29:11,743 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-03 01:29:11,743 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n",
      "2025-02-03 01:29:14,118 - INFO - Use pytorch device: cpu\n",
      "2025-02-03 01:29:14,134 - INFO - Encoding sellers' text...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a0102b9b6b47f0b8857d2c165d1cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:37:32,960 - INFO - Sellers' embeddings generated.\n"
     ]
    }
   ],
   "source": [
    "# # Augment text with synonyms\n",
    "# logging.info('Augmenting buyers\\' text with synonyms...')\n",
    "# buyers_df['augmented_text'] = buyers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "# logging.info('Augmenting sellers\\' text with synonyms...')\n",
    "# sellers_df['augmented_text'] = sellers_df['combined_text'].apply(lambda x: augment_text_with_synonyms(x, top_n=5))\n",
    "\n",
    "# # Preprocess augmented text\n",
    "# buyers_df['final_text'] = buyers_df['combined_text'].apply(preprocess_text_de) + ' ' + buyers_df['augmented_text'].apply(preprocess_text_de)\n",
    "# sellers_df['final_text'] = sellers_df['combined_text'].apply(preprocess_text_de) + ' ' + sellers_df['augmented_text'].apply(preprocess_text_de)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Load Models\n",
    "# -----------------------------------\n",
    "logging.info(\"Loading SentenceTransformer and CrossEncoder models...\")\n",
    "# bi_encoder = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer(\"xlm-roberta-base\")\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2')\n",
    "# bi_encoder = SentenceTransformer('aari1995/German_Semantic_STS_V2')\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Encode Sellers' Text\n",
    "# -----------------------------------\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:37:33,803 - INFO - Starting matching process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad3bfae8e0646d8a7d756d948bc073f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239f235bc8614a588f5a9ba911fdf2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:41:14,620 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Baden-Württemberg > Stuttgart > Stuttgart\n",
      "2025-02-03 01:41:14,644 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Baden-Württemberg > Karlsruhe > Calw\n",
      "2025-02-03 01:41:14,652 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Bayern > Mittelfranken > Roth\n",
      "2025-02-03 01:41:14,661 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Baden-Württemberg > Stuttgart > Esslingen\n",
      "2025-02-03 01:41:14,673 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Baden-Württemberg > Stuttgart > Esslingen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6941920624cbca729228cb64015ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcdcc31c66946b8bc954961f956ca36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4832998ce7dd424b8b61deb96b2ffd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afb9baa07bf41ed8b80e63e8f7d8d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201b3de726324241a4f30c3755777ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:44:43,384 - INFO - Match found: Buyer location: Nordrhein-Westfalen > Düsseldorf, Seller location: Nordrhein-Westfalen > Köln > Köln\n",
      "2025-02-03 01:44:43,392 - INFO - Match found: Buyer location: Nordrhein-Westfalen > Düsseldorf, Seller location: Nordrhein-Westfalen > Düsseldorf > Rhein-Kreis Neuss\n",
      "2025-02-03 01:44:43,401 - INFO - Match found: Buyer location: Nordrhein-Westfalen > Düsseldorf, Seller location: Nordrhein-Westfalen > Düsseldorf > Düsseldorf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fc2e5396dd4cf0a308ca384daf9fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803695c1723b4c84b054b98497a2bece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca7a7003d364ddfbdab794fc71604cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:46:45,840 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen\n",
      "2025-02-03 01:46:45,845 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Niedersachsen\n",
      "2025-02-03 01:46:45,851 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Hagen\n",
      "2025-02-03 01:46:45,853 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen\n",
      "2025-02-03 01:46:45,855 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen\n",
      "2025-02-03 01:46:45,860 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Arnsberg > Märkischer Kreis\n",
      "2025-02-03 01:46:45,870 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Arnsberg > Märkischer Kreis\n",
      "2025-02-03 01:46:45,873 - INFO - Match found: Buyer location: Hessen\n",
      "Nordrhein-Westfalen\n",
      "Niedersachsen, Seller location: Nordrhein-Westfalen > Detmold > Lippe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb160896e90048e291ba9750321caea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ea9f460d0e4b4180218d904baeb9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e0765241be4720acca883124f61a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8300f2a8ebb54ba587773edc15974def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7655ccf170004533a5a1c1f066122dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:48:23,764 - INFO - Match found: Buyer location: Niedersachsen > Lüneburg\n",
      "Hamburg\n",
      "Schleswig-Holstein\n",
      "Mecklenburg-Vorpommern, Seller location: Schleswig-Holstein\n",
      "2025-02-03 01:48:23,773 - INFO - Match found: Buyer location: Niedersachsen > Lüneburg\n",
      "Hamburg\n",
      "Schleswig-Holstein\n",
      "Mecklenburg-Vorpommern, Seller location: Schleswig-Holstein > Schleswig-Holstein\n",
      "2025-02-03 01:48:23,775 - INFO - Match found: Buyer location: Niedersachsen > Lüneburg\n",
      "Hamburg\n",
      "Schleswig-Holstein\n",
      "Mecklenburg-Vorpommern, Seller location: Schleswig-Holstein > Schleswig-Holstein\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0aaa33b24224d61b103c26f30073aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41065467b7e94063887bca87b7faab3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eab2a1b0f904c8cb5619d48966812dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2bf908c2b94b248c1281df695678f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadd3f65a0994cbcb1b2f54ac12a0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc06212eefa40c5ac490ae3fceb0dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:53:04,001 - INFO - Match found: Buyer location: Bremen\n",
      "Niedersachsen, Seller location: Niedersachsen\n",
      "2025-02-03 01:53:04,014 - INFO - Match found: Buyer location: Bremen\n",
      "Niedersachsen, Seller location: Niedersachsen > Lüneburg > Verden\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccb1f853da3487fa716e3679e6aed67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0ad7c9c969424da0dd79d19c0db2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:53:46,616 - INFO - Match found: Buyer location: Berlin\n",
      "Sachsen > Leipzig\n",
      "Sachsen > Dresden\n",
      "Brandenburg\n",
      "Niedersachsen > Hannover, Seller location: Berlin > Berlin > Berlin\n",
      "2025-02-03 01:53:46,633 - INFO - Match found: Buyer location: Berlin\n",
      "Sachsen > Leipzig\n",
      "Sachsen > Dresden\n",
      "Brandenburg\n",
      "Niedersachsen > Hannover, Seller location: Brandenburg > Brandenburg > Potsdam\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3479d8c5a7a445d984fdc3b104bfc5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2c08cb2e254ecc8e89c74ec057168a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7ff91a2ea44ca99003d3ea20327c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792a02fdec9949d69901bb9bde9d7b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaddcbb07254fa5886cf0c503f72dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594e5f2e1c444ebba808a0eb9d52825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a951ffa3748fda15539da0e69a323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68a7dc89cb94d17a36da1107b7eb312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e91011c68ee4c6581bdb133a9110fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfc035294ed492ba6d163a9ebeaf76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ea4c1cc1674d19a40828190331e7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9317d19876a64104a99a486fc5e73a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f3e0aa0994cd780f18f9a3128a6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fe0881c53946aca6ba5fc439d58b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199991a08357420d83b7cefd45a5c084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933035a96a314fe9b14dcbe74b3004a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02c862638f6b46359d482aecb4aef65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5893fb351b94e329be6feb7af07db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:00:50,616 - INFO - Match found: Buyer location: Hessen\n",
      "Sachsen\n",
      "Baden-Württemberg, Seller location: Baden-Württemberg\n",
      "2025-02-03 02:00:50,619 - INFO - Match found: Buyer location: Hessen\n",
      "Sachsen\n",
      "Baden-Württemberg, Seller location: Baden-Württemberg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3837da8c459b4977a0da916efe04bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce02a277571746f3867f2fb5a6630f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34e49c373284b3aa5b6cce74dfdbbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7917eeff86436bb1b6eae80ba8531b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12de27bb071461eb1f0d7c95d5c70eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32c776c635442d6b03466d2fb23cba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0bd0fbbc7c4f669a5ba1ca790b7a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750115b323b04634a4f3dc9eb813aa20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbae5fe7c65e4497bc67cab274770063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b092fac944a05ba0f6a3ca218342f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fb583892944c86836a5c306d00667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d8daf15fc6498da60ffe0c4d4f9d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c884aadbbf4ae3b403cb476b77247a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3261b83871ca4e28947514f9d0ad0001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a439634054314b7aa80ba2ee6823f958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9437595219634845adefeba89f49826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dafd2126a84b12b685e5c2731c76a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af283fca2f34eb3b46295c02fb1ce82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7accdf4fa098434286fd924f797e4af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a50ca0e9e74e6a8379c29e412f7c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4f2e5ee9d7470094ad620f91873261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:08:09,296 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Baden-Württemberg > Stuttgart\n",
      "2025-02-03 02:08:09,300 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Bayern\n",
      "2025-02-03 02:08:09,309 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Bayern\n",
      "2025-02-03 02:08:09,311 - INFO - Match found: Buyer location: Baden-Württemberg\n",
      "Bayern, Seller location: Bayern\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "555d463716df4dec9d6507472ddc20fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a05bd8f444d47ae902f3cb6777a8cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6111e5434b4f1185967bd454a09de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db92db204b486a909403a66b00b677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d956c226ceb494a84fc8174830e8839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739baf5c5b904c6e80730fba1bbc0031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ebf44cde3e4a15816401829bfe1ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:09:36,655 - INFO - Match found: Buyer location: Hamburg\n",
      "Schleswig-Holstein\n",
      "Baden-Württemberg > Freiburg\n",
      "Bayern > Oberbayern, Seller location: Baden-Württemberg > Freiburg > Emmendingen\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78d7708405e4e30a91fb70fb7445f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716990c04de34989a37a407cad6a021f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:10:33,765 - INFO - Match found: Buyer location: Baden-Württemberg, Seller location: Baden-Württemberg > Karlsruhe > Enzkreis\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d9e4b69a984cd7b8605fc2fd304f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1669f3b489a46889412ca13d91c635a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adddff5510e4fc7af944f41e6599729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78207126103c4d2bb55208ff26090278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ee3130ea3f4034bea3df9143b9863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024277f81e524ee58afaa668d6c0e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acefee834534a88952e9a19740b400c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501c98557b5c4ff5abc98036728fad6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0968f2ae8ba749ba9f3696e633c7f3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d28a079c6a24474b818f84f56e36ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002004255ead48ebbb2b1ae56887c632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec953acb6bc40fc90a2de0b28888283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4126c6bd1a89492d9927a2052af2905b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34634ec2dd84e93ae6f5b8da67039da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1119219f4a24f16b95435e6d34a1521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c00091f3f14aa38b779e6ffda695f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1222eff20b1c412182827d9f816a9427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:13:32,616 - INFO - Match found: Buyer location: Hamburg\n",
      "Schleswig-Holstein\n",
      "Berlin, Seller location: Berlin > Berlin > Berlin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4710dd7a5c145d084bf8d6258a4b748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a57ffc566841eaa3cf3f17c9209c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9dd3e72f1c4525bc3faec2de9541fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38514f8954747c08379f54b5cce614e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa7d8e83d424b739214f98ee3371422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:15:23,744 - INFO - Match found: Buyer location: Nordrhein-Westfalen > Münster, Seller location: Nordrhein-Westfalen > Münster > Steinfurt\n",
      "2025-02-03 02:15:23,753 - INFO - Match found: Buyer location: Nordrhein-Westfalen > Münster, Seller location: Nordrhein-Westfalen > Münster > Münster\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85cc15631584b628641650800a33416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d81b6eb75594afa9ab8422e65ea5e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53a60e04d4c480288e2ed3703398a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd40eb65efc34c4f8792a4516e597470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84ce37ad6ff452baa95abb841f1b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c85d6eb0404b099ff61675b1afd57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e177250ea7d4a49a831eb1b9bbb7c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3681395fbd4649aaac717809698c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59caed0ad44e416c881f03a7bedf2abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3eaf1150bcf4229ab07ae08d8cfcdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d42f36c69044018b74a89aa082602a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87b55535bac4d39b329c999edd70fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d31072219c94699805e6cd3353d60aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6cec3931e044cc9c0586b334bcac63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffd013fdf6842bb981dac47e88452b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:17:54,028 - INFO - Match found: Buyer location: Bayern > Oberbayern > München, Seller location: Bayern > Oberbayern\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9616efca695646419f8a3d37a5b867f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7220e4bbcb6247f18b87592b0978d78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 02:19:36,175 - INFO - Creating matches DataFrame...\n",
      "2025-02-03 02:19:36,888 - INFO - Saved all matches: 35 records => ./matches/nlp_business_all_matches_03_02-19.csv\n",
      "2025-02-03 02:19:36,903 - INFO - Saved high confidence matches: 35 => ./matches/nlp_business_high_conf_03_02-19.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------\n",
    "# 6. Matching Parameters\n",
    "# -----------------------------------\n",
    "similarity_threshold = 0.60\n",
    "cross_encoder_threshold = 0.65\n",
    "top_n = 100  # Number of top candidates to re-rank per buyer\n",
    "\n",
    "matches = []\n",
    "confidence_scores = []\n",
    "\n",
    "total_buyers = len(buyers_df)\n",
    "logging.info(\"Starting matching process...\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 7. Matching Loop\n",
    "# -----------------------------------\n",
    "matched_locations = []\n",
    "\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyers_df.iloc[i]['latitude']\n",
    "    buyer_longitudes = buyers_df.iloc[i]['longitude']\n",
    "    buyer_locations = buyers_df.iloc[i]['location']\n",
    "\n",
    "\n",
    "    # Encode buyer text\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute cosine similarities with all sellers\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_embeddings)[0]\n",
    "    \n",
    "    # Get indices of sellers with similarity >= threshold\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue  # No matches above threshold\n",
    "    \n",
    "    # Select top N matches based on similarity scores\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:top_n]]\n",
    "    \n",
    "    # Prepare pairs for cross-encoder\n",
    "    buyer_texts = [buyer_text] * len(top_indices)\n",
    "    seller_texts_top = [sellers_df.iloc[idx]['combined_text'] for idx in top_indices]\n",
    "    pairs = list(zip(buyer_texts, seller_texts_top))\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    cross_scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Filter based on cross-encoder threshold\n",
    "    for seller_idx, cross_score in zip(top_indices, cross_scores):\n",
    "\n",
    "\n",
    "        if cross_score >= cross_encoder_threshold:\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            # if match_locations(seller_locations, buyer_locations):\n",
    "            #     location_match = True\n",
    "            # else:\n",
    "            # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km threshold\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "            if location_match:\n",
    "                logging.info(f\"Match found: Buyer location: {buyer_locations}, Seller location: {seller_locations}\")\n",
    "                match = {\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller_row.get('id', ''),\n",
    "                    'seller_date': seller_row.get('date', ''),\n",
    "                    'seller_title': seller_row.get('title', ''),\n",
    "                    'seller_description': seller_row.get('description', ''),\n",
    "                    'seller_long_description': seller_row.get('long_description', ''),\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'seller_source': seller_row.get('url', ''),\n",
    "                    'seller_nace_code': seller_row.get('nace_code', ''),\n",
    "                    \n",
    "                    'similarity_score': cross_score,\n",
    "                    'distance_km': distance_km if distance_km else 'Within processed locations'\n",
    "\n",
    "                }\n",
    "                matches.append(match)\n",
    "                confidence_scores.append(cross_score)\n",
    "    \n",
    "    # Progress Logging\n",
    "    if (i + 1) % 50 == 0:\n",
    "        logging.info(f\"Processed {i+1}/{total_buyers} buyers.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 8. Create Matches DataFrame\n",
    "# -----------------------------------\n",
    "logging.info(\"Creating matches DataFrame...\")\n",
    "matches_df = pd.DataFrame(matches)\n",
    "        \n",
    "if not matches_df.empty:\n",
    "    matches_df['confidence_score'] = confidence_scores\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "# del seller_embeddings\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def geocode(location):\n",
    "    \"\"\"\n",
    "    Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "    Args:\n",
    "        location (str): The location string to geocode.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location, or None if not found.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "    location_data = geolocator.geocode(location)\n",
    "    if location_data:\n",
    "        return location_data.latitude, location_data.longitude\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# 2025-01-21 12:59:12,950 - INFO - Match found: Buyer location: ['Baden-Württemberg', 'Bayern'], Seller location: ['Rhein-Neckar-Kreis']\n",
    "\n",
    "# purchase_locations: ['Baden-Württemberg', 'Bayern'], sales_locations: ['Rhein-Neckar-Kreis'], False\n",
    "location1 = 'Brandenburg > Brandenburg > Potsdam'\n",
    "location2 = 'Osnabrück'\n",
    "#  {'Berlin',\n",
    "#  'Brandenburg',\n",
    "#  'Dresden',\n",
    "#  'Hannover',\n",
    "#  'Leipzig',\n",
    "#  'Niedersachsen',\n",
    "#  'Sachsen'}\n",
    "coordinates1 = geocode(location1 + \", Germany\")\n",
    "coordinates2 = geocode(location2 + \", Germany\")\n",
    "\n",
    "if coordinates1 and coordinates2:\n",
    "    distance = geodesic(coordinates1, coordinates2).kilometers\n",
    "    print(f\"Coordinates for '{location1}': {coordinates1}\")\n",
    "    print(f\"Coordinates for '{location2}': {coordinates2}\")\n",
    "    print(f\"Distance between '{location1}' and '{location2}': {distance:.2f} km\")\n",
    "else:\n",
    "    print(\"One or both locations could not be geocoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from geopy.geocoders import Nominatim\n",
    "# from geopy.distance import geodesic\n",
    "# import math\n",
    "\n",
    "# # Function to geocode a location string into latitude and longitude\n",
    "# def geocode(location):\n",
    "#     \"\"\"\n",
    "#     Geocodes a location string to latitude and longitude.\n",
    "    \n",
    "#     Args:\n",
    "#         location (str): The location string to geocode.\n",
    "    \n",
    "#     Returns:\n",
    "#         tuple: Latitude and longitude of the location, or None if not found.\n",
    "#     \"\"\"\n",
    "#     geolocator = Nominatim(user_agent=\"geoapi\")\n",
    "#     location_data = geolocator.geocode(location)\n",
    "#     if location_data:\n",
    "#         return location_data.latitude, location_data.longitude\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # Haversine formula to calculate distance between two points (lat1, lon1) and (lat2, lon2)\n",
    "# def haversine(lat1, lon1, lat2, lon2):\n",
    "#     R = 6371  # Radius of the Earth in kilometers\n",
    "#     phi1 = math.radians(lat1)\n",
    "#     phi2 = math.radians(lat2)\n",
    "#     delta_phi = math.radians(lat2 - lat1)\n",
    "#     delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "#     a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
    "#     c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "#     distance = R * c  # Distance in kilometers\n",
    "#     return distance\n",
    "\n",
    "\n",
    "# def match_locations(locations_input, target_location, max_distance_km=50):\n",
    "#     # Step 1: Normalize and parse the locations into a list of location parts\n",
    "#     def parse_location(location):\n",
    "#         return [part.strip().lower() for part in location.split(\">\")]\n",
    "\n",
    "#     # Parse the target location\n",
    "#     target_parts = parse_location(target_location)\n",
    "#     target_coords = geocode(target_location)  # Get coordinates of the target location\n",
    "    \n",
    "#     if not target_coords:\n",
    "#         return False  # If target location couldn't be geocoded, return False\n",
    "\n",
    "#     # Split the multiple input locations by line breaks and parse them\n",
    "#     locations = locations_input.strip().split(\"\\n\")\n",
    "#     parsed_locations = [parse_location(loc) for loc in locations]\n",
    "\n",
    "#     # Step 2: Function to compare the parsed locations with the target\n",
    "#     def compare_location(loc1_parts, loc2_parts):\n",
    "#         max_level = max(len(loc1_parts), len(loc2_parts))\n",
    "#         match_score = 0\n",
    "\n",
    "#         for level in range(max_level):\n",
    "#             loc1_value = loc1_parts[level] if level < len(loc1_parts) else None\n",
    "#             loc2_value = loc2_parts[level] if level < len(loc2_parts) else None\n",
    "            \n",
    "#             if loc1_value and loc2_value:\n",
    "#                 # Exact match at this level\n",
    "#                 if loc1_value == loc2_value:\n",
    "#                     match_score += 1\n",
    "#                 else:\n",
    "#                     break  # If any level does not match, break early\n",
    "#             elif loc1_value is None and loc2_value is None:\n",
    "#                 continue  # Both are missing, no issue\n",
    "#             else:\n",
    "#                 match_score += 0.5  # Partial match (one location is more specific)\n",
    "\n",
    "#         return match_score>0\n",
    "\n",
    "    # # Step 3: Check if any location matches (either exact or partial) and is within proximity\n",
    "    # for loc_parts in parsed_locations:\n",
    "    #     match_score = compare_location(loc_parts, target_parts)\n",
    "\n",
    "    #     # If there's a partial or exact match, we also check the geographical proximity\n",
    "    #     if match_score > 0:  # If there's a name match\n",
    "    #         location_str = \" > \".join(loc_parts)\n",
    "    #         loc_coords = geocode(location_str)  # Get coordinates of the input location\n",
    "            \n",
    "    #         if loc_coords:\n",
    "    #             lat1, lon1 = target_coords\n",
    "    #             lat2, lon2 = loc_coords\n",
    "    #             distance=geodesic(target_coords, loc_coords).kilometers\n",
    "    #             logging.info(f\"Distance between '{target_location}' and '{location_str}': {distance:.2f} km\")\n",
    "    #             # distance = haversine(lat1, lon1, lat2, lon2)\n",
    "    #             if distance <= max_distance_km:\n",
    "    #                 return True  # Match found within the proximity\n",
    "    #         else:\n",
    "    #             continue  # If geocoding failed for the input location, skip it\n",
    "\n",
    "    # # If no matches found, return False\n",
    "    # return False\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# locations_input = '''Berlin\n",
    "# Sachsen > Leipzig\n",
    "# Sachsen > Dresden\n",
    "# Brandenburg\n",
    "# Niedersachsen > Hannover'''\n",
    "\n",
    "# target_location = '''Brandenburg > Brandenburg'''\n",
    "\n",
    "# # Call the function\n",
    "# result = match_locations(locations_input, target_location)\n",
    "# print(result)  # True if any match (name + proximity), False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sellers and buyers files into dataframes\n",
    "sellers_df = pd.read_csv(sellers_filepath)\n",
    "buyers_df = pd.read_csv(buyer_filepath)\n",
    "\n",
    "# Add origin column to both dataframes\n",
    "sellers_df['origin'] = 'seller'\n",
    "buyers_df['origin'] = 'buyer'\n",
    "buyers_df['branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} > {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Concatenate both dataframes\n",
    "combined_df = pd.concat([sellers_df, buyers_df], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NACE Labels for Combined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5. Load your Seller/Branchen data and NACE codes\n",
    "# -------------------------------------------------------------------------\n",
    "def load_data(nace_codes_filepath):\n",
    "    nace_codes = load_nace_codes(nace_codes_filepath)\n",
    "    return nace_codes\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6. MAIN LOGIC\n",
    "# -------------------------------------------------------------------------\n",
    "    # Filepaths (update to your actual paths)\n",
    "# sellers_filepath = originalSalesNexxtChangeData       # CSV with a column 'branchen'\n",
    "sellers_filepath =  dejunaPurchases    # CSV with a column 'branchen'\n",
    "nace_codes_filepath = nacecode_array_obj_du\n",
    "\n",
    "# Load data\n",
    "sellers_df = combined_df \n",
    "nace_codes = load_data(nace_codes_filepath)\n",
    "print(\"🚀 Sellers and NACE codes loaded.\")\n",
    "\n",
    "# Initialize the Sentence Transformer model\n",
    "# For German or multilingual, consider e.g.: \n",
    "#    model_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"🚀 Loaded SentenceTransformer model: {model_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6a. Preprocess NACE descriptions\n",
    "# ---------------------------------------------------------------------\n",
    "# Convert each NACE code's description with the same text preprocessing\n",
    "nace_descriptions = [preprocess_text(desc, nlp) for desc in nace_codes.values()]\n",
    "# Create embeddings for these descriptions\n",
    "nace_embeddings = get_embedding_batch(nace_descriptions, model)\n",
    "# We'll keep a list of NACE codes in the same order\n",
    "nace_code_list = list(nace_codes.keys())\n",
    "print(\"🚀 Created embeddings for NACE descriptions.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6b. Preprocess 'branchen' column in sellers data\n",
    "# ---------------------------------------------------------------------\n",
    "# We'll store it in a new column 'preprocessed_branchen'\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "# Split 'branchen' on '>' and join each part\n",
    "\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "sellers_df['preprocessed_branchen'] = sellers_df['branchen'].apply(\n",
    "    lambda x: ' '.join(x.split('>'))\n",
    ")\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        # row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# # Join 'Sub-Industrie' and 'Industrie' columns for buyer data\n",
    "# sellers_df['preprocessed_branchen'] = sellers_df.apply(\n",
    "#     lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    "# )\n",
    "\n",
    "# Create embeddings for all sellers' branchen\n",
    "branchen_embeddings = get_embedding_batch(sellers_df['combined_text'].tolist(), model)\n",
    "print(\"🚀 Created embeddings for sellers' 'branchen' field.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6c. Compute similarity and assign best-match NACE code\n",
    "# ---------------------------------------------------------------------\n",
    "similarities = cosine_similarity(branchen_embeddings, nace_embeddings)\n",
    "best_match_indices = similarities.argmax(axis=1)\n",
    "\n",
    "# For each seller row, pick the NACE code with highest similarity\n",
    "sellers_df['assigned_nace_code'] = [nace_code_list[idx] for idx in best_match_indices]\n",
    "sellers_df['assigned_nace_similarity'] = [similarities[i][idx] for i, idx in enumerate(best_match_indices)]\n",
    "print(\"🚀 Assigned preliminary NACE codes based on 'branchen' similarity.\")\n",
    "\n",
    "# Optionally set a threshold. If similarity < threshold => 'Unassigned'\n",
    "similarity_threshold = 0.4\n",
    "sellers_df['nace_code'] = sellers_df.apply(\n",
    "    lambda row: row['assigned_nace_code'] \n",
    "                if row['assigned_nace_similarity'] >= similarity_threshold \n",
    "                else 'Unassigned',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6d. Save and review\n",
    "# ---------------------------------------------------------------------\n",
    "output_file = \"buyers_sellers_combined_nace.csv\"\n",
    "sellers_df.to_csv(output_file, index=False)\n",
    "print(f\"🚀 Saved sellers data with assigned NACE codes to: {output_file}\\n\")\n",
    "\n",
    "# Print a small sample\n",
    "print(\"Sample of NACE Code Assignments:\")\n",
    "print(sellers_df[['assigned_nace_code', 'assigned_nace_similarity', 'nace_code']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1= pd.read_csv(\"./matches/nlp_business_all_matches_16_19-31.csv\")\n",
    "temp2= pd.read_csv(\"./matches/nlp_business_all_matches_15_13-11.csv\")\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_df = pd.concat([temp1, temp2])\n",
    "# Get all duplicates based on specific columns\n",
    "# Get all rows that only exist once in the dataframe\n",
    "unique_rows = combined_df.drop_duplicates(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)\n",
    "duplicates = unique_rows[unique_rows.duplicated(subset=['buyer_date', 'buyer_title', 'buyer_description', 'buyer_location', 'seller_date', 'seller_title', 'seller_description', 'seller_location'], keep=False)]\n",
    "\n",
    "# unique_rows.to_csv('./matches/unique_rows.csv', index=False)\n",
    "# Display the unique rows\n",
    "# print(unique_rows)\n",
    "print(duplicates)\n",
    "# Display the duplicates\n",
    "# print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def extract_locations(location_string):\n",
    "    \"\"\"\n",
    "    Extracts locations from a string with nested structure.\n",
    "    \n",
    "    Args:\n",
    "        location_string (str): Input string containing locations in nested format.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted locations.\n",
    "    \"\"\"\n",
    "    locations = []\n",
    "\n",
    "    # Split the input into lines\n",
    "    lines = location_string.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Strip whitespace from the line\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Ignore empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # If the line contains '>', it indicates nested locations\n",
    "        if '>' in line:\n",
    "            # Extract the specific location after the last '>'\n",
    "            nested_location = line.split('>')[-1].strip()\n",
    "            locations.append(nested_location)\n",
    "        else:\n",
    "            # Append the standalone location\n",
    "            locations.append(line)\n",
    "\n",
    "    return locations\n",
    "\n",
    "def get_location_coordinates(location):\n",
    "    \"\"\"\n",
    "    Mock function to return coordinates for a given location.\n",
    "    Replace this with an actual geocoding API in production.\n",
    "\n",
    "    Args:\n",
    "        location (str): The location name.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Latitude and longitude of the location.\n",
    "    \"\"\"\n",
    "    # Mock coordinates for demonstration purposes\n",
    "    coordinates = {\n",
    "        \"Celle\": (52.6226, 10.0815),\n",
    "        \"Hannover\": (52.3759, 9.7320),\n",
    "        \"Karlsruhe\": (49.0069, 8.4037),\n",
    "        \"Mannheim\": (49.4875, 8.4660),\n",
    "        \"Goettingen\": (51.5413, 9.9158),\n",
    "    }\n",
    "    return coordinates.get(location, None)\n",
    "\n",
    "def locations_match(s1, s2):\n",
    "    \"\"\"\n",
    "    Checks if the locations extracted from two strings have any matches.\n",
    "    If no exact match, it calculates the distance between locations.\n",
    "\n",
    "    Args:\n",
    "        s1 (str): First input string containing locations.\n",
    "        s2 (str): Second input string containing locations.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if there are matching locations or distance < 50KM, False otherwise.\n",
    "    \"\"\"\n",
    "    # Extract locations from both strings\n",
    "    locations1 = set(_extract_location_parts(s1))\n",
    "    locations2 = set(_extract_location_parts(s2))\n",
    "\n",
    "    print(f\"Locations 1: {locations1}\")\n",
    "    print(f\"Locations 2: {locations2}\")\n",
    "    # Check for exact match first\n",
    "    if not locations1.isdisjoint(locations2):\n",
    "        return True\n",
    "\n",
    "    # If no exact match, calculate distances\n",
    "    for loc1 in locations1:\n",
    "        for loc2 in locations2:\n",
    "            coord1 = get_location_coordinates(loc1)\n",
    "            coord2 = get_location_coordinates(loc2)\n",
    "\n",
    "            if coord1 and coord2:\n",
    "                distance = geodesic(coord1, coord2).kilometers\n",
    "                print(f\"Distance between {loc1} and {loc2}: {distance} KM\")\n",
    "                if distance < 50:\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "s1 = \"Niedersachen > Celle\"\n",
    "s2 = \"Niedersachsen > Goettingen\"\n",
    "\n",
    "output = locations_match(s1, s2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, CrossEncoder\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshelve\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import os\n",
    "import shelve\n",
    "from unidecode import unidecode\n",
    "import gc\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. Location Processing Functions\n",
    "# -------------------------------------------------------------------------\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract hierarchical location components\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return []\n",
    "    try:\n",
    "        return [part.strip() for part in re.split(r'>|\\n', location) if part.strip()]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Location parsing error: {e}\")\n",
    "        return []\n",
    "\n",
    "def geocode_locations(unique_locations, cache_path='geocode_cache.db'):\n",
    "    \"\"\"Geocode locations with hierarchical fallback\"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"business_matcher\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "    \n",
    "    with shelve.open(cache_path) as cache:\n",
    "        for loc in unique_locations:\n",
    "            if loc not in cache:\n",
    "                try:\n",
    "                    # Try full location first, then fallback through hierarchy\n",
    "                    parts = _extract_location_parts(loc)\n",
    "                    for i in range(len(parts)):\n",
    "                        query = \", \".join(parts[-i-1:]) + \", Germany\"\n",
    "                        result = geocode(query)\n",
    "                        if result:\n",
    "                            cache[loc] = (result.latitude, result.longitude)\n",
    "                            break\n",
    "                    if loc not in cache:\n",
    "                        cache[loc] = (None, None)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Geocoding failed for {loc}: {e}\")\n",
    "                    cache[loc] = (None, None)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Text Processing Functions (German-optimized)\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_german_text(text):\n",
    "    \"\"\"Basic cleaning for German business text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Remove URLs and special characters\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\b\\d{10,}\\b|[^\\wäöüßÄÖÜ\\s]', ' ', text)\n",
    "    \n",
    "    # Handle common business abbreviations\n",
    "    replacements = {\n",
    "        r'\\bMio\\b': 'millionen',\n",
    "        r'\\bTsd\\b': 'tausend',\n",
    "        r'\\bca\\.': 'circa',\n",
    "        r'\\bz\\.B\\.': 'zum beispiel'\n",
    "    }\n",
    "    for pattern, repl in replacements.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    \n",
    "    return unidecode(text).lower().strip()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Core Matching Logic\n",
    "# -------------------------------------------------------------------------\n",
    "# Load data\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')[:100]\n",
    "\n",
    "# Process locations\n",
    "logging.info(\"Processing locations...\")\n",
    "for df in [buyers_df, sellers_df]:\n",
    "    df['location_parts'] = df['location'].apply(_extract_location_parts)\n",
    "    df['primary_location'] = df['location_parts'].apply(\n",
    "        lambda x: x[-1] if x else None\n",
    "    )\n",
    "\n",
    "# Geocode locations\n",
    "unique_locations = set(\n",
    "    buyers_df['primary_location'].dropna().tolist() +\n",
    "    sellers_df['primary_location'].dropna().tolist()\n",
    ")\n",
    "geocode_locations(unique_locations)\n",
    "\n",
    "# Load geocodes\n",
    "with shelve.open('geocode_cache.db') as cache:\n",
    "    sellers_df['coords'] = sellers_df['primary_location'].apply(\n",
    "        lambda x: cache.get(x, (None, None))\n",
    "    )\n",
    "    buyers_df['coords'] = buyers_df['primary_location'].apply(\n",
    "        lambda x: cache.get(x, (None, None))\n",
    "    )\n",
    "\n",
    "# Combine text fields\n",
    "def combine_text(row):\n",
    "    return ' '.join([\n",
    "        str(row.get('title', '')),\n",
    "        str(row.get('description', '')),\n",
    "        str(row.get('long_description', ''))\n",
    "    ])\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "def initialize_models():\n",
    "    \"\"\"Explicit model/tokenizer configuration\"\"\"\n",
    "    # Configure transformer with separate tokenizer args\n",
    "    bi_encoder= SentenceTransformer(\n",
    "    'T-Systems-onsite/german-roberta-sentence-transformer-v2',\n",
    "    device='cuda',  # Use GPU if available\n",
    "    tokenizer_kwargs={\n",
    "        'use_fast': True,\n",
    "        'model_max_length': 512,\n",
    "        'truncation': True\n",
    "    }\n",
    ")\n",
    "    # pooling = models.Pooling(transformer.get_word_embedding_dimension())\n",
    "    \n",
    "    # bi_encoder = SentenceTransformer(modules=[transformer, pooling])\n",
    "    cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "    return bi_encoder, cross_encoder\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text, axis=1)\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text, axis=1)\n",
    "\n",
    "# Clean text\n",
    "logging.info(\"Cleaning text...\")\n",
    "sellers_df['clean_text'] = sellers_df['combined_text'].apply(clean_german_text)\n",
    "buyers_df['clean_text'] = buyers_df['combined_text'].apply(clean_german_text)\n",
    "sellers_df['id'] = sellers_df.index\n",
    "# Initialize models\n",
    "logging.info(\"Initializing models...\")\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/german-roberta-sentence-transformer-v2')\n",
    "# cross_encoder = CrossEncoder('cross-encoder/stsb-roberta-large')\n",
    "bi_encoder, cross_encoder = initialize_models()\n",
    "# Encode sellers\n",
    "logging.info(\"Encoding sellers...\")\n",
    "seller_embeddings = bi_encoder.encode(\n",
    "    sellers_df['clean_text'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = seller_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(seller_embeddings)\n",
    "\n",
    "# Matching parameters\n",
    "SEMANTIC_THRESHOLD = 0.60\n",
    "CROSS_ENCODER_THRESHOLD = 0.65\n",
    "MAX_DISTANCE_KM = 50\n",
    "\n",
    "# Process buyers\n",
    "matches = []\n",
    "for buyer_idx, buyer_row in buyers_df.iterrows():\n",
    "    try:\n",
    "        # Semantic search\n",
    "        buyer_embedding = bi_encoder.encode([buyer_row['clean_text']])\n",
    "        distances, indices = index.search(buyer_embedding, 100)\n",
    "        # Filter candidates\n",
    "        candidates = []\n",
    "        for seller_idx, score in zip(indices[0], distances[0]):\n",
    "            if score >= SEMANTIC_THRESHOLD:\n",
    "                seller = sellers_df.iloc[seller_idx]\n",
    "                candidates.append((seller, score))\n",
    "        \n",
    "        # Location filtering\n",
    "        valid_matches = []\n",
    "        for seller, score in candidates:\n",
    "            # Hierarchical location match\n",
    "            location_match = any(\n",
    "                loc in buyer_row['location_parts']\n",
    "                for loc in seller['location_parts']\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Distance check\n",
    "            distance = None\n",
    "            if not location_match and None not in [buyer_row['coords'], seller['coords']]:\n",
    "                buyer_latitude, buyer_longitude= buyer_row['coords'].get('latitude', None), buyer_row['coords'].get('longitude', None)\n",
    "                seller_latitude, seller_longitude = seller['coords'].get('latitude', None), seller['coords'].get('longitude', None)\n",
    "                distance = geodesic((buyer_latitude, buyer_longitude),(seller_latitude, seller_longitude)).km\n",
    "                location_match = distance <= MAX_DISTANCE_KM\n",
    "\n",
    "            if location_match:\n",
    "                valid_matches.append({\n",
    "                    'buyer_id': buyer_row.get('id', ''),\n",
    "                    \n",
    "                    # 'buyer_date': buyer_row.get('date', ''),\n",
    "                    'buyer_title': buyer_row.get('title', ''),\n",
    "                    'buyer_description': buyer_row.get('description', ''),\n",
    "                    # 'buyer_long_description': buyer_row.get('long_description', ''),\n",
    "                    # 'buyer_location': buyer_row.get('location', ''),\n",
    "                    # 'buyer_nace_code': buyer_row.get('nace_code', ''), \n",
    "\n",
    "                    'seller_id': seller.get('id', ''),\n",
    "                    # 'seller_date': seller.get('date', ''),\n",
    "                    'seller_title': seller.get('title', ''),\n",
    "                    'seller_description': seller.get('description', ''),\n",
    "                    # 'seller_long_description': seller.get('long_description', ''),\n",
    "                    # 'seller_location': seller.get('location', ''),\n",
    "                    'seller_source': seller.get('url', ''),\n",
    "                    # 'seller_nace_code': seller.get('nace_code', ''),\n",
    "\n",
    "\n",
    "                    'semantic_score': score,\n",
    "                    'distance_km': round(distance, 2) if distance else 'Hierarchical match',\n",
    "                    'buyer_location': ' > '.join(buyer_row['location_parts']),\n",
    "                    'seller_location': ' > '.join(seller['location_parts'])\n",
    "                })\n",
    "\n",
    "        # Cross-encoder reranking\n",
    "        if valid_matches:\n",
    "            pairs = [(buyer_row['clean_text'], sellers_df.iloc[m['seller_id']]['clean_text']) \n",
    "                    for m in valid_matches]\n",
    "            cross_scores = cross_encoder.predict(pairs)\n",
    "            \n",
    "            for match, cross_score in zip(valid_matches, cross_scores):\n",
    "                if cross_score >= CROSS_ENCODER_THRESHOLD:\n",
    "                    match['confidence_score'] = cross_score\n",
    "                    matches.append(match)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing buyer {buyer_idx}:{buyer_row['coords']}: {e}\")\n",
    "\n",
    "# # Save results\n",
    "# if matches:\n",
    "#     matches_df = pd.DataFrame(matches)\n",
    "#     matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "#     matches_df.to_csv('./business_matches.csv', index=False)\n",
    "#     logging.info(f\"Saved {len(matches_df)} matches to business_matches.csv\")\n",
    "# else:\n",
    "#     logging.info(\"No matches found\")\n",
    "\n",
    "# logging.info(\"Creating matches DataFrame...\")\n",
    "# matches_df = pd.DataFrame(matches)\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "if not matches_df.empty:\n",
    "    matches_df = matches_df.sort_values('confidence_score', ascending=False)\n",
    "    \n",
    "    # Save All Matches\n",
    "    timestamp = datetime.now().strftime(\"%d_%H-%M\")\n",
    "    output_all = f'./matches/nlp_business_all_matches_{timestamp}.csv'\n",
    "    matches_df.to_csv(output_all, index=False)\n",
    "    matches_df.to_excel(f'./matches/nlp_business_all_matches_{timestamp}.xlsx', index=False)\n",
    "    logging.info(f\"Saved all matches: {len(matches_df)} records => {output_all}\")\n",
    "    \n",
    "    # Save High Confidence Matches\n",
    "    high_conf_df = matches_df[matches_df['confidence_score'] >= cross_encoder_threshold]\n",
    "    output_high_conf = f'./matches/nlp_business_high_conf_{timestamp}.csv'\n",
    "    high_conf_df.to_csv(output_high_conf, index=False)\n",
    "    logging.info(f\"Saved high confidence matches: {len(high_conf_df)} => {output_high_conf}\")\n",
    "else:\n",
    "    logging.info(\"No matches found.\")\n",
    "\n",
    "# -----------------------------------\n",
    "# 9. Memory Cleanup\n",
    "# -----------------------------------\n",
    "del seller_embeddings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:07:25,063 - INFO - Use pytorch device_name: cpu\n",
      "2025-02-03 01:07:25,064 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1dc800cb7e4f03b1cbea3dfad13e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c653e50b91d4a288fc40c3eb0978850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 01:11:38,563 - INFO - Save model to ./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 249.9783, 'train_samples_per_second': 1.456, 'train_steps_per_second': 0.096, 'train_loss': 572286407016448.0, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import logging\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace.csv')\n",
    "# Add negative examples (random pairs)\n",
    "\n",
    "# Combine text fields\n",
    "def combine_text(row):\n",
    "    return ' '.join([\n",
    "        str(row.get('title', '')),\n",
    "        str(row.get('description', '')),\n",
    "        str(row.get('long_description', ''))\n",
    "    ])\n",
    "buyer_text=buyers_df.apply(combine_text, axis=1).sample(69)\n",
    "seller_text=sellers_df.apply(combine_text, axis=1).sample(50)\n",
    "# Generate negative pairs\n",
    "negative_pairs = []\n",
    "for buyer, seller in zip(buyer_text, seller_text):\n",
    "    negative_pairs.append({\n",
    "        'buyer_text': buyer,\n",
    "        'seller_text': seller,\n",
    "        'similarity_score': np.random.uniform(0.1, 0.3)\n",
    "    })\n",
    "\n",
    "negative_pairs_df = pd.DataFrame(negative_pairs)\n",
    "\n",
    "# Combine known matches and negative pairs\n",
    "# negative_pairs['similarity_score'] = np.random.uniform(0.1, 0.3)\n",
    "# Generate synthetic training pairs\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load Data\n",
    "# Read the Excel file into a pandas dataframe\n",
    "known_matches = pd.read_excel(\"./data/nlp_business_all_matches_15_13-11-JN 2 -revised.xlsx\")\n",
    "\n",
    "# remove row with missing values in buyer_title or seller_title col \n",
    "known_matches = known_matches.dropna(subset=['buyer_title', 'seller_title'])\n",
    "# fill missing values or 'not in db' in similarity_score with 0.85 to 0.99 uniformly \n",
    "known_matches['similarity_score'] = known_matches['similarity_score'].apply(\n",
    "    lambda x: np.random.uniform(0.85, 0.99) if pd.isnull(x) or x == 'not in db' else x\n",
    ")\n",
    "# known_matches = pd.read_csv(\"./matches/nlp_business_all_matches_21_13-50.csv\")\n",
    "# known_matches = known_matches[['buyer_title', 'buyer_description', 'seller_title', 'seller_description', 'similarity_score']]\n",
    "known_matches['buyer_text'] = known_matches['buyer_title'] + ' ' + known_matches['buyer_description'] + ' ' + known_matches['buyer_long_description']\n",
    "known_matches['seller_text'] = known_matches['seller_title'] + ' ' + known_matches['seller_description'] + ' ' + known_matches['seller_long_description']\n",
    "\n",
    "combined_pairs = pd.concat([known_matches[['buyer_text', 'seller_text', 'similarity_score']], negative_pairs_df])\n",
    "\n",
    "# 2. Build Examples\n",
    "train_examples = []\n",
    "for idx, row in combined_pairs.iterrows():\n",
    "    train_examples.append(InputExample(texts=[row['buyer_text'], row['seller_text']], label=row['similarity_score']))\n",
    "\n",
    "# 3. Create Dataloader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "# fine_tuned_all-MiniLM-L12-v2\n",
    "# 4. Load Pretrained Model\n",
    "# model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# 5. Define Loss (CosineSimilarityLoss if label in [0..1])\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# 6. Train\n",
    "model_save_path = \"./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2\"\n",
    "# model_save_path = \"./fine_tuned_models/fine_tuned_T-Systems-onsite/cross-en-de-roberta-sentence-transformer\"\n",
    "# Split data into training and validation sets\n",
    "train_size = int(0.8 * len(train_examples))\n",
    "train_examples, val_examples = train_examples[:train_size], train_examples[train_size:]\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataloader = DataLoader(val_examples, shuffle=False, batch_size=16)\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(val_examples, name='val-eval')\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    # evaluator=evaluator,\n",
    "    epochs=2,\n",
    "    # evaluation_steps=100,\n",
    "    optimizer_params={'lr': 2e-5},\n",
    "    # warmup_steps=100,\n",
    "    output_path=model_save_path\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Use the fine-tuned model\n",
    "fine_tuned_model = SentenceTransformer(model_save_path)\n",
    "\n",
    "# Example inference\n",
    "text1 = \"Elektroinstallationsfirma oder ein Ingenieurbüro für Gebäudetechnik gesucht Matthias ist Sachverständiger für Gebäudetechnik & Brandschutz und sucht in Baden Württemberg oder Bayern eine Elektroinstallationsfirma oder ein Ingenieurbüro für Gebäudetechnik.\"\n",
    "text2 = \"Innovativer Elektrobetrieb im Albtal sucht Nachfolger #Pforzheim #Karlsruhe #Service #Wartung #Elektroinstallationen #Beleuchtungstechnik #gleitender Übergang\"\n",
    "embed1 = fine_tuned_model.encode(text1)\n",
    "embed2 = fine_tuned_model.encode(text2)\n",
    "\n",
    "cos_sim = np.dot(embed1, embed2) / (np.linalg.norm(embed1)*np.linalg.norm(embed2))\n",
    "print(\"Cosine Similarity:\", cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import json\n",
    "\n",
    "# Load the small spaCy model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text (remove domain-specific terms, URLs, emails, etc.)\n",
    "    text = re.sub(r'(geschäft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Token processing: lemmatization and POS filtering\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[:500]\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields for both buyers and sellers\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# Load fine-tuned bi-encoder and cross-encoder models\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('deepset/gbert-large-sts')\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# Build BM25 index for sellers\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# Set similarity and cross-encoder thresholds\n",
    "similarity_threshold = 0.80  # Adjusted similarity threshold for better precision\n",
    "cross_encoder_threshold = 0.9  # Adjusted cross-encoder threshold\n",
    "\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "matches = []\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "\n",
    "    # BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # Bi-encoder stage (calculate similarity scores)\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ONNX-optimized inference for cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # Location-aware scoring: calculate geodesic distance between buyer and seller locations\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km proximity\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "\n",
    "            if score >= 0.65 and location_match:  # Final matching criteria\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row['location'],\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row['location'],\n",
    "                    'similarity_score': score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# Save results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "matches_df.to_csv('./matches/nlp_business_high_conf_matches.csv', index=False)\n",
    "logging.info(f\"Matches saved: {len(matches_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "# Load the model (replace with correct path if needed)\n",
    "# Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('aari1995/German_Semantic_STS_V2')\n",
    "# model = AutoModel.from_pretrained('aari1995/German_Semantic_STS_V2')\n",
    "\n",
    "# matryoshka_dim = 1024 # How big your embeddings should be, choose from: 64, 128, 256, 512, 768, 1024\n",
    "# model = SentenceTransformer(\"aari1995/German_Semantic_V3\", trust_remote_code=True, truncate_dim=matryoshka_dim)\n",
    "\n",
    "model = SentenceTransformer(\"aari1995/German_Semantic_STS_V2\")\n",
    "# model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "sellers_combined_texts = sellers_df['combined_text'].tolist()\n",
    "sellers_embeddings = model.encode(sellers_combined_texts, convert_to_tensor=True)\n",
    "\n",
    "# # Tokenize sentences\n",
    "# encoded_input = tokenizer(sellers_combined_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# # Compute token embeddings\n",
    "# with torch.no_grad():\n",
    "#     model_output = model(**encoded_input)\n",
    "# sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "# Define sentences\n",
    "# buyers_combined_texts = buyers_df['combined_text'].tolist()\n",
    "\n",
    "# Compute embeddings for sellers\n",
    "# print(\"🚀 Encoded sellers' text.\", seller_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set similarity threshold\n",
    "similarity_threshold = 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over each buyer's combined text\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_title = buyer_row['title']\n",
    "    buyer_description = buyer_row['description']\n",
    "    logging.info(f\"Processing buyer {i+1}/{len(buyers_df)}: {buyer_title}\")\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "    buyer_locations = buyer_row['location']\n",
    "        # Tokenize sentences\n",
    "    # encoded_input = tokenizer(buyer_text, padding=True, truncation=True, return_tensors='pt')\n",
    "    buyer_embedding = model.encode(buyer_text, convert_to_tensor=True)\n",
    "\n",
    "    # Compute token embeddings\n",
    "    # with torch.no_grad():\n",
    "    #     model_output = model(**encoded_input)\n",
    "    # buyer_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    # Compute embedding for the current buyer's text\n",
    "    \n",
    "    # # Calculate cosine similarities\n",
    "    cosine_scores = util.cos_sim(buyer_embedding, sellers_embeddings)[0]\n",
    "    # Print cosine scores if greater than similarity threshold\n",
    "    for seller_idx, score in enumerate(cosine_scores):\n",
    "        if score >= similarity_threshold:\n",
    "            # Store the results in a DataFrame\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "            seller_locations = sellers_df.iloc[seller_idx]['location']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            # if match_locations(seller_locations, buyer_locations):\n",
    "            #     location_match = True\n",
    "            # else:\n",
    "            # Calculate distance between all combinations of seller and buyer coordinates\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km threshold\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "            \n",
    "                # print(f\"Cosine Score: {score:.4f} (Index: {seller_idx}) sellers_title: {sellers_df.iloc[seller_idx]['title']}, distance: {distance_km} km\")\n",
    "            if location_match:\n",
    "                results.append({\n",
    "                    'buyer_title': buyer_title,\n",
    "                    'buyer_description': buyer_description,\n",
    "                    'seller_title': sellers_df.iloc[seller_idx]['title'],\n",
    "                    'seller_description': sellers_df.iloc[seller_idx]['description'],\n",
    "                    'distance': distance_km,\n",
    "                    'seller_location': seller_locations,\n",
    "                    'buyer_location': buyer_locations,\n",
    "                    'buyer_combined_text': buyer_text,\n",
    "                    'seller_combined_text': sellers_combined_texts[seller_idx],\n",
    "                    'similarity_score': score.item()\n",
    "                })\n",
    "\n",
    "            # Convert results to DataFrame\n",
    "            results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Find indices where cosine scores are greater than the similarity threshold\n",
    "    # matching_indices = torch.where(cosine_scores >= similarity_threshold)[0].tolist()\n",
    "    # print(matching_indices)\n",
    "    # results = []\n",
    "    # for seller_idx in matching_indices:\n",
    "    #     seller_row = sellers_df.iloc[seller_idx]\n",
    "\n",
    "    # # # Store results in a DataFrame\n",
    "    #     if cosine_scores >= similarity_threshold:\n",
    "    #         logging.info(f\"Match found: Buyer text: {buyer_text}, Seller text: {seller_text}, Similarity: {score:.4f}\")\n",
    "\n",
    "    #         # results.append({\n",
    "    #         #     'buyer_text': buyer_text,\n",
    "    #         #     'buyer_title': buyer_title,\n",
    "    #         #     'buyer_description': buyer_description,\n",
    "    #         #     'seller_text': seller_text,\n",
    "    #         #     'seller_title': sellers_df.iloc[seller_idx]['title'],\n",
    "    #         #     'seller_description': sellers_df.iloc[seller_idx]['description'],\n",
    "    #         #     'similarity_score': score.item()\n",
    "    #         # })\n",
    "    # # for seller_idx, (seller_text, score) in enumerate(zip(sellers_combined_texts, cosine_scores)):\n",
    "    # #         logging.info(f\"Match found: Buyer text: {buyer_text}, Seller text: {seller_text}, Similarity: {score:.4f}\")\n",
    "            \n",
    "\n",
    "    # results_df = pd.DataFrame(results)\n",
    "# # Print results\n",
    "# for sentence, score in zip(sentences_to_compare, cosine_scores):\n",
    "#     print(f\"Sentence: {sentence}\")\n",
    "#     print(f\"Similarity Score: {score.item():.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('./matches/resultsdf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 🔑 1. Enhanced Preprocessing\n",
    "# --------------------------\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# nlp = spacy.load(\"de_core_news_lg\", disable=[\"ner\"])\n",
    "# nlp = spacy.load(\n",
    "#     \"de_core_news_sm\",\n",
    "#     disable=[\"ner\"],\n",
    "#     exclude=[\"vectors\"]  # Disable vector subsystem\n",
    "# )\n",
    "# 🔧 Switch to small model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text\n",
    "    text = re.sub(r'(geschäft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "def _extract_location_parts(location):\n",
    "    \"\"\"Extract and categorize location parts into states, districts, or cities.\"\"\"\n",
    "    locations = set()\n",
    "\n",
    "    if not location or not isinstance(location, str):\n",
    "        return locations\n",
    "\n",
    "    try:\n",
    "        # Split location string by \" > \", handling the hierarchical structure\n",
    "        parts = re.split(r'[\\n]\\s*', location)\n",
    "\n",
    "        for part in parts:\n",
    "            part = part.split(\">\")\n",
    "            locations.add(part[-1].strip())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting location parts: {e}\")\n",
    "\n",
    "    return list(locations)\n",
    "# Load updated datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(\n",
    "    lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1\n",
    ")\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x:  preprocess_text(x, nlp))\n",
    "\n",
    "# #Location processing\n",
    "# logging.info('Processing locations...')\n",
    "sellers_df['processed_location'] = sellers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "buyers_df['processed_location'] = buyers_df['location'].apply(lambda x: list(_extract_location_parts(x)) if pd.notnull(x) else [])\n",
    "\n",
    "# # Parse latitude and longitude from JSON strings\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "# Combine text fields\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# --------------------------\n",
    "# 🔑 2. Hybrid Retrieval Setup\n",
    "# --------------------------\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Precompute BM25 index\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# --------------------------\n",
    "# 🔑 3. Enhanced Model Loading\n",
    "# --------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 🔑 Load fine-tuned bi-encoder (pretrain on business texts)\n",
    "bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_all-MiniLM-L12-v2')\n",
    "\n",
    "# 🔑 Optimized cross-encoder with ONNX\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# --------------------------\n",
    "# 🔑 4. Optimized Matching Loop\n",
    "# --------------------------\n",
    "# 🔑 Dynamic threshold calculation (precompute from validation data)\n",
    "similarity_threshold = 0.58  # Calculated from validation set\n",
    "cross_encoder_threshold = 0.72\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    \n",
    "    # 🔑 Hybrid retrieval: BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # 🔑 Bi-encoder stage\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    \n",
    "    # 🔑 Batch cosine similarity calculation\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # 🔑 Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # 🔑 Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # 🔑 ONNX-optimized inference\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # 🔑 Location-aware scoring\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            \n",
    "            # 🔑 Combined score calculation\n",
    "            geo_score = calculate_geo_score(buyer_row, seller_row)  # Implement geo-scoring\n",
    "            final_score = 0.7 * score + 0.3 * geo_score\n",
    "            \n",
    "            if final_score >= 0.65:\n",
    "                # ... rest of match processing ...\n",
    "                print(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# --------------------------\n",
    "# 🔑 5. Post-processing\n",
    "# --------------------------\n",
    "def calculate_geo_score(buyer, seller):\n",
    "    \"\"\"Calculate normalized geographic compatibility score (0-1)\"\"\"\n",
    "    # Implement sophisticated location matching\n",
    "    return min(1.0, 1 / (1 + geodesic_distance_km))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import json\n",
    "\n",
    "# Load the small spaCy model without vectors\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Clean text (remove domain-specific terms, URLs, emails, etc.)\n",
    "    text = re.sub(r'(geschäft|dienstleistung|industrie)\\w*', '', text)\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+@\\S+|\\b\\d{10,}\\b', '', text)\n",
    "    text = re.sub(r'[^a-zA-ZäöüÄÖÜß\\s\\'\\-]', '', text)\n",
    "\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Token processing: lemmatization and POS filtering\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        elif token.pos_ in {'VERB', 'ADJ'} and token.text not in stop_words:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    return ' '.join([token for token in tokens if len(token) > 2])\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')[:500]\n",
    "\n",
    "# Preprocess buyers' text fields\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "# Preprocess sellers' text fields\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# Combine text fields for both buyers and sellers\n",
    "def combine_text_fields(row):\n",
    "    return ' '.join([\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        row.get('branchen_preprocessed', '')\n",
    "    ])\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# Load fine-tuned bi-encoder and cross-encoder models\n",
    "# bi_encoder = SentenceTransformer('./fine_tuned_models/fine_tuned_paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder = SentenceTransformer('deepset/gbert-large-sts')\n",
    "# bi_encoder = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "# Build BM25 index for sellers\n",
    "logging.info('Building BM25 index...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "tokenized_seller_texts = [text.split() for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "# Set similarity and cross-encoder thresholds\n",
    "similarity_threshold = 0.80  # Adjusted similarity threshold for better precision\n",
    "cross_encoder_threshold = 0.9  # Adjusted cross-encoder threshold\n",
    "\n",
    "logging.info(\"Encoding sellers' text...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Sellers' embeddings generated.\")\n",
    "\n",
    "matches = []\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "\n",
    "    # BM25 first stage\n",
    "    buyer_tokens = buyer_text.split()\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-500:][::-1]  # Top 500 BM25 matches\n",
    "    \n",
    "    # Bi-encoder stage (calculate similarity scores)\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    matching_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    \n",
    "    if len(matching_indices) == 0:\n",
    "        continue\n",
    "\n",
    "    # Adaptive top_n based on score distribution\n",
    "    score_std = np.std(sim_scores[matching_indices])\n",
    "    adaptive_top_n = min(100, int(len(matching_indices) * (1 - score_std)))\n",
    "    top_indices = matching_indices[np.argsort(sim_scores[matching_indices])[::-1][:adaptive_top_n]]\n",
    "    \n",
    "    # Batch cross-encoder prediction\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # ONNX-optimized inference for cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = np.atleast_1d(torch.sigmoid(outputs.logits).squeeze().detach().numpy())\n",
    "\n",
    "    # Location-aware scoring: calculate geodesic distance between buyer and seller locations\n",
    "    for idx, score in zip(top_indices, cross_scores):\n",
    "        if score >= cross_encoder_threshold:\n",
    "            seller_idx = bm25_candidates[idx]\n",
    "            seller_row = sellers_df.iloc[seller_idx]\n",
    "            seller_latitudes = sellers_df.iloc[seller_idx]['latitude']\n",
    "            seller_longitudes = sellers_df.iloc[seller_idx]['longitude']\n",
    "\n",
    "            location_match = False\n",
    "            distance_km = None\n",
    "\n",
    "            for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "                for p_lat, p_lon in zip(buyer_latitudes, buyer_longitudes):\n",
    "                    if None in [s_lat, s_lon, p_lat, p_lon]:\n",
    "                        continue\n",
    "                    distance = geodesic((s_lat, s_lon), (p_lat, p_lon)).km\n",
    "                    if distance <= 50:  # 50 km proximity\n",
    "                        distance_km = distance\n",
    "                        location_match = True\n",
    "                        break\n",
    "                if location_match:\n",
    "                    break\n",
    "\n",
    "            if score >= 0.65 and location_match:  # Final matching criteria\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row['location'],\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row['location'],\n",
    "                    'similarity_score': score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer {buyer_row['title']} with Seller {seller_row['title']}\")\n",
    "\n",
    "# Save results\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "matches_df.to_csv('./matches/nlp_business_high_conf_matches.csv', index=False)\n",
    "logging.info(f\"Matches saved: {len(matches_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = buyers_df['combined_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "from geopy.distance import geodesic\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# -------------------------------\n",
    "# Logging & spaCy Model Setup\n",
    "# -------------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Synonyms Setup\n",
    "# -------------------------------\n",
    "SYNONYM_GROUPS = [\n",
    "    # 1. Company / Enterprise / Business Entities\n",
    "    {\"unternehmen\", \"firma\", \"betrieb\", \"geschäft\", \"organisation\", \"konzern\", \"gesellschaft\", \"unternehmung\", \"institution\"},\n",
    "    # 2. Acquisition / Succession\n",
    "    {\"übernahme\", \"nachfolge\", \"akquisition\", \"übertragung\", \"nachfolgeregelung\", \"übergabe\"},\n",
    "    # 3. Building Technology & Engineering\n",
    "    {\"gebäudetechnik\", \"installation\", \"elektroinstallation\", \"ingenieurbüro\", \"bau\", \"architektur\", \"technik\", \"techniker\"},\n",
    "    # 4. Dental & Zahntechnik\n",
    "    {\"zahntechnik\", \"zahntechniker\", \"dentallabor\", \"dentallabore\", \"zahnlabor\", \"zahntechnikerin\"},\n",
    "    # 5. Health, Fitness & Physiotherapy\n",
    "    {\"physiopraxis\", \"physio\", \"fitness-studio\", \"fitnessstudio\", \"praxis\", \"gesundheit\", \"physiotherapie\"},\n",
    "    # 6. Machinery / Machine Construction\n",
    "    {\"maschinenbau\", \"maschinenkonstrukteur\", \"maschinenbauunternehmen\", \"maschinenbaufirma\", \"maschinenkonstruktion\"},\n",
    "    # 7. Gastronomy / Hospitality\n",
    "    {\"gastronomie\", \"caf\", \"café\", \"gastgewerbe\", \"restaurant\", \"gasthof\", \"pachtübernahme\"},\n",
    "    # 8. Consulting & Advisory\n",
    "    {\"beratung\", \"consulting\", \"unternehmensberatung\", \"managementberatung\", \"beratungsgespräch\", \"berater\"},\n",
    "    # 9. Electrotechnology\n",
    "    {\"elektrotechnik\", \"elektroinstallation\", \"elektroinstallationsfirma\", \"elektrotechniker\", \"bauinstallation\", \"elektro\"},\n",
    "    # 10. Stone Processing / Gravure\n",
    "    {\"sandstrahl\", \"gravure\", \"stein\", \"steingravur\", \"steinbearbeitung\", \"steinverarbeitung\"},\n",
    "    # 11. Creative / Design / Handicraft\n",
    "    {\"start-up\", \"design\", \"kunst\", \"handwerk\", \"kreativ\", \"unikate\", \"patent\", \"designschutz\", \"kunsthandwerk\"},\n",
    "    # 12. IT / Software / Digital\n",
    "    {\"it\", \"informationstechnologie\", \"it-dienst\", \"it-service\", \"edv\", \"software\", \"programm\", \"applikation\", \"app\", \"softwarelösung\"},\n",
    "    # 13. Production / Manufacturing\n",
    "    {\"produktion\", \"fertigung\", \"herstellung\", \"produktionsverfahren\", \"fertigungsprozess\"},\n",
    "    # 14. Logistics / Distribution\n",
    "    {\"logistik\", \"transport\", \"distribution\", \"versand\", \"lagerhaltung\", \"lieferkette\"},\n",
    "    # 15. Trade / Sales / Commerce\n",
    "    {\"handel\", \"verkauf\", \"einzelhandel\", \"großhandel\", \"grosshandel\", \"vertrieb\", \"commerce\"},\n",
    "    # 16. Finance\n",
    "    {\"finanzen\", \"geld\", \"kapital\", \"bankwesen\", \"finanzdienstleistung\", \"investment\", \"beteiligung\", \"vermögen\", \"finanzierung\"},\n",
    "    # 17. Marketing & Communication\n",
    "    {\"marketing\", \"werbung\", \"promotion\", \"vertriebsförderung\", \"marktforschung\", \"public relations\", \"kommunikation\", \"marketingstrategie\"},\n",
    "    # 18. Human Resources / Personnel\n",
    "    {\"personal\", \"hr\", \"human resources\", \"mitarbeiter\", \"rekrutierung\", \"personaldienstleistung\", \"personalvermittlung\", \"arbeitskräfte\"},\n",
    "    # 19. Research, Development & Innovation\n",
    "    {\"entwicklung\", \"forschung\", \"innovation\", \"forschungs- und entwicklungsabteilung\", \"innovationsmanagement\"},\n",
    "    # 20. Legal / Regulatory\n",
    "    {\"recht\", \"jurisprudenz\", \"gesetz\", \"legal\", \"rechtsschutz\", \"anwaltskanzlei\", \"rechtsberatung\", \"jurist\", \"gesetzgebung\"},\n",
    "    # 21. Real Estate & Construction\n",
    "    {\"immobilien\", \"grundstücke\", \"liegenschaften\", \"immobilienverwaltung\", \"bauprojekt\", \"immobilienentwicklung\"},\n",
    "    # 22. Energy / Utilities\n",
    "    {\"energie\", \"strom\", \"elektrizität\", \"gas\", \"erneuerbare energie\", \"energieversorgung\", \"energiewirtschaft\"},\n",
    "    # 23. E-Commerce / Online Trade\n",
    "    {\"e-commerce\", \"onlinehandel\", \"internetverkauf\", \"digitaler verkauf\", \"onlineshop\", \"webshop\"},\n",
    "    # 24. Digital Transformation\n",
    "    {\"digitalisierung\", \"digital\", \"digitaler wandel\", \"it-transformation\", \"digital transformierung\", \"digitalisierungslösungen\", \"digitale transformation\"},\n",
    "    # 25. Management & Leadership\n",
    "    {\"management\", \"führung\", \"leitung\", \"geschäftsführung\", \"managementberatung\", \"organisationsentwicklung\", \"betriebsführung\"}\n",
    "]\n",
    "\n",
    "def expand_with_synonyms(keywords):\n",
    "    \"\"\"Expand the keyword set using the synonym groups.\"\"\"\n",
    "    expanded = set(keywords)\n",
    "    for group in SYNONYM_GROUPS:\n",
    "        if group.intersection(keywords):\n",
    "            expanded.update(group)\n",
    "    return expanded\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Text Preprocessing and Keyword Extraction\n",
    "# -------------------------------\n",
    "def extract_keywords(text, nlp_model):\n",
    "    \"\"\"\n",
    "    Extract keywords from text using spaCy. We keep nouns, proper nouns, verbs, and adjectives.\n",
    "    Then, we expand the keyword set using our synonym groups.\n",
    "    \"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return set()\n",
    "    doc = nlp_model(text)\n",
    "    keywords = set()\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if lemma not in stop_words and len(lemma) > 2:\n",
    "                keywords.add(lemma)\n",
    "    return expand_with_synonyms(keywords)\n",
    "\n",
    "def combine_text_fields(row, fields):\n",
    "    \"\"\"Combine multiple fields from a row into one string.\"\"\"\n",
    "    return \" \".join([str(row[field]) for field in fields if pd.notnull(row[field])])\n",
    "\n",
    "# -------------------------------\n",
    "# 2.1. Industry Tag Extraction\n",
    "# -------------------------------\n",
    "# Define an industry dictionary with indicative keywords.\n",
    "INDUSTRY_KEYWORDS = {\n",
    "    \"SHK\": {\"shk\", \"heizung\", \"sanitär\", \"elektro\", \"installateur\", \"elektrotechnik\", \"heizungsbaumeister\"},\n",
    "    \"Friseur\": {\"friseur\", \"friseursalon\", \"haarschnitt\", \"kosmetik\"},\n",
    "    \"Bäckerei\": {\"bäckerei\", \"brot\", \"backwaren\"},\n",
    "    \"Maler\": {\"maler\", \"malerbetrieb\", \"anstrich\"},\n",
    "    \"Schlosserei\": {\"schlosser\", \"schlosserei\"},\n",
    "    \"Dentallabor\": {\"zahntechnik\", \"dentallabor\", \"zahnlabor\"},\n",
    "    \"Physio\": {\"physiopraxis\", \"physio\", \"fitness\", \"fitnessstudio\", \"physiotherapie\"},\n",
    "    \"Maschinenbau\": {\"maschinenbau\", \"maschinenkonstrukteur\", \"maschinen\"},\n",
    "    \"Gastronomie\": {\"gastronomie\", \"caf\", \"café\", \"restaurant\", \"gastgewerbe\"},\n",
    "    \"Elektro\": {\"elektroinstallation\", \"elektrotechnik\", \"elektroinstallationsfirma\"},\n",
    "    \"Bau\": {\"bau\", \"architektur\", \"bauunternehmen\"},\n",
    "    \"Tankstelle\": {\"tankstelle\", \"waschstraße\", \"kfzbetrieb\"},\n",
    "    \"KFZ\": {\"werkstatt\", \"kfz\", \"automobil\", \"auto\"}\n",
    "}\n",
    "\n",
    "def extract_industry_tags(text):\n",
    "    \"\"\"\n",
    "    Extract industry tags by scanning the text for any of the indicative keywords.\n",
    "    Returns a set of matching industry tags.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    tags = set()\n",
    "    for tag, kws in INDUSTRY_KEYWORDS.items():\n",
    "        for kw in kws:\n",
    "            if kw in text:\n",
    "                tags.add(tag)\n",
    "                break\n",
    "    return tags\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load and Preprocess the Datasets\n",
    "# -------------------------------\n",
    "logging.info(\"Loading datasets...\")\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "buyer_fields = [\"title\", \"description\", \"long_description\", \"Industrie\", \"Sub-Industrie\"]\n",
    "seller_fields = [\"title\", \"description\", \"long_description\", \"branchen\"]\n",
    "\n",
    "logging.info(\"Processing buyer data...\")\n",
    "buyers_df[\"combined_text\"] = buyers_df.apply(lambda row: combine_text_fields(row, buyer_fields), axis=1)\n",
    "buyers_df[\"keywords\"] = buyers_df[\"combined_text\"].apply(lambda text: extract_keywords(text, nlp))\n",
    "buyers_df[\"industry_tags\"] = buyers_df[\"combined_text\"].apply(lambda text: extract_industry_tags(text))\n",
    "\n",
    "logging.info(\"Processing seller data...\")\n",
    "sellers_df[\"combined_text\"] = sellers_df.apply(lambda row: combine_text_fields(row, seller_fields), axis=1)\n",
    "sellers_df[\"keywords\"] = sellers_df[\"combined_text\"].apply(lambda text: extract_keywords(text, nlp))\n",
    "sellers_df[\"industry_tags\"] = sellers_df[\"combined_text\"].apply(lambda text: extract_industry_tags(text))\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Parse Geographic Coordinates\n",
    "# -------------------------------\n",
    "def parse_coordinates(coord_str):\n",
    "    try:\n",
    "        return json.loads(coord_str)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "buyers_df[\"latitude\"] = buyers_df[\"latitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df[\"longitude\"] = buyers_df[\"longitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df[\"latitude\"] = sellers_df[\"latitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df[\"longitude\"] = sellers_df[\"longitude\"].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Similarity and Geographic Functions\n",
    "# -------------------------------\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Compute Jaccard similarity between two sets.\"\"\"\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union\n",
    "\n",
    "def within_distance(buyer_latitudes, buyer_longitudes, seller_latitudes, seller_longitudes, max_distance_km=50):\n",
    "    \"\"\"\n",
    "    Return (True, distance) if any buyer-seller coordinate pair is within max_distance_km.\n",
    "    If no coordinate information is available, assume a geographic match.\n",
    "    \"\"\"\n",
    "    if not buyer_latitudes or not buyer_longitudes or not seller_latitudes or not seller_longitudes:\n",
    "        return True, None\n",
    "    buyer_coord = (np.mean(buyer_latitudes), np.mean(buyer_longitudes))\n",
    "    for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "        if s_lat is None or s_lon is None:\n",
    "            continue\n",
    "        distance = geodesic(buyer_coord, (s_lat, s_lon)).km\n",
    "        if distance <= max_distance_km:\n",
    "            return True, distance\n",
    "    return False, None\n",
    "\n",
    "# -------------------------------\n",
    "# 5.1. Additional Domain‐Specific Helper Functions\n",
    "# -------------------------------\n",
    "def domain_match_required(buyer_title, seller_industries):\n",
    "    \"\"\"\n",
    "    If the buyer title explicitly mentions a specific domain keyword,\n",
    "    require that the seller's industry tags include a matching domain.\n",
    "    \"\"\"\n",
    "    buyer_title_lower = buyer_title.lower()\n",
    "    seller_ind_lower = {tag.lower() for tag in seller_industries}\n",
    "    # For example, if buyer mentions \"shk\", require seller to have \"shk\"\n",
    "    if \"shk\" in buyer_title_lower and \"shk\" not in seller_ind_lower:\n",
    "        return False\n",
    "    if \"friseur\" in buyer_title_lower and \"friseur\" not in seller_ind_lower:\n",
    "        return False\n",
    "    if \"maler\" in buyer_title_lower and \"maler\" not in seller_ind_lower:\n",
    "        return False\n",
    "    if (\"dentallabor\" in buyer_title_lower or \"zahntechniker\" in buyer_title_lower) and \\\n",
    "       not ((\"dentallabor\" in seller_ind_lower) or (\"zahnlabor\" in seller_ind_lower)):\n",
    "        return False\n",
    "    # Additional rules can be added here\n",
    "    return True\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Improved Rule-Based Matching with Refined Industry Filtering\n",
    "# -------------------------------\n",
    "# Base similarity threshold and bonus constants.\n",
    "keyword_similarity_threshold = 0.3\n",
    "INDUSTRY_BONUS = 0.15  # full bonus if specific match; lower if only generic \"Bau\" overlaps\n",
    "\n",
    "logging.info(\"Starting improved rule-based matching with refined industry filtering...\")\n",
    "matches = []\n",
    "\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_keywords = buyer_row[\"keywords\"]\n",
    "    buyer_industries = buyer_row[\"industry_tags\"]\n",
    "    buyer_title = buyer_row[\"title\"]\n",
    "    buyer_latitudes = buyer_row[\"latitude\"]\n",
    "    buyer_longitudes = buyer_row[\"longitude\"]\n",
    "    \n",
    "    for j, seller_row in sellers_df.iterrows():\n",
    "        seller_keywords = seller_row[\"keywords\"]\n",
    "        seller_industries = seller_row[\"industry_tags\"]\n",
    "        seller_title = seller_row[\"title\"]\n",
    "        \n",
    "        # Require at least one common industry tag.\n",
    "        common_industries = buyer_industries.intersection(seller_industries)\n",
    "        if not common_industries:\n",
    "            continue\n",
    "        \n",
    "        # If the only shared tag is the generic \"Bau\", reduce the bonus.\n",
    "        if common_industries == {\"Bau\"}:\n",
    "            industry_bonus = 0.05  # lower bonus for generic match\n",
    "        else:\n",
    "            industry_bonus = INDUSTRY_BONUS\n",
    "        \n",
    "        # If buyer title explicitly requires a specific domain, enforce it.\n",
    "        # if not domain_match_required(buyer_title, seller_industries):\n",
    "        #     continue\n",
    "        \n",
    "        sim = jaccard_similarity(buyer_keywords, seller_keywords)\n",
    "        sim += industry_bonus\n",
    "        \n",
    "        if sim >= keyword_similarity_threshold:\n",
    "            location_match, distance_km = within_distance(\n",
    "                buyer_latitudes, buyer_longitudes,\n",
    "                seller_row[\"latitude\"], seller_row[\"longitude\"],\n",
    "                max_distance_km=50\n",
    "            )\n",
    "            if location_match:\n",
    "                matches.append({\n",
    "                    \"buyer_id\": buyer_row.get(\"id\", i),\n",
    "                    \"seller_id\": seller_row.get(\"id\", j),\n",
    "                    \"buyer_title\": buyer_title,\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'distance_km': distance_km,\n",
    "                    \"seller_title\": seller_title,\n",
    "                    \"keyword_similarity\": sim,\n",
    "                    \"industry_buyer\": list(buyer_industries),\n",
    "                    \"industry_seller\": list(seller_industries),\n",
    "                    \"distance_km\": distance_km\n",
    "                })\n",
    "                logging.info(f\"Match: Buyer '{buyer_title}' & Seller '{seller_title}' | Sim: {sim:.2f} | Industries: {buyer_industries} vs. {seller_industries}\")\n",
    "\n",
    "matches_df = pd.DataFrame(matches)\n",
    "matches_df = matches_df.sort_values(\"keyword_similarity\", ascending=False)\n",
    "output_path = \"./matches/rule_based_matches_with_synonyms_and_industries.csv\"\n",
    "matches_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"Improved rule-based matching completed. Total matches found: {len(matches_df)}\")\n",
    "logging.info(f\"Matches saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Elektroinstallationsfirma oder ein Ingenieurbüro für Gebäudetechnik gesucht Matthias ist Sachverständiger für Gebäudetechnik & Brandschutz und sucht in Baden Württemberg oder Bayern eine Elektroinstallationsfirma oder ein Ingenieurbüro für Gebäudetechnik. Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=207804\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=212714\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=211934\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210882\\n\\nMatthias Datensatz gibt es 2x!\\nIch bin Sachverständiger für Gebäudetechnik und Vorbeugender Brandschutz, SiGe, Sicherheitsbeauftragter, Gefahrgutbeauftragter, SiFa und Suche ein geeignetes Unternehmen in der Region Baden - Württemberg und Bayern zu übernehmen. Ingenieurbüro oder Elektroinstallationsfirma. Beteiligung oder Übernahme. Bis 1 Mio € Ingenieurdienstleistungen Elektroinstallation, Elektrofirma, Gebäudetechnik, Brandschutztechnik',\n",
       " 'Heizung Sanitärbetrieb im Raum Celle - Hannover gesucht Felix ist Meister im Bereih Heizung Sanitär und sucht einen betrieb zur Übernahme zwischen Celle und Hannover Ich Felix Heinze habe einen Meistertitel im Bereich Heizung und Sanitär. Und suche einen Betrieb zur Übernahme. In der Region Celle und Hannover. 5-6 Angestellte. Ca. 100T Handwerk Heizung, Sanitär, Klima (SHK)',\n",
       " 'Physiotherapeut sucht Praxis in Nürnberg Daniel ist seit 2014 Physiotherapeut sowie Heilpraktiker für Physiotherapie und sucht eine Physiotherapie-Praxis im Raum Nürnberg zur Übernahme. - Physiotherapeut seit 2014\\n- Bachelorabschluss in Gesundheitsmanagement und Gesundheitsförderung\\n- Heilpraktiker für Physiotherapie\\n- 1 Jahr Fachliche Leitung einer Physiopraxis\\n- 4 Jahre Erfahrung als Freiberufler ( Nebentätigkeit ) Gesundheitswesen Physiotherapie',\n",
       " 'Jungunternehmer sucht Physio-Praxis o. Fitness-Studio in Düsseldorf o. Ratingen Sam ist Jungunternehmer und sucht Physiotherapie-Praxen + Fitnesstudios zur Übernahme 02.12.24: Mail, ob Suche noch aktuell ist\\n19.05.24: Anfrage: Er schrieb: Ich schaue generell nach Standorten zur Übernahme indem sich eine Praxis+ Fitnessstudio kombinieren lassen. Deshalb sind kleinere Immobilien <400qm wahrscheinlich wenig interessant aber generell bin ich erstmal für alles offen.  Gesundheitswesen Physiotherapie, Fitnessstudios',\n",
       " 'Schreinermeister sucht Tischlerei zur Übernahme Peter ist Schreinermeister und sucht einen Betrieb zur Übernahme in Süd-Hessen bis Nord-Baden-Württemberg mit vorwiegender Tätigkeit und Kundengruppe im individuellen Möbelbau nach Maß Ursprünglich wg. Tischlerei Esslingen angeschrieben. Inserat .\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=208447\\n\\nSchreinerei mit bestehenden Mitarbeiterstamm und guter Auftragslage erwünscht.\\nPrivatkunden, gewerbliche Kunden im hochwertigen Möbel- und Innenausbau. \\nKommt aus Siedelsbrunn (35 km nordöstlich von Mannheim)\\nSitzt nördlich von Mannheim / Heidelberg Handwerk Schreinerei, Möbelbau',\n",
       " 'Geschäftsführer sucht Transportfirma zum Kauf Andreas ist Inhaber einer Spedition und möchte seinen Betrieb erweitern. Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208011\\n\\nGeschäftsführer einer Spedition aus NRW. Sucht passende Spedition zur Übernahme. War ursprünglich Interessent einer Spedition in Bodenwerder Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'War ursprünglich Kauf-Interessent einer Spedition in Bodenwerder. Schrieb:\\nHallo habe Interesse an ihrer Anzeige brauche mehr Details zur Spedition Unbekannt Unbekannt',\n",
       " 'MBI-Kandidat sucht Logistik-Firma Jan ist Betriebswirt und sucht eine Spedition / Transportunternehmen im Raum Niedersachsen zur Unternehmensnachfolge. War ursprünglich Kauf-Interessent einer Spedition in Bodenwerder. Er schrieb:\\nich interessiere mich für eine Unternehmensnachfolge in der Transport-/ Speditionsbranche im Raum Niedersachsen.\\nKurz zu meiner Person: ich bin Anfang 30, Betriebswirt und klassischer MBI-Kandidat.\\nEine vorherige Anstellung im Unternehmen, für eine möglichst glatte Übergabe, wäre für mich denkbar. Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Führungskraft möchte Logistik-Firma übernehmen Lasse hat 20 Jahre Erfahrung im Straßen-Transport / Speditionsgewerbe und sucht eine passende Firma zum Kauf in Nord-Deutschland 02.12.24: für Lasse dieses Inserat angecshrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208188\\n\\n20 Jahre Straßen-Transport / Speditionsgewerbe in Führungsverantwortung tätig. beschäftigt sich schon länger mit Nachfolge/Selbstständigkeit.\\nVerkehrsfachwirt\\nVerkehrsleiter\\nSilo / Kühler / Plane. War ursprünglich Kauf-Interessent einer Spedition in Bodenwerder Transport und Logistik Straßentransport, Spedition',\n",
       " 'Logistik-Berater sucht Firma zur Übernahme Fabian sucht Spedition in: Südliches Niedersachsen oder Nord-Hessen War Interessent der Spedition in Bodenwerder / Niedersachsen\\nscheint Berater für Logistikunternehmen zu sein. war Interessent der Spedtition Schäfer in Bodenwerder, schrieb: \"Guten Abend, ich bin seit mehr als 20 Jahren im Logistikbereich tätig, verfüge über Kenntnisse in der Unternehmensführung, Digitalisierung etc. und würde gerne mehr über das Unternehmen und die Rahmenbedingungen erfahren. Viele Grüße Fabian Rogalla Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Logistik-Geschäftsführer sucht Transportunternehmen Jürgen ist Speditions- / Logistik-Geschäftsführer und sucht eine gesunde Spedition zur Übernahme Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208188\\n\\nGuten Tag,\\n\\nals Speditions/Logistik-Geschäftsführer mit breiter Erfahrung, Fachkundenachweis und Verkehrsleiter mit mehrjähriger Erfahrung und akademischem Background besitze ich übergreifende Fach- und Führungskompetenz in allen Bereichen der ergebnisorientierten Unternehmensführung und Entwicklung eines Logistik-/ Serviceunternehmens mit verschiedenen Standorten und eigenem Fuhrpark. Meine Persönlichkeit und Führung ist mittelstands-/ergebnisorientiert, pragmatisch, authentisch, empathisch, werteorientiert, hands- on, stark integrationsfähig und in höchstem Maße von Loyalität/Vertrauen geprägt. Kernaufgabe war immer Aufbau und Steuerung sämtlicher Strukturen, Entwicklung und Steuerung mittels KPI´s und die Unternehmensentwicklung.\\n\\nIn meiner mehrjährigen beruflichen Tätigkeit als Geschäftsführer -nach einer von mir erfolgreich durchgeführten Restrukturierung- eines erfolgreichen, international tätigen mittelständischen Logistikers mit kundenspezifischen Dienstleistungen und breiter Wertschöpfungskette, habe ich unternehmerisches Denken und Durchsetzungsstärke bewiesen. Meine Arbeitsweise ist geprägt durch eine deutliche marktorientierte Ausrichtung, fachbereichsübergreifendes, prozessorientiertes, durchsetzungsstarkes Handeln mit starker Nähe zum operativen Geschäft und kontinuierlichen Produktivitätsuntersuchungen, Geschäftsprozessoptimierung, Innovations-, Qualitäts-/Risikoorientierung und eine der strategischen Ausrichtung folgenden Personalentwicklung.\\n\\nEin Start kann nach Vereinbarung erfolgen. Den erforderlichen Umzug in die Nähe des Firmensitzes würde ich kurzfristig durchführen.\\n\\nFür ein persönliches Gespräch stehe ich Ihnen jederzeit sehr gerne zur Verfügung. Meinen CV Zeugnisse stelle ich Ihnen selbstverständlich bei Bedarf zur Verfügung. Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Disponent möchte sich selbstständig machen Andre mit Erfahrung als Disponent & Verkehrsleiter sucht Schüttgut-Transportfirma Selbst bin ich langjährig als Disponent und Verkehrsleiter tätig im Raum 26/27/28/49\\nSucht Spedition, die sich mit Schüttguttransporte wie Baustoffe oder Agrarprodukte mit z.B. Kipper oder Schubboden oder mit Schwertransporte mit z.B. Tieflader beschäftigt. Oder auch Agrar- und Baustoffhandel, Sand- und Kiesgewinnung- und handel, Tiefbau, Abbruch, Landwirtschaftliche Dienstleistungen Transport und Logistik Schüttguttransporte, Schwertransporte',\n",
       " 'Geschäftsführer sucht Dienstleistungsunternehmen Christian ist Geschäftsführer im Logistik-Bereich und sucht Spedition zum Kauf in Berlin, Leipzig, Dresden oder Brandenburg. Gesucht wird ein etabliertes und profitables Dienstleistungs- oder Handwerksunternehmen, welches mindestens seit 10 Jahren erfolgreich existiert, optimaler Weise in der Rechtsform einer GmbH. Hat Erfahrung im Bereich Logistik, Spedition.\\nSucht 1-5 Mio Unternehmenswert\\n\\nAusschlusskriterien: Firmen die Qualifikationen benötigen, die nicht durch das eingesetzte Personal (bspw. Meister) abgedeckt werden können. Sucht eher Investment und möchte dann betriebswirtschftlich beraten.\\n\\nDer Käufer ist ein erfahrener Manager (über 12 Jahre Erfahrung als Geschäftsführer, 57 Jahre alt) in der Dienstleistungsbranche (Logistik) mit akademischer Qualifikation (Dipl.-Kfm.). Idee ist, meine Expertise im kaufmännischen Bereich und im Bereich Unternehmensführung einzubringen, die operative Führung sollte aber durch die zweite Ebene abgedeckt sein. Transport und Logistik Logistikdienstleistungen, Spedition',\n",
       " 'Logistiker sucht Spedition oder Kontrakt-Logistik Michael ist Führungskraft und möchte Unternehmen im Logistik-Bereich übernehmen Als erfahrener Logistiker mit langjähriger Erfahrung unter anderem im C-Level Bereich bin ich auf der Suche nach einem Unternehmen im Logistikumfeld. Dies kann eine Spedition sein, aber auch im Bereich Kontraktlogistik oder Multi-Customer-Warehouses. Transport und Logistik Spedition, Kontraktlogistik, Lagerhaltung',\n",
       " 'Geschäftsführer sucht Spedition zur Übernahme Eduard ist Inhaber einer Spedition und möchte ein zweites Unternehmen übernehmen, um Synergien zu schaffen. War Kauf-Interessent einer Spedition in Nord-Hessen. Etwas Problem-fokussiert, aber freundlicher Typ.\\n\\nSehr geehrte Damen und Herren,\\nich bin durch Ihre Verkaufsanzeige auf die geplante Transaktion aufmerksam geworden.\\n\\nGerne würde ich mehr darüber erfahren. Unterstützung erhalte ich von einem externen Unternehmensberater welcher auch auf Finanzierungen spezialisiert ist.\\n\\nMeinerseits bringe ich unternehmerisches Denken und Handeln sowie Kompetenz und Eloquenz in der Kundenbetreuung mit. Zudem verfüge ich über ein breites, europaweites Netzwerk hinsichtlich Produzenten jeglicher Erzeugnisse, was sich sicherlich positiv auf die Auftragslage auswirken könnte. Durch diese Vernetzung befinden sind bereits einige Kunden in der Pipeline. Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Investor kauft Speditionen auf Geschäftsführer mehrere Logistik-Unternehmen kauft günstige Transportfirmen. War Kauf-Interessent einer Spedition in Nord-Hessen. Ernsthaftes Interesse war fraglich. Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Inahber von 2 Speditionen sucht weitere Firmen zum Kauf Florian ist Inhaber von 2 Speditionen und sucht Geschäftsfelderweiterung im Transportsektor War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\n \"mit großen Interesse habe ich Ihr Inserat gelesen, da ich immer wieder nach Erweiterungsmöglichkeiten meiner bisherigen Tätigkeiten ausschau halte.\", Inhaber und GF zweier Speditionen (https://tide-spedition.de/\\nhttps://ja-spedition.de/\\nInhaber von BBQ Imbiss, und einer Consulting Firma. (?) Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Umzugsunternehmer sucht Möbelspedition Volkan ist Inhaber eines Umzugsunternehmens und sucht Möbelspedition zum Kauf War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nMeine Frau und meine Schwieger Eltern haben ein Umzugsunternehmen Möbeltransport / Möbelspedition und Lagerung in erster Linie wollen wir mit meiner Frau zusammen das Unternehmen ein bisschen vergrößern und eventuell eine neue Sparte ins unternehmen reinholen meine Vorstellung wäre natürlich das Unternehmen selbst zu leiten und wenn der Preis stimmt natürlich auch die Immobilie mit zu übernehmen.  Transport und Logistik Umzugsdienste, Möbeltransporte',\n",
       " 'Familienunternehmen sucht Geschäftsfelderweiterung im Schwerlastverkehr Dominik ist Inhaber einer Transportfirma und sucht Spedition zum Kauf im Bereich Schwerlastverkehr. War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nSehr geehrte Damen und Herren, \\nda wir ein kleines Familienunternehmen seit 42 Jahre aufgebaut haben, suchen wir nun den Einstieg in den Schwerlastverkehr. Auf Grund unserer Familientradition, kann ich mir gut vorstellen einen anderen Familienbetrieb zu übernehmen um diesen im Sinne \"Familie\" weiter fortzuführen. Der Einstieg ist wenn, geplant als Geschäftsführung inkl. Betriebsleiter vor Ort. Da wir ein Unternehmen in der Nähe von Nürnberg haben, wäre dies ein zweites Standbein für uns. Ein Eigenkapital von 10-20% kann durch Privateinlage gestemmt werden, natürlich ist dies Abhängig der Kaufpreisvorstellung. Transport und Logistik Schwertransporte, Spezialtransporte',\n",
       " 'Unternehmer sucht Transportunternehmen Serhat ist Inhaber einer Transportfirma und sucht Spedition zum Kauf War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nWir als die Tekmann Transporte GmbH suchen neue Geschäftsfelder und sind bereit erfolgreich im der Transport Branche tätig. Und haben Interesse zu Übernahme ihres Betriebes.\\nDaher freuen wir uns für eine Rückmeldung\\nMfg Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Familienunternehmen sucht Logistik-Firma / Spedition Daniel ist Leiter eines Familienunternehmens im Logistik-Sektor und sucht Spedition zum Kauf in Hessen oder NRW. War Kauf-Interessent einer Spedition in Nord-Hessen. Er schrieb:\\nwir sind eine mittelständisches Familienunternehmen mit ca. 165 Mitarbeitern und sitzen in NRW. Da wir uns aktuell vergrößern möchten, sind wir auf der Suche nach einem geeigneten Partner der sein Unternehmen verkaufen möchte und zu uns passen. Transport und Logistik Spedition und Logistikdienstleistungen',\n",
       " 'Speditionsgruppe sucht passende Firma zum Kauf Kevin ist Leiter einer Speditionsgruppe und sucht ein Transportunternehmen zum Kauf in Hessen, Sachsen oder baden-Württemberg War Kauf-Interessent einer Spedition in Nord-Hessen. Er schieb:\\nwir sind an der Nachfolge Ihres Unternehmens interessiert.\\n\\nZu uns:\\nAus einem Zusammenschluss von Speditionsunternehmern entstand in 2024 die Unternehmensgruppe - Core Logistics GmbH.\\nDie Beteiligten sind einerseits erfahrene Spediteure mit 15-25 Jahren Berufserfahrung und auf der anderen Seite Unternehmer die Erfahrung im Bereich des Unternehmenswachstum haben.\\n\\nGemeinsam führen wir bereits heute eine Speditionsgruppe zwischen Darmstadt/Ulm/Zwickau mit aktuell rund 14 Mio Umsatz und führen die Kompetenzen zu einer Firmengruppe zusammen.\\n\\nGerne tauschen wir uns einmal am Telefon aus um grundsätzlich zu prüfen, ob eine Unternehmensnachfolge mit uns gemeinsam möglich ist. Transport und Logistik Spedition, Logistikdienstleistungen',\n",
       " 'Logistikgruppe sucht Möbelspedition Peter ist in der Geschäftsführung einer Logistik-Gruppe und sucht eine Möbelspedition zur Übernahme. War Kauf-Interessent einer Spedition in Nord-Hessen. Reber ist eine Logistik-Gruppe, die u.a. im Möbeltransport tätig ist Transport und Logistik Möbeltransporte, Umzugsdienste',\n",
       " 'Logistik-Immobilien gesucht Investor sucht Logistik-Immobilie oder Firma im Bereich \"Container\" War Kauf-Interessent einer Spedition in Nord-Hessen.\\nWir suchen aktuell nach Logistik-Immobilien aber auch nach Immobiliennahen operativen Gesellschaften, beispielsweise im Logistik Bereich. Hier sind wir vor allem auch an Containern interessiert.  Immobilien und Logistik Logistikimmobilien, Containerdienste',\n",
       " 'Geschäftsführer sucht Pflegedienst zur Übernahme Markus ist Geschäftsführer eines Pflegedienstes und sucht einen weitere Pflegedienst in Raum München + 1,5h Fahrtzeit zur Übernahme Anfrage über Purposition am 29.08.24 von Marcus Kerwin, Voli-Pflege.de, bis 0,5 Mio\\nBayern, am liebsten bis 1,5 h rund um München\\nAuschluss: keine außerklinische Intensivpflege\\nIch habe selbst Inserat bei NC veröffentlicht:\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=210760 Gesundheitswesen Ambulanter Pflegedienst, Pflegeleistungen',\n",
       " 'Unternehmen sucht Erweiterung: Tank, Behälter, Heizung, Oberflächenschutz etc. Wir (ein Unternehmen im Bereich Tank-, Behälterschutz und Behälter-Anlagenbau) suchen eine Firma zum Kauf im Bereich Behälter / Tanks usw. Wir sind ein Unternehmen aus Süddeutschland und bauen, installieren und warten Behälteranlagen (Von Öl-Tanks, Heizungstanks bis hin zu Industriebehältern). Wir suchen eine passende Erweiterung für unser Kerngeschäft. Gesucht werden Unternehmen, auf die eines der folgenden Stichworte zutrifft: \\n\\nRund um Öl:\\nBehälterbau\\nBehälteranlagen\\nTankschutz, Tank-Wartung, Tank-Reinigung, Tank-Instandhaltung, Tank-Reparatur\\nTank-Installation, Tank-Abbau, Tank-Entsorgung\\nIndustriebehälter Herstellung Reinigung\\n\\nRund um Wasser:\\nHeizungstanks (Bau, Reinigung, Entkalkung)\\nBoiler\\n\\nRund um Oberflächenschutz:\\nOberflächenbeschichtung\\nOberflächenschutz\\nSandstrahlverfahren\\nBehälterbeschichtung und Tankbeschichtung\\nOberflächensanierung\\n\\nHabe selbst Inserat veröffentlicht:\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=210880 Industriedienstleistungen Tankbau, Oberflächenschutz, Industrieanstriche',\n",
       " 'Unternehmer sucht Hausverwaltung zum Kauf Thomas ist Immobilienmakler und sucht Hausverwaltung zur Übernahme in NRW 05.09.24. Rückfragen gestellt\\n04.09.24: Käufer hat Anfrage via UB geschickt. Er schrieb\\nSeit Jahren als Immobilienmakler tätig.\\nDie Partnerin arbeitet seit Jahren in der Hausverwaltung.\\nDer nächste logische Schritt, ist die Gründung bzw. Übernahme einer eigenen Verwaltung.\\nRegion Köln / vorzugsweise Kölner Westen und Rhein-Erft Kreis.\\nDa wir seit Jahren aber auch im Raum Düsseldorf/Neuss tätig sind, ist dort eine Übernahme auch denkbar. Immobilien Hausverwaltung',\n",
       " 'Immobilienunternehmer sucht Hausverwaltung Reza ist Immobilienunternehmner und sucht eine Hausverwaltung zum Kauf 20.09.24: Rückmeldung zu Kaufpreis erhalten\\n20.09.24: Habe Rückfrage zu Kaufpreis gestellt\\n18.09.24 Käufer hat Anfrage via UB geschickt:\\nHi ich bin Reza und bin als Immobilienunternehmer interessiert eine Hausverwaltung bis 350.000 € zu übernehmen.\\nIch selbst habe Erfahrung in der Vermietung und Verkauf, sowie der Verwaltung von Objekten aus dem Family Office. möchte: Umfassende Einarbeitung druch Inhaber Immobilien Hausverwaltung',\n",
       " 'Elektroingenieur sucht Elektrofirma zur Übernahme Mario ist Elektroingenieur und sucht eine Elektrofirma / Elektroinstallatin / Kommunikationstechnik-Firma in München +100 km 20.10.24: (Inserat angeschrieben, dann Absage: Verkäufer hat keine Lust auf Agentur)\\nhttps://www.hwk-muenchen.de/74,0,bbdetailoffer.html?id=15706\\n\\n\\n05.09.24: habe selbst Kauf-Gesuch bei NC veröffentlicht: (mit Purposition account): S-34f2b6\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=210800\\n\\n04.09.24 Anfrage via PP geschickt: \\nIch suche eine Elektrofirma zum Kauf in München oder Umgebung. Ich bin selbst Elektroingenieur (FH) und zur Zeit in Anstellung. Der Tätigkeitsschwerpunkt der Firma könnte klassische Elektroinstallation und / oder Kommunikationstechnik hauptsächlich für Wohngebäude sein. bis 150 Tsd. Elektroindustrie Elektroinstallation, Kommunikationstechnik',\n",
       " 'Hausverwalter sucht HV zur Übernahme Julius hat bereits eine Hausverwaltung und sucht nun eine weitere Hausverwaltung zum Kauf. Guten Tag, wir betreiben seit rund 12 Jahren nebenberuflich eine kleine Hausverwaltung mit rund 50 Einheiten, ausschliesslich aus dem eigenen Grundbesitz in der Mietverwaltung. Wir suchen eine kleine-mittelgroße HV bis 1 Mio. zur Übernahme aus dem Raum Ruhrgebiet oder Oberbayern um uns zu vergrößern und die Aktivitäten zu erweitern. Immobilien Hausverwaltung',\n",
       " 'Junger Hausverwalter sucht in Mittelfranken Kim arbeit in einer WEG-Verwaltung und möchte sich durch die Übernahme einer Hausverwaltung selbstständig machen. mein Name ist Kim Kevin Burgahrt, arbeite seit längerem in einer WEG-Verwaltung und suche jetzt im Raum Mittelfranken eine Verwaltung die ich übernehmen/kaufen kann. 50 - 250 Tsd. Euro. Immobilien Hausverwaltung',\n",
       " 'Unternehmer sucht Hausverwaltung Tristan ist im Immobilienberich tätg und sucht eine Hausverwaltung bis 2.000 Einheiten. wir sind an der Übernahme einer Hausverwaltung interessiert bis 2.000 Einheiten.\\nHat Erfahrung im Bereich der Miet.- und WEG-Verwaltung, aber hat noch keine eigene Hausverwaltung. Am 16.10.24 telefoniert Immobilien Hausverwaltung',\n",
       " 'Heizungsbaumeister sucht SHK Betrieb Heizungsbaumeister und Inhaber eines SHk Betriebs sucht weiteren Betrieb zur Übernahme. Hatten Kontakt wg. Kriegl Sanitär zum Kauf. Kauf-Interessent hat bereits einen SHK Betrieb und möchte sich erweitern. Sucht Heizung Sanitär Betriebe rund um Nürnberg Fürth. Handwerk Heizung, Sanitär, Klima (SHK)',\n",
       " 'SHK Betrieb in Sachsen gesucht Markus ist gelernter Anlagenmechaniker für SHK-Technik und sucht SHK Betrieb. 16.10.24: Betrieb angeboten, bisher keine Rückmeldung\\n02.09.24: Anfrage:\\nSucht SHK Betrieb, Unter 5-10 Mitarbeiter. Raum Sachsen ( Chemnitz/ Zwickau )\\n18jahre Berufserfahrung im Bau und Kundendienst\\nGelernter Anlagenmechaniker für SHK Technik Handwerk Heizung, Sanitär, Klima (SHK)',\n",
       " 'Heizungsbaumeister sucht SHK Betrieb Felix ist Heizungsbaumeister und möchte sich SHK Betrieb zwischen Celle - Hannover übernehmen, 21.10.24: Betrieb angeboten ( https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=203445 ), passt aber nicht (Betrieb sitzt auf Elterngrundstück, SOftware veraltet etc.)\\n27.09.24 nettes Telefonat, 3%\\nAnfrage 22.09.24\\nIch Felix Heinze habe einen Meistertitel im Bereich Heizung und Sanitär. Und suche einen Betrieb zur Übernahme. In der Region Celle und Hannover. 5-6 Angestellte. Ca. 100T Handwerk Heizung, Sanitär, Klima (SHK)',\n",
       " 'Angehender Heizungsbaumeister sucht SHK Betrieb Angehender Heizungsbaumeister sucht kleinen SHK Betrieb zur Übernahme. Anfrage über UB am 23.09.24:\\nsuche einen Shk Sanitär-Heizungs-Klimatechnik in Niedersachsen.  Übernahmepreis wäre mir erstmal nicht so wichtig, ob 3 oder 5 Mitarbeiter. Ich selber besuche gerade die Meisterschule die ich voraussichtlich im Mai beenden werde, In NRW komme aber ursprünglich aus Niedersachsen. Ich suche nämlich auch einen Inhaber der interesse hätte den Meisterkurs bezahlen würde und demnach sofort in die Einarbeitung für die Übernahme seiner Firma anfangen würde .  Handwerk Heizung, Sanitär, Klima (SHK)',\n",
       " 'Firma gesucht: Gebäudetechnik oder Brandschutz Matthias ist Sachverständiger für Gebäudetechnik und Vorbeugender Brandschutz und möchte ein passendes Unternehmen im Bereich Gebäudetechnik oder Brandschutz übernehmen. 13.11.24: Verkäufer-Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=209981\\n\\nnoch nicht angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210882\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208421\\n\\nInserat angeschrieben (Rückmeldung 28.10.24 Christine Schmitt, ist schon in Verhandlung, Update an Kauf-Interessent geschickt)\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210993\\n\\nAnfrage über dejuna am 26.09.24:\\n\"Ich bin Sachverständiger für Gebäudetechnik und Vorbeugender Brandschutz, SiGe, Sicherheitsbeauftragter, Gefahrgutbeauftragter, SiFa und Suche ein geeignetes Unternehmen in der Region Baden - Württemberg und Bayern zu übernehmen. Ingenieurbüro oder Elektroinstallationsfirma (wahrscheinlich passt Brandschutz und gebäudetechnik am ehesten, weil er keinen Meister hat? nochmal nachfragen). Beteiligung oder Übernahme. Bis 1 Mio €\"\\nKommt wohl aus Singen / BW? Ingenieurdienstleistungen Gebäudetechnik, Brandschutztechnik',\n",
       " 'Schlossermeister sucht weiteren Schlosserei-Betrieb zur Übernahme. Mehmet ist Schlossermeister und sucht einen weiteren Metallbau-Betrieb zur Übernahme. am 26.09.24 wg. Schlosserei Weber Mannheim per Mail angeschrieben. Ist Schlossermeister, hat bereits eine Schlosserei und sucht weitere Schlosserei zum Kauf Handwerk Metallbau, Schlosserei',\n",
       " 'Schreinermeister sucht Tischlerei Daniel ist Schreinermeister und sucht Tischlerei im Rheingau Ich bin Schreinermeister und schon seit mehreren Jahren selbstständig als Einzelunternehmer.\\nGerne würde ich eine Schreinerei übernehmen hier im Rheingau das bedeutet zwischen Wiesbaden und Rüdesheim. Handwerk Schreinerei, Tischlerei',\n",
       " 'KFZ-Meister sucht Werkstatt Mohammad ist KFZ-Meister und sucht eine Autowerkstatt zur Nachfolge in Sachsen Telefonat am 23.08.24: sehr nettes Telefonat (freundlciher Typ, sprachlich für nicht nativ speaker absolut ok, klingt gebildet, freundlich. hat sich bereits mit Kaufpreisfinanzierung etc beschäftigt)\\nKontakt wg. diesem Inserat gehabt: https://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=205816 Ich bin Kfz-Meister und suche nach einem Betrieb zur Übernahme.\\n\\n2020 habe ich meinen Gesellenabschluss gemacht. Anschließend habe ich die Meisterschule besucht und 2021 den Meisterabschluss als Kraftfahrzeugtechniker erfolgreich bestanden.\\nNebenbei habe ich noch den geprüften Fachmann für kaufmännische Betriebsführung absolviert. Hierbei konnte ich meine Fertigkeiten und Kenntnisse optimieren und erweitern.\\n\\nIm Moment befinde ich mich im Studium zum \"Dipl.-Ing. (FH)\\nKraftfahrzeugelektronik, welches ich voraussichtlich 2025 abschließen werde.\\n\\nIch suche eine Kfz-Werkstatt mit Hebebühnen zum Kauf.  Fahrzeugdienstleistungen Kfz-Reparaturwerkstätten',\n",
       " 'Historisches Weingut gesucht Konrad ist Unternehmer und sucht ein historisches Weingut in der Region Pfalz, Baden, Breisgau. 25.10.24: Anfrage via dejuna:\\nBin Quereinsteiger und Suche erstklassiges bestens funktionierendes Weingut ab 15 ha in Pfalz, Baden , Breisgau.\\nSchön wäre ein historisches Weingut.\\nGutes Personal und eigene Weinherstellung. Der Zielbetrieb sollte alle Mitarbeiter mitbringen.\\nIch selbst werde im kaufm. Bereich mitarbeiten, meine Frau im Bereich Vinothek und Kundenbetreuung. Frage zu Kaufpreis unbeantwortet Landwirtschaft und Weinbau Weinbau, Weingut',\n",
       " 'Geschäftsführer sucht Logistik-Unternehmen Geschäftsführer sucht Transport / Logistik-Unternehmen im Bereich Industrie, Pharma, Spezialtransporte, Consumer Goods 09.08.24 hat Käufer Anfrage via UB gesendet:\\n- Vorzugsweise Transport und Logistik im Bereich Industrie, Pharma, Spezialtransporte, Consumer Goods\\n- Den Kaufpreis lasse ich offen, ist er bei einem Kleinstunternehmen bis 1,5-2 MIO kann ich diesen finanzieren, mir stehen selbst als Einzelinvestor sofort 350k zur Verfügung, bei größeren Finanzierungen mit Partner (habe ich an der Hand falls das Unternehmen passen sollte)\\n- Standort vorzugsweise der Norden, ich selbst lebe mit meiner Familie in Hamburg – schließe aber je nach möglicher Passgenauigkeit eines Unternehmens natürlich andere Standorte nicht aus\\n\\nMoin aus Hamburg,\\n\\nich bin 47 Jahre alt und seit mehr als 25 Jahren in der Transport und Logistikbranche tätig. Seit 2011 in Geschäftsführungspositionen. Erfahrung im Bereich Landfracht, Fuhrpark, Luft- und Seefracht sowie Kontraktlogistik.\\nBedingt Gesellschaftlicher Entscheidungen habe ich beschlossen meine Zukunft beruflich neu auszurichten. Nachdem ich immer schon Unternehmer werden wollte, bin ich seit geraumer Zeit auf der Suche nach einem Unternehmen, wo ein Nachffolger gesucht wird oder was direkt übernommen werden kann.\\nBevorzugut im Norden v. Deutschland, da ich hier auch meinen Lebensmittelpunkt habe. Grenznah an Österreich oder die Schweiz wäre auch in Ordnung. Transport und Logistik Spezialtransporte, Industrielogistik, Pharmalogistik',\n",
       " 'Maschinenkonstrukteur sucht Maschinenbau-Betrieb suche für Uwe (er ist Maschinenbaukonstrukteur) einen Maschinenbau-Betrieb zur Übernahme, der eigene Produkte oder Maschinen herstellt (also keinen reinen Fertigungsbetrieb). 13.11.24: Inserat angeschrieben (mit Makler)\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210167\\n\\n07.11.24: beide Inserate angeschrieben\\n07.11.24: Rückmeldung: findet er beides spannend\\n07.11.24: Mail wg. Hydraulikunternehmen + Medizintechnik-Unternehmen geschrieben: https://www.ihk.de/sbh/boersen/exiboerse/unternehmen/vs-ex-a-25-24-6292208\\nhttps://www.ihk.de/sbh/boersen/exiboerse/unternehmen/vs-ex-a-24-21-medizintechnikunternehmen-zum-verkauf-5315142\\n\\n09.09.24: Telefonat mit Kauf-Interessent. Notizen:\\nIst Maschinenkonstrukteur (hat mal Industriemechaniker gelernt, Fachrichtung Maschinensystemtechnik,\\nsucht Maschinenbauer mit eignem Produkt/Maschinen.\\nSondermaschinenbau, Vorrichtung, Maschinenelementen (Pumpe, Getriebe/ Kupplungen), Hochdruckpunpte, Steuergeräte, mechanische Komponenten, Pharma Maschinenbau \\nreiner Fertigungsbetrieb eher nicht, lieber mit eigenem Produkt | eigener Maschinen-Entwicklung/Bau\\nZeitraum: nächstes halbes Jahr\\nStandorte: Sucht in: Schwäbisch Hall, Heidelberg, Bodenseeregion, Friedrichshafen, Hunsrück, Heilbronn\\neher kleinere Unternehmen interessant zB. bis 1 Mio\\n\\n\\n13.08.24: per Inserat angeschrieben\\nhttps://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=199996 Herstellung und Engineering Maschinenbau, Anlagenbau',\n",
       " 'Finanzberaterin sucht Versicherungsagentur zur Übernahme Finanzberaterin sucht Versicherungsagentur zur Übernahme 20.09.24: Rückfragen gestellt\\nAnfrage über UB am 17.09.24\\nEine motivierte und frisch angehende Finanzberaterin und suche Deutschlandweit Versicherungsmarkler die eine Nachfolge suche.\\nWohnhaft bin ich Nähe Bodensee aber durch die Digitalisierung unserer Beratung im Dach- Raum suchend. Finanzdienstleistungen Versicherungsmakler',\n",
       " 'Logistik-Firma sucht Spedition zur Übernahme Geschäftsführer einer Spedition sucht weitere Spedition zur Geschäftserweiterung Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208011\\n\\n30.10.24: Habe Rückfragen gestellt\\n30.10.24: Anfrage via UB:\\nWir sind ein belgisches Transportunternehmen mit insgesamt 100 Fahrzeugen, inklusive Subunternehmern, davon sind 30 in Deutschland zugelassen.\\nUnser Unternehmen konzentriert sich hauptsächlich auf Komplett- und Teilladungen mit Tautliner.\\nWir sind auch auf Luftfracht mit Rollerbet- und Kofferanhängern spezialisiert.\\nDies betrifft Transporte von und nach Belgien – Deutschland – Österreich und der Schweiz sowie den Benelux-Ländern.\\nWir mieten derzeit eine Unterkunft (Parkplatz – Büro ) in Düren für unsere deutsche Tochtergesellschaft.\\n \\nWas wir suchen, kann eine komplette Spedition umfassen mit Fuhrpark oder einfach nur Immobilien  (Büro + Parkplatz)\\n \\nBei den von uns transportierten Waren handelt es sich in der Regel um Paletten Ware mit oder ohne ADR.\\n \\nWir suchen ein geeignetes Unternehmen, vorzugsweise auf der Achse AACHEN – ESCHWEILER – DÜREN – KERPEN – mit einem Jahresumsatz zwischen 5 – 10 Millionen Euro.\\n(Laut Websiete machen sie bisher Gefahrguttransport, Nahrungsmittel-Transport, Baugewerbe) Transport und Logistik Spedition, Transportdienstleistungen',\n",
       " 'Hausverwaltungsgruppe sucht weitere Hausverwaltung Junge Hausverwaltung sucht weitere Verwaltung zur Übernahme. 30.10.24: Feedback vom Käufer: \"Lassen Sie uns Anfang kommenden Jahres sprechen, wie eine Zusammenarbeit aussehen könnte.\"\\n30.10.24: Follow up per Mail\\n02.10.24: Telefonat, arbeitet schon Hausveraltungsmaklern emorion, tbk, dr adams, der hauslehrer \\n(DUB, nexxt Change brauche ich nicht screenen, die diese Makler dort veröffentlichen)\\nist Interessiert an Suchmandat und bespricht sich intern (deal 120/h + 3)\\n30.09.24: hat Interesse an Suchmandat, möchte telefonieren\\n25.09.24 Suchmandat angeboten\\n\\nBiz Inserat:\\nWir sind eine junge, innovative Hausverwaltung, welche historisch eigene Bestände verwaltet und sich nun auch für Dritte öffnet.\\nNeben der Übernahme von Mandaten in der eigenen Hausverwaltung wollen wir auch aktiv durch strategische Zukäufe wachsen. Wir schauen deutschlandweit nach spannenden Hausverwaltungen mit Fokus Wohnen jeder Größe und unterstützen auch bei Nachfolgesituationen.\\nWir suchen insbs. in:\\n\\nRhein-Main: Frankfurt, Mainz, Offenbach, Darmstadt und Region\\nRhein-Ruhr: Gladbach, Wuppertal, Düsseldorf und Region\\nOberbayern: München, Ingolstadt und Region\\nBerlin\\nLeipzig Immobilien Hausverwaltung',\n",
       " 'Jung-Unternehmer sucht Hausverwaltung zum Kauf Jung-Unternehmer sucht Hausverwaltung zum Kauf in Oberbayern 06.11.2024: Rückfragen gestellt, direkt Antwort erhalten\\n05.11.2024: Anfrage\\nWir (zwei Jung-Unternehmer) suchen eine Hausverwaltung zum kaufen aus dem Raum Pfaffenhofen a. D. ILM, Ingolstadt und München. Bis 200.000 € Immobilien Hausverwaltung',\n",
       " 'Ambulanter Pflegedienst in Süd-Hessen gesucht Pflegedienstleitung sucht ambulanten Pflegedienst zur Übernahme 06.11.2024: Rückfragen gestellt, Antworten (s.u.) bekommen\\n06.11.2024: Anfrage\\nIch suche für mich eine Ambulante pflegedienst in Süd Hessen (Darmstadt, Frankfurt) zu kaufen.\\nEigene Kapital haben wir nicht, aber wir sind Eigentümer .\\nWegen Kaufpreis, ich weiß noch nicht, ich möchte nicht etwas groß zu kaufen. \\nIch bin gelernte krankenschwester, und Pflegedienst leitung  Gesundheitswesen Ambulante Pflege, Pflegedienstleistungen',\n",
       " 'Industriemeister Metall sucht Schlosserei zur Übernahme Serkan ist Industriemeister Metall und sucht Schlosserei in Mannheim zur Übernahme. Bis 150 Tsd. € Inserat angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=207374\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=204787\\n\\n07.11.24: Rückfrage zu Meistertitel + Kaufpreis an Serkan geschickt\\n07.11.24: NC Inserat angeschrieben: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=204787\\n\\n06.11.24: Habe ihm Rückfragen gemailt, Tag später Antwort erhalten\\n06.11.24: Hat sich auf Schlosserei Inserat Mannheim gemeldet. Er schrieb:\\nIch könnte mir eher vorstellen, dass ich eine komplette Schlosserei übernehmen kann. Falls mal etwas reinkommen sollte, dürfen Sie mich gerne diesbezüglich kontaktieren.\\nAusbildung Mechatroniker für Kältetechnik\\nWeiterbildung Industriemeister Metall (Schlossermeister Weber sagt: das ist kein Handwerksmeister, er bräuchte also einen MetallbauMeister als Angestellten?!)\\nIch komme aus Hassloch, deshalb bin ich offen für weitere Orte zwischen Mannheim und Hassloch.\\nSucht bis 150 Tsd. € Handwerk Metallbau, Schlosserei',\n",
       " 'Versicherungsmakler sucht Versicherungsagentur Martin ist Versicherungsmakler und sucht eine Versicherungsagentur zur Übernahme 06.11.24 Anfrage via UB:\\nIch suche eine zu übernehmende Versicherungsagentur in und um Leipzig.\\nDie erforderlichen Kenntnisse und Erlaubnisse besitze ich. Finanzdienstleistungen Versicherungsmakler',\n",
       " 'Physiotherapeutin sucht Physio-Praxis Corinna ist Physiotherapeutin und sucht eine Physio-Praxis mit Mitarbeiten zur Übernahme. 10.11.24: Anfrage via UB:\\nSuche Physiotherapie-Praxis. Gern auch mit angestellten Therapeuten.. Ich suche im Raum Chemnitz und Erzgebirge eine Möglichkeit meinen Wirkungskreis zu vergrößern. Wer gern sein Pensionsdasein endlich genießen möchte soll sich gern melden! Gesundheitswesen Physiotherapie',\n",
       " 'Jung-Unternehmer sucht Malerbetrieb zum Kauf in Dortmund + 25km Ibrahim hat einen Betrieb für Innenausbau und sucht einen Malerbetrieb zum Kauf Eckpunkte Kauf-Gesuch:\\nSucht Malerbetrieb oder Fliesenleger-Betrieb.\\nDer Betrieb sollte 25km rund um Dortmund sein, mindestens 5 Mitarbeiter umfassen, idealerweise mit einem Malermeister oder Bauleiter und einer Bürokraft.\\nEin eigener Fuhrpark sowie eine eigene Lagerstätte mit integriertem Büro wären wünschenswert.\\nBis 1 Mio €\\n––\\n20.11.24: Verkäufer 2 angeschrieben https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=212252\\n20.11.24: Verkäufer 1 angerufen. Betrieb ist schon verkauft\\n20.11.24: Telefonat mit Käufer\\n19.11.24: Inserat angeschrieben wg. Malerbetrieb aus Bochum. Provision in Mail erwähnt. Nochmal checken, ob er das gelesen hat\\nKäufer: https://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=211043 \\nVerkäufer 1 könnte sein: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=209176 Handwerk Maler-Betrieb, Fliesenleger-Betrieb',\n",
       " 'Pflegedienste gesucht: Unternehmen möchte expandieren Herbert ist Inhaber einer Pflegegruppe und sucht einen Pflegedienst zur Übernahme 20.11.24: Anfrage über dejuna\\nMoin,\\nwir sind ein renovierter Pflegedienst seit 1992 mit rund 150 Mitarbeitern in Ostfriesland. Hier suchen wir zum Ausbau und Erweiterung weitere Pflegedienste. Wir sind komplett digitalisiert und suchen Pflegedienste wo die Nachfolge nicht geregelt ist.\\nWir suchen im Raum Ostfriesland – Friesland- Oldenburg. Das Emsland würde ich schon ausschließen wollen.  \\nKaufpreis bis 1.000.000,- Gesundheitswesen Ambulante Pflege, Pflegedienstleistungen',\n",
       " 'Ehepaar sucht Pflegedienst Ehepaar sucht Pflegedienst zum Kauf im Raum Köln +50km 02.12.24: Rückfrage zum Kaufpreis gestellt\\n24.11.24: Anfrage via dejuna:\\nmein Ehemann und ich sind auf der Suche nach einem ambulanten Pflegedienst zum Kauf. Wir kommen aus Köln, im Umkreis von ca 50km würden wir über Angebote sehr freuen. Gesundheitswesen Ambulante Pflege, Pflegedienstleistungen',\n",
       " 'Investor sucht Hausverwaltung Gerrit ist Investor im Immobiliensektor und möchte eine Hausverwaltung in Norddeutschland und Berlin übernehmen 03.12.24: braucht kein Suchmandat, möchte aber Angebote\\n02.12.24: Suchmandat angeboten (120 + 3)\\n\\n25.11.24: Anfrage via dejuna:\\nInvestor sucht wietere Hausverwaltung zum Kauf. Wir suchen:\\n- Eine etablierte Hausverwaltung\\n- Ansässig in Norddeutschland PLZ-Bereich 2**** und Metropolregion Berlin\\n- Ab 1.000 verwaltete Wohneinheiten\\n- Vorrangig Mietverwaltung von Wohnungen, Garagen und Gewerbe\\n- Anteilig auch Verwaltung nach dem WEG-Gesetz\\n- Zuverlässiger Mitarbeiterstamm\\n- Kaufpreis im 6- bis niedrigen 7-steligem Bereich\\nUnser Angebot:\\n- Wir bieten Ihrem Unternehmen und Mitarbeiter*Innen eine langfristige\\nPerspektive\\n- Wir stelen bei einer Nachfolge-Lösung einen Volzeit geschäftsführenden\\nGeselschafter (MBI ‒ Management Buy In)\\n- Kapitalstarke Unternehmensgruppe für Zukunfts-Investitionen\\n- Fortführung Ihres Lebenswerkes als eigenständiges Unternehmen Immobilien Hausverwaltung',\n",
       " 'Immobilienkaufmann sucht Hausverwaltung Jan ist Immobilienkaufmann und sucht eine Hausverwaltung in Norddeutschland Inserate angeschrieben:\\nhttps://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=210028\\n\\n02.12.24: Rückfrage zu Kaufpreis gestellt, dann telefoniert, sehr nett, Suchmandat erwähnt (120, 3)\\n\\n01.12.24 Anfrage via dejuna:\\nSehr geehrte Damen und Herren,\\nich bin gelernter Immobilienkaufmann (IHK) mit mehrjähriger Erfahrung in der Immobilenverwaltung und Geschäftsführer einer Hausverwaltung. Suche in der Region zwischen Bremen, Hannover, Hamburg und Lüneburg. Lieber Mitverwaltung, als nur reine WEG-Verwaltungen. Gerne mehr als 200 Einheiten.\\nVon 50 T bis max. 1 Mio.\\n\\nSofern Sie Unternehmen in den betreffenden Regionen im Angebot haben, freue ich mich über Ihre Nachricht.\\n\\nFür Rückfragen zum Kaufgesuch kontaktieren Sie mich gern, per E-Mail.\\n\\nMit freundlichen Grüßen\\nJan-Erik Rohrbeck Immobilien Hausverwaltung',\n",
       " 'Bestattungsunternehmen gesucht Jung-Unternehmer sucht Bestattungsunternehmen zur Übernahme in Niedersachsen 02.12.24\\nDieses Inserat könnte passen: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=200512\\n\\n02.12.24 Hat via dejuna angerufen\\n- sucht Bestattungsunternehmen\\n- hat lange als Bestatter gearbeitet\\n- bis 150 T\\n- nice to have: Unternehmen soll Abrechnungssoftware z.B. mit adelta Finanz o.ä. zusammenarbeiten (um autom. Rechnungen zu versenden)\\n\\nDeal: 2 für Vermittlung, Beratung zusätzlich (fragte nach Hilfe bei Businessplan, usw.)\\nSchnack sehr gerne! Handwerk Bestattungsgewerbe, Bestattungsunternehmen',\n",
       " 'Dentallabor gesucht Björn ist Zahntechniker-Meister und sucht ein Dentallabor zur Übernahme 11.12.24: Verkäufer 2 angeschrieben:\\nSteinfurt: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=208586\\n06.12.24: Verkäufer angeschrieben:\\nBorken: https://www.nexxt-change.org/DE/Verkaufsangebot/Detailseite/detailseite_jsp.html?adId=211131\\n\\n05.12.24 sein Kauf-Gesuch angeschrieben: https://www.nexxt-change.org/DE/Kaufgesuch/Detailseite/detailseite_jsp.html?adId=211076\\n\\nSein Inserat: \\nZahntechnikermeister und BdH mit langjähriger Führungserfahrung sucht zum nächstmöglichen Zeitpunkt ein Dentallabor ohne Investitionsstau im Westmünsterland oder nördlichen Ruhrgebiet zur Übernahme oder Beteiligung.\\nBorken / Coesfeld / Recklinghausen / Botrop / Kreis Wesel\\n\\nErgänzung am 07.01.25 per Mail:\\nIch bin 45, seit fast 20 Jahren ZTM mit einem Abschluss als BdH. Seit rund 20 Jahren arbeite ich fast immer als Laborleiter. Bei meinem derzeitigen Ag arbeite ich seit nunmehr 14 Jahren und bin eigentlich sehr zufrieden. Nur die Aussicht auf einen eigenen Betrieb könnte mich zu einem Wechsel bewegen. Derzeit führe ich ein Team mit ca. 25 Technikern.\\nMeine schwerpunkte liegen in der Kundenbetreuung und der Ästhetischen Frontzahnversorgung.\\nEinen kurzen Lebenslauf füge ich bei. Gesundheits- und Sozialwesen Gesundheitswesen, Dentallabor',\n",
       " 'Hausverwaltung gesucht Vter & Sohn-Gespann sucht eine Hausverwaltung zum Kauf 08.12.24: Anfrage via dejuna:\\nIch bin auf der Suche nach einer zum Kauf anstehenden Hausverwaltung in der Region Soest/ Möhnesee/ Arnsberg oder Waldeck/ Kassel/ Bad Wildungen. mein Sohn ist seit einiger Zeit in der Hausverwaltung tätig und wird sich mittelfristig selbstständig machen. Dabei werde ich ihn unterstützen und die ersten Jahre auch mitarbeiten. Bei einer möglichen Finanzierung sehe ich keine Probleme. Auch wenn der bisherige Inhaber sein Unternehmen erst in den nächsten Jahren verkaufen möchte und bereit wäre, seinen potentiellen Nachfolger noch sukzessive einzuarbeiten, wäre es für uns interessant. Immobilien Hausverwaltung',\n",
       " 'Unternehmen sucht Dachdeckerei, Zimmerei, Schreinerei, Tischlerei und Fensterbau-Firma zum Kauf Handwerksbetrieb sucht: Dachdeckerei, Zimmerei, Schreinerei, Tischlerei und Fensterbau-Firma zum Kauf Kleinanzeigen Inserat (noch nicht kontaktiert)\\nWir sind ein etabliertes Unternehmen mit einer starken Präsenz im Bereich des Dachdeckerhandwerks. Als Teil unserer Wachstumsstrategie sind wir auf der Suche im Raum München, Rosenheim und Umgebung nach Unternehmen, die zur Übernahme oder Weiterführung geeignet sind. Unser Ziel ist es, unser Fachwissen und unsere Ressourcen zu nutzen, um das Erbe Ihres Unternehmens zu bewahren und weiter auszubauen.\\n\\nGesuchtes Unternehmen:\\nBranche: Dachdeckerhandwerk, Spengler, Zimmerer, Schreiner, Fensterbau\\nEigenschaften: Wir suchen nach Unternehmen mit einer soliden Kundenbasis, erfahrenem Fachpersonal und einem guten Ruf in der Branche.\\nMindestumsatz: netto p.a. 1.000.000 €\\nWir sind an Unternehmen aller Größenordnungen interessiert, von Einzelunternehmen bis hin zu Betrieben mit über 20 Mitarb. Handwerk Dachdecker, Sprengler, Schreiner, Gensterbau',\n",
       " 'Unternehmer sucht Gebäudereinigung Ali hat eine etablierte Reinigungsfirma und sucht eine zweite Gebäudereinigung zum Kauf 11.12.24: Anruf von Ali, Zusammenfassung (2 und 120, interessiert):\\nich bin auf der Suche nach einem Unternehmen im Kreis Bürstadt 68642 umkreis 40 KM.   bei Worms , das in den Bereichen Gebäudereinigung, Gartenbau oder Malerbetrieb tätig ist und zur Übernahme steht. Besonders interessiert bin ich an einem Betrieb, der gut etabliert ist und über eine solide wirtschaftliche Grundlage sowie bestehende Kundenbeziehungen verfügt.\\n\\nMein Ziel ist es, den Betrieb erfolgreich weiterzuführen und auszubauen, wobei mir die Qualität der Dienstleistungen und ein gutes Verhältnis zu bestehenden Mitarbeitenden und Kunden besonders am Herzen liegen. Ich bin an offenen Gesprächen mit Eigentümern interessiert, die ihre Nachfolge planen oder andere Überlegungen zum Verkauf haben. Handwerk Gebäudereinigung',\n",
       " 'Hausverwalter sucht zweite Hausverwaltung zum Kauf Thomas hat bereits eine Hausverwaltung und sucht eine zweite Immobilienverwaltung zum Kauf im Raum Ansbach, Nürnberg oder München. 17.12.24: Anfrage via dejuna\\nIch bin seit über 30 Jahren im Bereich der Hausverwaltung tätig.\\nDerzeit vergrößern wir unseren Bestand durch Zukauf, da unser Sohn, 33 Jahre auch mit langjähriger Verwaltungserfahrung, das Unternehmen ausbauen möchte.\\nWir haben Interesse an den Regionen/Umgebungen: Ansbach, Nürnberg, München. Radius + 50km\\nKaufpreis bis 1 Mio Immobilien Hausverwaltung',\n",
       " 'Bestattungsunternehmen gesucht Matthias sucht ein Bestattungsunternehmen im Raum Stuttgart zur Übernahme. 19.12.24: Rückfragen gestellt, direkt Antwort erhalten\\n18.12.24: Anfrage via dejuna\\nBin auf der Suche zur Übernahme eines Bestattungsunternehmen im Raum Stuttgart .Mfg.M.Wei\\nIch suche für mich selbst , hätte aber dann mit einem anderen Bestattungsinstiut eine Kooperation. Eigenkapital ist vorhanden. Handwerk Bestattungsgewerbe, Bestattungsunternehmen',\n",
       " 'Gewerbe-Immobilien-Verwaltung / Hausverwaltung gesucht Christian sucht eine Verwaltung für Gewerbeimmobilien zum Kauf (Hausverwaltung) 28.12.24: Anfrage\\nLiebes Dejuna-Team,\\n\\nwir interessieren uns für den Kauf eines kleineren bis mittelgroßen Property Management-Unternehmens/einer Hausverwaltung im Großraum München.\\nIdealerweise hat das Unternehmen primär Kunden und einen Bestand, der aus Gewerbeimmobilien besteht. Denn hier liegt auch die Expertise unseres Teams. Die Übernahme einer Hausverwaltung für Wohnimmobilien wollen wir natürlich nicht ausschließen.\\nVielen Dank für eine Kontaktaufnahme und ggf. erste Ideen im neuen Jahr.\\n\\nViele Grüße nach Hamburg\\n\\nChristian Simanek Immobilien Hausverwaltung',\n",
       " 'SHK Firma gesucht Claas sucht einen SHK-Betrieb zur Übernahme in Hamburg Ich bin interessiert an der Übernahme oder Mehrheitsbeteiligung an einem SHK-Betrieb in Hamburg. Gerne benachrichtigen Sie mich über Betriebe, welche auf mein Gesuch passen. Handwerk Heizung, Sanitär, Klima (SHK)',\n",
       " 'Makler sucht Hausverwaltung Immobilienmakler sucht Hausverwaltung 01.01.25 Anfrage via dejuna:\\nInteresse an Übernahme einer Hausverwaltung\\n\\nSehr geehrte Damen und Herren,\\n\\nwir sind auf der Suche nach einer bestehenden Hausverwaltung, die in den nächsten Jahren – beispielsweise aus Altersgründen – ihren Betrieb veräußern möchte. Unser Ziel ist es, unser Geschäftsfeld strategisch auszubauen und langfristig weiterzuentwickeln.\\n\\nDerzeit liegt unser Fokus auf der Mietverwaltung, doch ab 2025 werden wir unser Angebot um die WEG-Verwaltung erweitern. Im Laufe dieses Jahres übernehmen wir bereits einen Bestand und sind offen dafür, weitere Verwaltungsobjekte in unseren Einzugsbereich zu integrieren.\\n\\nDabei könnten wir uns vorstellen, vorhandene Mitarbeiter zu übernehmen. Zudem würden wir es begrüßen, wenn der bisherige Inhaber den Übergabeprozess aktiv begleiten möchte – gerne auch in leitender Funktion – und den Betrieb Schritt für Schritt übergibt. Sollte der Inhaber im Rentenbezug in Teilzeit oder nach seinen Vorstellungen weiterhin mitarbeiten wollen, würden wir dies ebenfalls gerne ermöglichen. Es ist jedoch keine Voraussetzung.\\n\\nDie Erfahrung und Expertise des bisherigen Inhabers möchten wir nicht verlieren, da wir wissen, wie wertvoll diese für den erfolgreichen Übergang sein können. Aus Erfahrung wissen wir auch, dass viele Unternehmer nach dem Verkauf eines Geschäftszweigs nicht sofort ganz aufhören möchten. Eine gewisse Übergangszeit oder die Möglichkeit, kürzerzutreten, können wir flexibel anbieten.\\n\\nUnser primäres Suchgebiet umfasst Nordrhein-Westfalen, insbesondere das Ruhrgebiet mit Städten wie z.B. Wuppertal, Düsseldorf, Essen, Bochum und Oberhausen usw.\\n\\nSollten Sie Unternehmen betreuen, die an einer Nachfolgelösung interessiert sein, freuen wir uns über Ihre Kontaktaufnahme.\\n\\nMit freundlichen Grüßen\\nMartin Bittscheidt Immobilien Hausverwaltung',\n",
       " 'Unternehmer sucht Hausverwaltung Unternehmer sucht WEG-Hausverwaltung zum Kauf 05.01.25 Anfrage via dejuna\\nZur Abdeckung einer hybriden Marktposition soll ein neues Unternehmen entstehen. Primär sollen hierbei sowohl die Strukturen eines klassischen Maklerbüros, als auch die fundierte Tätigkeit einer Hausverwaltung kombiniert werden.\\nEntsprechend wird eine WEG-Hausverwaltung zur Übernahme/Nachfolgeabdeckung gesucht. Der Tätigkeitsradius umfasst den Landkreis Pinneberg/Schleswig-Holstein und Teile des Hamburger Westens. Additional würde dieses Inserat auch für den Landkreis Itzehoe/Schleswig-Holstein passend sein. Immobilien Hausverwaltung',\n",
       " 'Bestatter sucht Bestattungsunternehmen zur Übernahme Bestatter sucht Bestattungsinstitut zur Übernahme 07.01.24 Anfrage via dejuna:\\nIch bin Bestatter und suche ein Bestattungsinstitut zur Übernahme im Raum: Zwischen Bonn, Siegburg, Neuwied, Montabaur, Bad Marienberg, Gummersbach.\\nDas wäre bei Kennzeichen der Handwerk Bestattungsgewerbe, Bestattungsunternehmen',\n",
       " 'Zahntechniker-Meister sucht Dentallabor Dirk ist Zahntechniker-Meister und sucht ein Dentallabor zur Übernahme 07.01.25 Anfrage auf dejuna Auschreibung (Dentallabor bei München)\\nich bin gerade gezwungen mich mit meinem Dentallabor zu vergrößern und suche eine geeignete Immobilie. Ich suche etwas zwischen 100 und 140 m², mit Platz für mindestens drei Personen. An besten im Bereich Giesing +5km. Gesundheits- und Sozialwesen Gesundheitswesen, Dentallabor',\n",
       " 'Zahntechniker-Meisterin sucht Denttallabor zur Übernahme Viktoria ist Zahntechniker-Meisterin und sucht ein Dentallabor rund um Hannover (150km) Ich bin Viktoria Pahl, eine Zahntechnikerin-Meisterin aus Hannover. Ich würde gerne ein Labor übernehmen in Niedersachen oder von Hannover 150km Umkreis.\\n\\nLiebe Grüße\\nViktoria Pahl Gesundheits- und Sozialwesen Gesundheitswesen, Dentallabor']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buyers_df['combined_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from geopy.distance import geodesic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the spaCy model (disable NER for speed)\n",
    "nlp = spacy.load(\"de_core_news_sm\", disable=[\"ner\"])\n",
    "\n",
    "# Pre-compile regex patterns for efficiency\n",
    "URL_PATTERN = re.compile(r'http\\S+|www\\.\\S+')\n",
    "EMAIL_PATTERN = re.compile(r'\\S+@\\S+')\n",
    "LONG_NUMBER_PATTERN = re.compile(r'\\b\\d{10,}\\b')\n",
    "DOMAIN_PATTERN = re.compile(r'(geschäft|dienstleistung|industrie)\\w*', re.IGNORECASE)\n",
    "NON_ALPHA_PATTERN = re.compile(r'[^a-zA-ZäöüÄÖÜß\\s\\'\\-]')\n",
    "\n",
    "def preprocess_text(text, nlp_model):\n",
    "    \"\"\"Clean and preprocess text using regex and spaCy.\"\"\"\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    \n",
    "    # Remove domain-specific terms, URLs, emails, long numbers, and non-alphabetic characters\n",
    "    text = DOMAIN_PATTERN.sub('', text)\n",
    "    text = URL_PATTERN.sub('', text)\n",
    "    text = EMAIL_PATTERN.sub('', text)\n",
    "    text = LONG_NUMBER_PATTERN.sub('', text)\n",
    "    text = NON_ALPHA_PATTERN.sub('', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    doc = nlp_model(text)\n",
    "    tokens = []\n",
    "    business_stopwords = {\"gmbh\", \"ag\", \"eg\", \"e.k\", \"gesellschaft\", \"unternehmen\", \"firma\"}\n",
    "    stop_words = STOP_WORDS.union(business_stopwords)\n",
    "    \n",
    "    # Keep tokens that are nouns, proper nouns, verbs, or adjectives (using lemmas)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'VERB', 'ADJ'} and token.text.lower() not in stop_words:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if len(lemma) > 2:\n",
    "                tokens.append(lemma)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def tokenize_for_bm25(text, nlp_model):\n",
    "    \"\"\"Tokenize text using spaCy to ensure consistency with preprocessing.\"\"\"\n",
    "    doc = nlp_model(text)\n",
    "    return [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "def get_bounding_box(lat, lon, max_distance_km=50):\n",
    "    \"\"\"\n",
    "    Compute an approximate bounding box for a given coordinate.\n",
    "    Note: 1 degree latitude ~ 111 km.\n",
    "    \"\"\"\n",
    "    delta = max_distance_km / 111  # Approximate conversion to degrees\n",
    "    return (lat - delta, lat + delta, lon - delta, lon + delta)\n",
    "\n",
    "def passes_geographic_filter(buyer_latitudes, buyer_longitudes, seller_latitudes, seller_longitudes, max_distance_km=50):\n",
    "    \"\"\"\n",
    "    Check if any pair of buyer and seller coordinates is within the specified maximum distance.\n",
    "    Uses a bounding box pre-filter before calculating the precise geodesic distance.\n",
    "    Returns a tuple: (True/False, distance in km or None)\n",
    "    \"\"\"\n",
    "    if not buyer_latitudes or not buyer_longitudes or not seller_latitudes or not seller_longitudes:\n",
    "        return False, None\n",
    "    \n",
    "    # Use the average buyer coordinate as a reference point\n",
    "    buyer_lat = np.mean(buyer_latitudes)\n",
    "    buyer_lon = np.mean(buyer_longitudes)\n",
    "    min_lat, max_lat, min_lon, max_lon = get_bounding_box(buyer_lat, buyer_lon, max_distance_km)\n",
    "    \n",
    "    for s_lat, s_lon in zip(seller_latitudes, seller_longitudes):\n",
    "        if s_lat is None or s_lon is None:\n",
    "            continue\n",
    "        # Quick check: if seller coordinate is outside the bounding box, skip\n",
    "        if s_lat < min_lat or s_lat > max_lat or s_lon < min_lon or s_lon > max_lon:\n",
    "            continue\n",
    "        # Calculate precise distance\n",
    "        distance = geodesic((buyer_lat, buyer_lon), (s_lat, s_lon)).km\n",
    "        if distance <= max_distance_km:\n",
    "            return True, distance\n",
    "    return False, None\n",
    "\n",
    "# -------------------------------\n",
    "# Load and Preprocess Datasets\n",
    "# -------------------------------\n",
    "logging.info('Loading datasets...')\n",
    "buyers_df = pd.read_csv('./data/dejuna_buyer_latest_hf_nace_geocoded.csv')\n",
    "sellers_df = pd.read_csv('./data/branche_nexxt_change_sales_listings_scrape_hf_nace_geocoded.csv')\n",
    "\n",
    "logging.info('Preprocessing buyers\\' text fields...')\n",
    "buyers_df['title_preprocessed'] = buyers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['description_preprocessed'] = buyers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['long_description_preprocessed'] = buyers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "buyers_df['preprocessed_branchen'] = buyers_df.apply(lambda row: f\"{row['Industrie']} {row['Sub-Industrie']}\", axis=1)\n",
    "\n",
    "logging.info('Preprocessing sellers\\' text fields...')\n",
    "sellers_df['title_preprocessed'] = sellers_df['title'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['description_preprocessed'] = sellers_df['description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['long_description_preprocessed'] = sellers_df['long_description'].apply(lambda x: preprocess_text(x, nlp))\n",
    "sellers_df['branchen_preprocessed'] = sellers_df['branchen'].apply(lambda x: preprocess_text(x, nlp))\n",
    "\n",
    "# Parse latitude and longitude from JSON strings\n",
    "logging.info('Parsing latitude and longitude from JSON strings...')\n",
    "sellers_df['latitude'] = sellers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "sellers_df['longitude'] = sellers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['latitude'] = buyers_df['latitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "buyers_df['longitude'] = buyers_df['longitude'].apply(lambda x: json.loads(x) if pd.notnull(x) else [])\n",
    "\n",
    "def combine_text_fields(row):\n",
    "    \"\"\"Combine preprocessed text fields into one string.\"\"\"\n",
    "    fields = [\n",
    "        row.get('title_preprocessed', ''),\n",
    "        row.get('description_preprocessed', ''),\n",
    "        row.get('long_description_preprocessed', ''),\n",
    "        # Use 'branchen_preprocessed' if available, otherwise 'preprocessed_branchen'\n",
    "        row.get('branchen_preprocessed', '') or row.get('preprocessed_branchen', '')\n",
    "    ]\n",
    "    return ' '.join(fields)\n",
    "\n",
    "logging.info('Combining text fields...')\n",
    "buyers_df['combined_text'] = buyers_df.apply(combine_text_fields, axis=1)\n",
    "sellers_df['combined_text'] = sellers_df.apply(combine_text_fields, axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# Load Models and Build BM25 Index\n",
    "# -------------------------------\n",
    "logging.info('Loading models...')\n",
    "# Load a bi-encoder for sentence embeddings\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "# Load the cross-encoder (ONNX-optimized) and its tokenizer\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained('./onnx_models/cross-encoder-de')\n",
    "\n",
    "logging.info('Building BM25 index for sellers...')\n",
    "seller_texts = sellers_df['combined_text'].tolist()\n",
    "# Use the spaCy tokenizer for BM25\n",
    "tokenized_seller_texts = [tokenize_for_bm25(text, nlp) for text in seller_texts]\n",
    "bm25_index = BM25Okapi(tokenized_seller_texts)\n",
    "\n",
    "logging.info(\"Encoding sellers' text with bi-encoder...\")\n",
    "seller_embeddings = bi_encoder.encode(seller_texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "logging.info(\"Seller embeddings generated.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Matching Parameters\n",
    "# -------------------------------\n",
    "similarity_threshold = 0.3      # Bi-encoder cosine similarity threshold\n",
    "cross_encoder_threshold = 0.35   # Cross-encoder threshold for initial filtering\n",
    "final_similarity_threshold = 0.25  # Final similarity threshold after geographic filtering\n",
    "\n",
    "# -------------------------------\n",
    "# Matching Function for a Single Buyer\n",
    "# -------------------------------\n",
    "def process_buyer(buyer_row, seller_embeddings, bm25_index, sellers_df):\n",
    "    \"\"\"Process one buyer and return valid matches as a list of dictionaries.\"\"\"\n",
    "    matches = []\n",
    "    buyer_text = buyer_row['combined_text']\n",
    "    buyer_latitudes = buyer_row['latitude']\n",
    "    buyer_longitudes = buyer_row['longitude']\n",
    "    \n",
    "    # BM25 candidate retrieval using the spaCy tokenized text\n",
    "    buyer_tokens = tokenize_for_bm25(buyer_text, nlp)\n",
    "    bm25_scores = bm25_index.get_scores(buyer_tokens)\n",
    "    # Retrieve the top 500 candidates\n",
    "    bm25_candidates = np.argsort(bm25_scores)[-1000:][::-1]\n",
    "    \n",
    "    # Compute the buyer embedding using the bi-encoder\n",
    "    buyer_embedding = bi_encoder.encode([buyer_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Compute cosine similarities between buyer and BM25 candidate seller embeddings\n",
    "    seller_subset_embeddings = seller_embeddings[bm25_candidates]\n",
    "    sim_scores = cosine_similarity(buyer_embedding, seller_subset_embeddings)[0]\n",
    "    candidate_indices = np.where(sim_scores >= similarity_threshold)[0]\n",
    "    if len(candidate_indices) == 0:\n",
    "        return matches  # No candidates pass the bi-encoder threshold\n",
    "    \n",
    "    # Use a percentile-based filter (top 90% among candidates)\n",
    "    candidate_scores = sim_scores[candidate_indices]\n",
    "    percentile_cutoff = np.percentile(candidate_scores, 10)\n",
    "    top_candidate_indices = candidate_indices[candidate_scores >= percentile_cutoff]\n",
    "    if len(top_candidate_indices) == 0:\n",
    "        top_candidate_indices = candidate_indices  # Fallback to all candidates if none pass cutoff\n",
    "    \n",
    "    # Prepare candidate pairs for cross-encoder inference\n",
    "    seller_texts_top = [sellers_df.iloc[bm25_candidates[idx]]['combined_text'] for idx in top_candidate_indices]\n",
    "    pairs = [(buyer_text, seller_text) for seller_text in seller_texts_top]\n",
    "    \n",
    "    # Batch inference using the cross-encoder\n",
    "    features = cross_tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = cross_encoder(**features)\n",
    "    cross_scores = torch.sigmoid(outputs.logits).squeeze().detach().numpy()\n",
    "    # Handle case where cross_scores is a scalar (0-d array)\n",
    "    if cross_scores.ndim == 0:\n",
    "        cross_scores = np.array([cross_scores])\n",
    "    \n",
    "    # Iterate over candidates, apply geographic filter, and record matches\n",
    "    for idx, cross_score in zip(top_candidate_indices, cross_scores):\n",
    "        seller_idx = bm25_candidates[idx]\n",
    "        seller_row = sellers_df.iloc[seller_idx]\n",
    "        seller_latitudes = seller_row['latitude']\n",
    "        seller_longitudes = seller_row['longitude']\n",
    "        \n",
    "        # Check if the seller and buyer are within the desired geographic proximity\n",
    "        location_match, distance_km = passes_geographic_filter(\n",
    "            buyer_latitudes, buyer_longitudes, seller_latitudes, seller_longitudes, max_distance_km=50)\n",
    "        \n",
    "        # Combine cross-encoder score with location match; adjust as needed\n",
    "        if cross_score >= cross_encoder_threshold and location_match:\n",
    "            final_score = cross_score  # You could also weight location here if desired\n",
    "            if final_score >= final_similarity_threshold:\n",
    "                match = {\n",
    "                    'buyer_title': buyer_row['title'],\n",
    "                    'buyer_description': buyer_row['description'],\n",
    "                    'buyer_location': buyer_row.get('location', ''),\n",
    "                    'seller_title': seller_row['title'],\n",
    "                    'seller_description': seller_row['description'],\n",
    "                    'seller_location': seller_row.get('location', ''),\n",
    "                    'similarity_score': final_score,\n",
    "                    'distance_km': distance_km,\n",
    "                }\n",
    "                matches.append(match)\n",
    "                logging.info(f\"Match found: Buyer '{buyer_row['title']}' with Seller '{seller_row['title']}' | Score: {final_score:.2f} | Distance: {distance_km:.2f} km\")\n",
    "    return matches\n",
    "\n",
    "# -------------------------------\n",
    "# Main Matching Loop\n",
    "# -------------------------------\n",
    "all_matches = []\n",
    "logging.info(\"Starting buyer-seller matching...\")\n",
    "for i, buyer_row in buyers_df.iterrows():\n",
    "    buyer_matches = process_buyer(buyer_row, seller_embeddings, bm25_index, sellers_df)\n",
    "    if buyer_matches:\n",
    "        all_matches.extend(buyer_matches)\n",
    "\n",
    "# Save matching results to CSV\n",
    "matches_df = pd.DataFrame(all_matches)\n",
    "# matches_df = matches_df.sort_values('similarity_score', ascending=False)\n",
    "output_path = './matches/nlp_business_high_conf_matches.csv'\n",
    "matches_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"Matching completed. Total matches found: {len(matches_df)}\")\n",
    "logging.info(f\"Matches saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"./onnx_models/cross-encoder-de\",\n",
    "    file_name=\"model.onnx\",\n",
    "    provider=\"CPUExecutionProvider\"  # or \"CPUExecutionProvider\"\n",
    ")\n",
    "\n",
    "# Load optimized version\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\"./onnx_models/cross-encoder-de\")\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install required packages\n",
    "# pip install optimum[onnxruntime] torch onnxruntime\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Use AutoModelForSequenceClassification to load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    ")\n",
    "\n",
    "# 2. Export to ONNX format with explicit architecture\n",
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "from optimum.onnxruntime import ORTOptimizer, ORTQuantizer\n",
    "\n",
    "# Save PyTorch model first\n",
    "model.save_pretrained(\"./tmp_model\")\n",
    "\n",
    "# 3. Convert using optimum-cli (recommended)\n",
    "# Run in terminal:\n",
    "# optimum-cli export onnx --model ./tmp_model --task text-classification ./onnx_models/cross-encoder-de\n",
    "\n",
    "# 4. Load the converted ONNX model\n",
    "# Option 2: Use German-optimized model\n",
    "cross_encoder = ORTModelForSequenceClassification.from_pretrained(\n",
    "    \"./onnx_models/cross-encoder-de\",\n",
    "    file_name=\"model.onnx\",\n",
    "    provider=\"CPUExecutionProvider\"  # or \"CPUExecutionProvider\"\n",
    ")\n",
    "# 5. Load tokenizer\n",
    "cross_tokenizer = AutoTokenizer.from_pretrained(\"./tmp_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
